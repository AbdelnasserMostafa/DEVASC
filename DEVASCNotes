DEVASC Notes
============
F5: The!Sky@Is#Blue1959

1) Why does using networked APIs present a challenge?
- Special considerations are required because of an unreliable network.

2) Which data format heavily uses whitespace indentations to define data structure?
- YAML

3) Which two data formats translate their data exclusively to a Python dictionary? (choose two)
- JSON
- YAML

4) which two options describe the purpos of revision control system, such as Git?
- Track who made changes
- Keep historic versions of a file

5) Where is the central location of the files of the project, from which all the developers can pull?
- remote repository

6) which command is used to initialize a Git project?
- git init

7) Which command is used to create and navigate to branch "my-new-branch"?
- git checkout -b my-new-branch

8) Why does using networked APIs present a challenge?
- Special considerations are required because of an unreliable network

9) Which data format heavily uses whitespace indentations to define data structure?
- YAML

10) Which two options are reasons that the use of APIs is growing? (Choose two.)
- APIs allow for reuse of server code to service different clients like mobile, web, and so on
- APIs enable developers to use cloud services.

11) Match the element of a parsed JSON file to the corresponding Python data type
- Answer:
    JSON            Python
    ------          ----
    dict            object
    list            array
    None            null
    str             string
    
12) Which Python XML parsing library produces dictionary object most similar to the built-in JSON parser?
- xmltodict

13) What is the purpose of XML namespaces?
- Namespaces allow multiple elements in an object, with the same tag name, to represent different concepts.

14) Which command is used to navigate through branches?
- git checkout

15) What does the git add command do?
- It adds files to the staging area.

16) A senior developer who is working on the same branch as you has just added a fix for a critical bug to a shared Git repository. Which command should you use to update your working directory and test their code with yours?
- git pull

17) What are the three major key points of the lean philosopy?
- Purpose
- process
- People

18) What is a sprint?
- A short period in which people focus on small, but meaningful development tasks.

19) What is the first step in a test-driven development iteration?
- Write tests that pass

20) What is the correct order of steps in a TDD iteration?
- Step 1: Write test; Step 2: Run tests (should fail); Step 3: Write code; Step 4: Run tests (should pass); Step 5: Refactor code

21) Which of these code constructs can be used to efficiently split the workload between multiple groups of software developers?
- Modules

22) What is the optimal combnation of coupling and cohesion?
- low cohesion and loos coupling

23)
- It presents a process of concealing data and methods of an object, with the intention to hide the implementation and restricting direct access to the objects data.

24) An event in the system changed the state of the model. How will the view component update the screen to accompany those changes?
- The model will notify the controller and view that there has been a change, and the view will request the changes from the model and controller.

25) Which two statements state when and how the notify procedure should be called in the observer pattern?
- It should be called on a state change by the application code triggering the change
- It should be called on a state change by the publisher itself.

26) Match the HTTP methods and status codes with their relation towards HTTP resources;
- Resource is modified       ---> POST
- Resource is removed        ---> DELETE
- Resource is replaced       ---> PUT
- Resource is not found      ---> 404
- Resource is created        ---> 201
- Resource is no longer here ---> 301

27) Match the RPC protocols with its related terms.
- Device Configuration       ---> NETCONF
- Very Simple                ---> JSON-RPC
- WSDL                       ---> SOAP (Simple Object Access Protocol)
- HTTP/2                     ---> gRPC (Google Remote Procedure Call)

28) Which two statements are true about REST APIs? (Choose two)
- REST uses a set of HTTP methods to define its set of operations
- REST responses contain function call results from the request

29) Which two elements can be viewed in the Response panel? (Choose two)
- HTTP status code
- formatted response body

30) With which values are variables populated when you share or publish a Postman Collection?
- Initial values, defined in the narrowest scope

31) How are webhook notifications transported to subscribers?
- As HTTP POST requests

32) What is the difference between rate limiting and payload limiting?
- In former, you limit the rate of API responses the API produces while in the later, you limit the size of the API response bodies

33) How frequently is a new token issued in custom token authentication?
- Only once for every user.

34) Which of these statements correctly displays the format that is used in HTTP authentication requests?
- WWW-Authenticate:<auth_type><realm><credentials>

35) What does the term hardcoding mean?
- Including data directly in the source code.

36) Match terminology with the correct explainations
- Is used to encrypto the messages                              ----> Public Key
- Is used to decrypt the messages                               ----> Private Key
- Is a protocol, used to provide security to communication      ----> RSA
- Ia an algorithm, used to encrypt the communication            ----> TLS

37) Which Cisco management platform offers the most flexibility in supporting different types of solutions, platforms, and equipement vendors?
- Cisco NSO

38) Which Cisco UCS management option is hosted in the public cloud?
- Cisco Intersight

39) Which tool provides you a library of Microsoft PowerShell Cmdlets that you can use to retrieve and manipulate Cisco UCS Manager-managed objects?
- Cisco UCS Manager PowerTool Suite

40) Which collaboration product provides an API to easily create a bot?
- Cisco Webex Teams

47) Which API allows you to configure users and devices in Cisco Unified Communication Manager?
- Administrativ XML API

48) Which security category does Cisco Firepower belong to?
- Network protection

49) What is the easiest way to learn the Cisco FTD REST API?
- Use the provided REST explorer that is hosted on FTD and learn by doing.

50) Which two options are advantages of Cisco SD-WAN as compared to Cisco Meraki?
- Enables traffic segmentation
- Uses traditional Cisco Infrastructure

51) For which two common tasks is Network automation used? (choose two)
- Device Provisioning
- Reporting

52) Which option ensures that JSON is used as the enoding type when using RESTCONF for IOS XE Software?
- application/vnd.yang.data+json

53) Match the Cisco NX-OS capability to its description
- Allows you to send and receive XML documents that are based off XSD schema        ---> NETCONF
- REST API that sends and receive structured objects                                ---> NX-API REST
- REST-like API that sends commands and receives structured objects                 ---> NX-API CLI
- Allows you to execute scripts locally on the switch                               ---> Python Onbox

54) Which Cisco product allows you to use Python API to create a mapping logic between service and device parameters?
- Cisco NSO

55) Which Webex Teams API endpoint can be used to list the participants of a group space?
/memberships

55) Which Cisco DevNet resource offers you the ability to chat with an expert about APIs?
- DevNet Support

56) What is the PDU called at the Internet layer?
- Packet

51) How many hexadecimal numbers represent each MAC address?
- 12

52) At which layer of the TCP/IP stack does IP operate?
- Internet layer

53) Which term is used to describe the application identifier for TCP and UDP?
- Port

54) What are the three functional planes of Cisco switches? (choose thee)
- Control Plane
- Management Plane
- Data Plane

54) Which protocol enables devices to obtain the subnet mask for a network?
- DHCP

55) Which two network devices are typically used to perform NAT for an enterprise network?
- Firewall
- Router

56) Match protocols to their use cases.
- This protocol is used as virtual terminal for remote system			---> SSH
- This protocol is used for verifying status of TCP ports				---> Telnet
- This protocol is used to retrieve information from the MIB			---> SNMP
- This protocol is used by applications sending emails					---> SMTP

57) In which two ways would a DNS Server and Default Gateway malfunction manifest itself?
- A DNS Server malfunction would cause the nslookup command to return an invalid or missing entry.
- If the user's Default Gateway fails, all traffic from that subnet will remain local to that subnet.

58) Which statement about troubleshooting connectivity issues is accurate?
- Layer 1, Layer 2, and Layer 3 connectivity issues can be investigated using ping, traceroute and arp commands. While deep packet investigation can be done with the tcpdump command

59) Match the description with the corresponding term.
- Jetter			---> The variation of delay.
- Bandwidth			---> The amount of free capacity on a link.
- Latency			---> The time that it takes for a packet to reach its destination
- Packet Loss		---> Number of frames not received.

60) Which of the following are two main encoding formats?
- JSON
- XML

61) Which three are NETCONF protocol operations? (choose three)
- get-config
- lock
- kill-session

62) Which YANG statement is used for an attribute that has one value, no children, and a single instance?
- leaf

63) Which statement is correct regarding HTTP operations?
- The HTTP DELETE operation is equivalent to the NETCONF edit-config with operation delete.

64) What are two aspects of client-side validation? (choose two)
- YDK service will automatically perform local validation
- Semantic check (key uniqness/presence, mandatory leafs, and so on).

65) Which two of the following methods are supported in the NX-API REST? (Choose two)
- POST
- GET

66) Which deployment model uses host operating system features to provide an isolated environment for multiple tenants to run applications on the same server?
- Containers

67) Which application deployment option refers to an on-demand environment for developing and testing software applications?
- PaaS

68) What is edge computing?
- Edge computing is a centralized network architecture that lets you transfer your compute, storage, communication control, and decision making closer to the network edge.

69) Which three options descibe parts of DevOps life cycle
- Monitor
- Code
- Deploy

70) Which of these CI-CD pipeline tools provides a code repository?
- GitLab

71) Match the commands with the correct actions.
- Create a directory 					---> mkdir
- Remove a file or directory, or both	---> rm
- Exports variables						---> export
- Prints the working directory 			---> pwd
- Creates a file						---> touch

72) What are two benefits of Software-Defined and Intent-Based Networks? (Choose two)
- Centralized Provisioning
- Network Security

73) Which three of the following options best describe Infrastructure as Code(IaC)? (Choose Three)
- A broad definition, under which is Network as Code, executed using the DevOps module.
- Uses a declarative model to capture a desired state or result.
- Bring with it the benefits of centralized storage, collaboration, and evolution of the code.

74) What is the name of the Ansible agent required to be installed on every managed device?
- There is no required agent, Ansible is an agentless mnagement solution.

75) Which of the following best describes the components that are found in an Ansible Playbook? (Choose three)
- A Module is a code to be executed while a Task is an action referencing a Module
- A Play is a set of Tasks to be executed while a Playbook is a collection of Plays
- A Role is a set of Playbooks that are to be executed in a repeatable manner.

76) Which of the following statement best describes CI/CD Pipelines?
- More currently known as CI/CD Pipelines, is a process of adding quality control and assurance to the creation, collaboration, testing, and final delivery of scripted changes to the production environment.

77) Consider a system that uses a single server to which users can connect through their browser and open sessions for an unspecified amount of time. In which layer of the testing pyramid should the maximum number of opened connections on that server be tested?
- Stress testing

78) What are fictures in unit testing?
- Code used for setting up the environment for the test cases.

79) With which instruction must each Dockerfile start with?
- FROM

80) Which command creates a new Docker container?
- docker pull ubuntu:latest

81) Same-origin policy relies on which parts/components of a request to be the same?
- Hostname, Port number

82) What is the difference between a network firewall and a web application firewall?
- WAF operates on Layers 3-7

83) What is the importance of having a test development environment for the DevNet Changes you want to apply to your production network?
- To test and perfect the scipted changes before they're applied on a large scale to the production network.
=====================

Notes:
=====

- Ansible and its elements can be descirbed in the following three statements (note: They are all chained together):
	 - Module is a code that needs to be executed while a task is an action referencing a module.
	 - A play is a task that needs to be executed while a playbook is a collection of plays.
	 - A Role is a collection of plabook that can be executed in a repatable manner.

-- SDLC:
        - Requirments Gathering 
        - Analysis
        - Design
        - Development
        - Quality Assurance
        - Deployment
        - Enhancement and Maintenance
        - Repeat

-- There are several methodologies that exist to help implement SDLC. These include Waterfall, Agile, prototyping, rapid app development, and extreme programming:
        - Waterfall: A development methodology where a linear process is visualized as moving down through phases.
        - Agile: One of the newer and most popular methodologies that are based on frequent and small incremental releases.
        - Prototyping: Designed with the idea to build an initial version of a product quickly in order to better understand requirements and scope. By using this prototype, the client can get an actual feel of the system in a relatively short time, even though this effort may complicate larger projects.
        - Rapid app development: Puts less emphasis on planning and more emphasis on process. It is a type of incremental model in which all components are developed in parallel.
        - Extreme programming: An extension of Agile, with unit testing, code reviews, simplicity, as well as customer communication and feedback taken to the "extreme".

-- TDD Test-Driven Development:
        - TDD ensures that use cases are tested, and source code has automated tests by using the test-first approach
        - Development id done in iterations, where you do the following:
                - Write Tests.
                - Run these tests - they must fail (code may not even compile)
                - Write source code
                - Run all tests - they must pass
                - Refactor the code where necessary
                
- Refactoring the code before moving to the next task has multiple advantages:
	- Better code structure: You might find out that certain part of the code will be used multiple times so you move it to a separate function.
	- Better code readability: You might want to split or combine different parts of the code to be more readable for other developers who are not familiar with the task.
	- Better design: You might find that refactoring the code in some way will simplify the design or even improve performance of software.

- The TDD iteration is finished when code refactoring is done and all tests pass. Then you can proceed to the next iteration.
- The following are reasons for a code review:
    - identify bugs
    - Improve code quality
    - Get familiar with different parts of the project
    - Learn something new
- In the Git community, code review is done with a pull request.

- How to perform a code review? 
	- Multiple tools exist that help you during the code review phase. Some of these tools are GitHub and GitLab, which have a feature called Pull Request (GitHub) or Merge Request (GitLab), through which developers can initiate the code review.

- Consider this typical feature development or bug fixing workflow:
    - Create a new branch.
    - Commit code changes to the remote repository.
    - Create a pull request.
    - Automated tests
    - Code review
    - Code accept (merge)
    - Perform additional automated tests.
    
- In a typical software development environment, pull request creation automatically triggers additional checks that are made before the reviewers begin reviewing code:
    - Frontend code tests
    - Backend code tests
    - Database migration tests
    - Code style test
    - Building a Docker image test
    - Other tests
       
- Modular Design Benifits;
	- Maintaining Modularity:
	- Here are some of the design guidelines that you should consider:
    	- Acyclic dependencies principle
        	- Ensures, that when you split your monolithic application into multiple modules, these modules, and classes accompanying them, have dependencies in one direction only. If there are cyclic dependencies, where modules or classes are depended in both direction, changes in module A can lead to changes in module B, but then changes in module B can cause unexpected behavior in the module A, where changes originated from. In large and complex systems, these kinds of cyclic dependencies are harder to detect and are often sources of code bugs. It is also not possible to separately reuse or test modules with such dependencies.
        
        	- To decouple these two levels of modules, strategies like dependency inversion or dependency injection, which rely on introduction of an abstraction layer, can be used.
        		- Dependency inversion is defined as follows:
                	- High-level modules should not depend on low-level modules. Both should depend on abstractions.
                	- Abstractions should not depend upon details. Details should depend upon abstractions.
    	- Stable dependencies principle
    	- Single responsibility principle

OOP Object Oriented Programming
-------------------------------
The concepts that define what OOP enables (AKA The Four Pilar of OOP):
    - Abstraction
        An abstraction's ambition is to hide the logic implementation behind an interface. Abstractions offer higher level of semantic contract between clients and the class implementations. When defining an abstraction, your objective is to expose a way of accessing the data of the object without knowing the details of the implementation. Abstract classes are usually not instantiated, they require subclasses that provide the behavior of the abstract methods. A subclass cannot be instantiated until it provides some implementation of the methods that are defined in the abstract class from which it is derived. It can also be said that the methods need to be overridden.

    - Encapsulation
        In OOP, different objects interact among each other in their runtime. One object can access other objects data and methods, no matter if the type of the object is the same or not. Many times, you want some of the data and methods to stay private to the object, so that the object can use them internally, but other objects cannot access them. Encapsulation in OOP conceals the internal state and the implementation of an object from other objects. It can be used for restricting what can be accessed on an object. You can define that part of data that can be accessed only through designated methods and not directly, this is known as data hiding. In Java, C# and similar statically typed, compiled languages, you will find that there is an option to explicitly define variables and methods, that other objects cannot access, using the private keyword or protected keyword, that also restricts the child classes from accessing it.

        In Python, encapsulation with hiding data from the others is not so explicitly defined and can be interpreted rather as a convention. It does not have an option to strictly define data being private or protected, however, in Python you would use notation of prefixing the name with an underscore (or double underscore) to mark something as nonpublic data. When using double underscore, name mangling occurs. This means that a variable name, prefixed with two underscores, is in a runtime that is concatenated with the class name. If you have a __auditLog() method in a Device class, which is prefixed with a double underscore, the name of the method becomes _Device__auditLog(). This is helpful to prevent accidents, where subclasses override methods, and break the internal method calls on a parent class. Still nothing prevents you from accessing the variable or method even though, by convention, it is considered private.
		
    - Inheritance
	
    - Polymorphism
        Polymorphism, in OOP, goes hand in hand with class hierarchy. When a parent class defines a method that needs to be implemented by the child class, this method can be considered as polymorphic, since the implementation would have its own way of presenting a solution that the higher-level class proposed. Polymorphism can be found in any setup of an object, that can have multiple forms. When a variable or a method accepts more than one type of value or parameter, it is considered to be polymorphic as well.
        
Designing Software Architecture and Design Patterns
Decision on software architecture can be made, while studying these characteristics of a system:

    - Performance
    - Availability
    - Modifiability
    - Testability
    - Usability
    - Security
    
Some of the commonly known software architecture patterns:

    - Layered or n-tier architecture pattern
         - The layered architecture pattern, also known as n-tier architecture pattern, is one of the most common general-purpose software architecture patterns. This design pattern closely relates to the organizational structures of most companies (Conway's law), so it is an instinctive choice for most application developments that concern enterprise businesses. Software components within this pattern are formed into horizontal layers, where each layer performs a specific role in the application. There is no specification on the number of layers that you should use, but the combination of four layers is the most frequently used approach. The four typical layers that are used in the architecture patterns are presentation, business, persistence, and database layer. For larger and more complex applications, more layers can be used.
         Note: Business and persistence layer are sometimes combined into one layer, so you end up with 3 layers or, so called, 3-tier architecture.
         
    - Event-driven architecture pattern
    - Microservices architecture pattern
    - Model View Controller (MVC) architecture pattern
    - Space-based architecture
    
HTTP Protocol
- Process of the request-response cycle:
    1. Client sends an HTTP request to the Web.
    2. Server receives the request.
    3. Server processes the request.
    4. Server returns an HTTP respnse.
    5. Client receives the response
    
How to inspect the request-response cycle in an HTTP packet
    1. Visit an URL in browser.
    2. Enter the developer mode (usually the F12 key).
    3. Select an HTTP session
    4. Check out the request and response headers.
    5. Inspect the header data
    6. Inspect the response body data.
    
HTTP Request
An HTTP request is the message sent by the client and consists of four parts:
    - Request-line, which specifies the method and location of accessing the resource. It consists of request method (or HTTP verb), request Universal Resource Identifier (request-URI) and of protocol version, in that order.
    - Zero or more HTTP headers. These contain additional information about request target, authentication, or take care of content negotiation, and so on.
    - Empty line, indicating the end of the headers.
    - Message body, that contains the actual data, transmitted in the transaction. It is optional and mostly used in HTTP power-on self-test (POST) requests.
    
HTTP Response
An HTTP response is the reply to the HTTP request and is sent by the server. The structure is similar to that of the request and consists of the following parts:
    - Status-line, that consists of the protocol version, a response code (called HTTP Response Code) and a human readable reason phrase, that summarizes the meaning of the code.
    - Zero or more HTTP headers. These contain additional information about the response data.
    - Empty line, indicating the end of the headers.
    - Message body, which contains the response data, transmitted in the transaction.

HTTP URL
HTTP Requests use an URL to identify and locate the resources, targeted by the request. The "Resource" term in URL is very broadly defined, so it can represent almost anything — a simple web page, an image, a web service, or something else.

HTTP URL example
http://www.example.com:8080/resource/resource?action=show#page=4

URLs are composed from predefined URI components:
    1. Scheme: Each URL begins with a scheme name that refers to a specification for assigning identifiers within that scheme. Examples of popular schemes are http, https, mailto, ftp, data, and so on.
    2. Host: URL host can be either a fully-qualified domain name (FQDN) or an IPv4 or IPv6 public address.
    3. Port: An optional parameter which specifies the connection port. If no port is set, the default port for the scheme is taken (the default port 80 for HTTP).
    4. Resource Path: A sequence of hierarchical path segments, separated by a slash ( / ). It is always defined, although it may have zero length (for example, https://www.example.com/).
    5. Query: An optional parameter, preceded by the question mark (?) passed to the server that contains a query string of non-hierarchical data.
    6. Fragment: Fragment is also an optional parameter, that starts with a hash ( # ) and provides directions to a secondary resource (for example, specific page in a document) and is processed by the client only.

Two commonly mentioned terms in relation to URLs are URNs and URIs:
    URI (Universal Resource Identifier) identifies a resource: ../people/alice.
    URL (Universal Resource Locator) also tells where to find it: http://www.example.com/people/alice.
    URN (Universal Resource Names) identifies a resource using (a made up) urn scheme: urn:people:names:alice.

URI is used to unambiguously identify a resource and is a superset of URLs and URNs (Universal Resource Names), which means that all URNs and URLs are URIs, but not vice versa. While the URI identifies the resource, it does not tell where it is located.

URN is an URI that uses the urn scheme and identifies a resource within a given namespace. Namespace refers to a group of names or identifiers (for example, a file system, network, and so on). URNs do not guarantee the availability of a resource.

Some web-based API usage examples are:
    - Resource manipulation: APIs commonly support Create, Read, Update, Delete (CRUD) actions on resources.
    - Automation: More and more remote systems can be automated via exposed API endpoints, either sending data automatically or reacting to some predefined conditions.
    - System configuration: A lot of networking equipment can be remotely configured via various HTTP-based protocols.
    - Service management: Web services such as monitoring and provisioning benefit greatly from API usage due to abstraction and standardization.

HTTP Methods:
    - GET:  Requests a representation of a specific resource. Should only retrieve data and is considered safe and idempotent.   
    - POST: Is used to submit an entity to the specified resource, often causing a state-change or side effects on the server. Requests made with POST should include a request body.   
    - DELET: Deletes the specified resource. Subsequent calls should not cause any side-effects.   
    - PUT:  This method replaces all current representations of the target resource with the request payload.    
    - HEAD: Asks for a response identical to that of a GET request, but without the response body. Useful for validating resource availability    
    - PATCH: Patch applies partial modification to a resource. Useful for instances where using PUT might be too cumbersome. PATCH is not an idempotent method and is used for merging resources.
        
HTTP Status Codes
HTTP response status codes are a predefined set of numerical codes that indicate the status of a specific HTTP request in the response header. Status codes are separated into five classes (or categories) by functionality. You can create your own status codes, but it is strongly advised that you don't—most user agents will not know how to handle them.  Following is a brief overview of status code categories and a few of the status codes that you are likely to encounter when working with web-based APIs. A complete list of status codes is available in RFC 7231, which describes the HTTP/1.1 standard.

1xx (Informational)
    Most codes from this category indicate that the request was received and understood. They usually mean that the request processing continues and alerts the client to wait for the final response. They are rarely used.

2xx (Successful)
    200 (OK): Standard response for a successful HTTP request. The information returned depends on the request’s method.
    201 (Created): Indicates that a resource has been successfully created.
    204 (No content): The server has successfully fulfilled the request and the response body is empty. A 204 code is useful when you want to confirm that a POST request was received by the server.

3xx (Redirection)
    301 (Moved Permanently): This and all future requests should be directed to the given URI.
    302 (Found): The requested resource resides temporarily under a different URI.
    304 (Not Modified): Indicates that the resource has not been modified since the version specified by the request headers. Useful for reducing overhead.

4xx (Client Error)
    400 (Bad Request): The server cannot process the request, due to a malformed request (bad syntax, deceptive routing, size too large).
    401 (Unauthorized): The request requires a valid authorized user. Usually means that the user is not authenticated or that authentication failed.
    403 (Forbidden): The request was valid, but the server is refusing action. The user might not have the necessary permissions for a resource.
    404 (Not Found): The server has not found anything matching the request URI. No indication is given of whether the condition is temporary or permanent.
    Other status codes include more specific information about request error.

5xx (Server Error)
    500 (Internal Server Error): A generic error message, given when an unexpected condition was encountered, and no more specific message is suitable.
    501 (Not Implemented): The server does not support the functionality required to fulfil the request.
    503 (Service Unavailable): Service cannot handle the request. It is usually a temporary condition attributed to a server crash, maintenance, overload, and so on.
    Other status codes include more specific information about the server error.

HTTP Headers
The headers are a list of key-value pairs that the client and server use to pass additional information or metadata between them in requests. They consist of a case-insensitive name, followed by a colon ":" and then its value. There are dozens of different headers, some defined by the HTTP standard and others defined by specific applications, so only the most common ones will be mentioned.

There are four distinct types of headers:
General headers:
    - Headers from this category are not specific to any particular kind of message.
    - Primarily used to communicate information about the message itself and how to process it.
    - Cache-Control: Specifies caching parameters.
    - Connection: Defines connection persistency.
    - Date: A datetime timestamp.

Request headers:
    - These headers carry information about the resource to be fetched.
    - They also contain the information about the client.
    - Accept-(*): A subset of headers, that define the preferred response format.
    - Authorization: Usually contains a Base64 encoded authentication string, composed of username and password for basic HTTP authentication.
    - Cookie: Contains a list of key-value pairs that contain additional information about the current session, user, browsing activity, or other stateful information.
    - Host: Is used to specify the internet host and port number of the resource being requested. This header is required in request messages.
    - User-Agent: Contains the information about the user agent originating the request.

Response headers:
    - Hold additional information about the response and the server providing it.
    - Age: Conveys the amount of time since the response was generated.
    - Location: Is used to redirect the client to a location other than the request URI from a header.
    - Server: Contains the information about the software used by the origin server to handle the request.
    - Set-Cookie: Is used to send cookies from the server to the client. Contains a list of key-value pairs, called cookies.

Entity headers:
    - Contain information about the response body.
    - Allow: Lists the supported methods identified by the requested resource.
    - Content-Type: Indicates the media type of the body (also called Multipurpose Internet Mail Extensions [MIME] type), sent to the recipient. Used for content negotiation.
    - Content-Language: Describes the language of the intended audience for the enclosed body.
    - Content-Length: Indicates the size of the body.
    - Content-Location: Is used to supply resource location for the entity, that is accessible from somewhere else than the request URI.
    - Expires: Gives the datetime after which the response is considered stale.
    - Last-Modified: Indicates the date and time at which the origin server believes the variant was last modified.

The problem with both the Basic HTTP authentication and API Key authentication is that the authentication details have to be sent (and threfore processed and compared) with every API call. This action creates a weak point in security, since every request contains this data and is thus a viable attack surface.

Custom Token (or Access/Authentication Token) authentication solves this problem, by using a custom token instead of this authentication data. In addition, the token is then commonly used for authorization too.

The process of authentication is as follows:
    1. The client tries to access the application.
    2. Since the client is not authenticated, it redirects it to an authentication server, where the user provides the username and password.
    3. Authentication server validates the credentials. If they are valid, a custom, time-limited and signed authentication token is issued and returned to the user.
    4. Client's request now contains the custom token, so the authenticated request is passed to the API service.
    5. API service validates the token and serves the required data.

HTTP authentication uses several different authentication schemes:
    - Anonymous (no authentication)
    - Basic (Base64 encoded credentials as username:password)
    - Bearer (HTTP implementation of custom token authentication)
    - Digest (MD5 hashed credentials)
    - Mutual (two-way authentication)
    - And more uncommon ones (HMAC, HOBA, AWS, OAuth, Negotiate, and so on)
    
Three different encryption methods are commonly used:
    Full-disk encryption: 
    Encrypts all the data on the disk, usually with the help of the operating system. Encryption key is commonly a password, that the administrator enters when disk encryption starts. Some disks even feature physical chips, that are used for disk encryption (Self-Encrypting Drives [SED]). Accessing any data or API credentials stored there, or even just booting up a system that is installed on that disk, requires the encryption key.

    File system encryption: 
    Alternatively, only certain files, file systems or partitions can be encrypted. In this way, only the sensitive data can be encrypted (for example, documents, credentials, and so on), while the operational data (for example, operating system, office programs) are not. Symmetric encryption is used here too.

    Database encryption: 
    Since data is commonly stored in a database, the database software often allows you to encrypt the data at the application-level. Similarly to disk and file encryption, a password is needed to transparently access the data.

Certificates contain predefined information, which are defined in the X.509 standard and include:
    - Owner's public key
    - Owner's distinguished name (DN)
    - CA's distinguished name
    - Valid from and expiry dates
    - Certificate's unique serial number
    - Protocol information

Depending on the scope, digital certificates can be further classified as one of the three types:
    - Single-domain: Applies to one hostname only (e.g. www.example.com).
    - Wildcard: Applies to an entire domain and its subdomains (e.g. *.example.com will include domains like mail.example.com and admin.example.com).
    - Multi-domain: Applies to several different domains (either single-domain or wildcard).

When a client and a server want to begin the encrypted connection, either for browsing or consuming an API, it is done via a series of messages, which is called a TLS handshake.

Client        
>>>>>> "Clinet Hello" Message
Sever
<<<<<< "Server Hello: Message
<<<<<< "Certificate" Messabe
Client verifies the server's certificate against the CA.
client 
>>>>>> sends a random secret, encrypted with the server's public key
Server
    server decrypt the secret
    Symmetric session are generated from the secret on the client and the server.
    Communication is encrypted from this point onward with the session keys.
client
>>>>>  Encrypted "Finished" Message
Server
<<<<<  Encrypted "Finished" Message

Network Management Platforms
The following are requirements to Orchestrate and Automate Services End-to-End and Manage Individual Elements and Services
    - Agility
    - Flexibility
    - Scalability
    - Security
    - Availability
    - Automation
    - Ineroperability
    - API
    
Cisco Network Managment Platforms
There are three management platforms from Cisco that they fit into an enterprise and/or service provider environment:
    1. Cisco NSO (Network Services Orchestrator) for End-to-End Orchestration of Services
       	- Cisco NSO enabled by Tail-f is a model-driven (YANG) platform for automating your network orchestration. It support multivendor networks through a rich variety of Network Element Drivers (NEDs). NSO supports the process of validating, implementing, and abstracting your network configuration and network services, providing support for the entire transformation into intent-based networking.
    2. Cisco ACI (Application Centeric Infrastructure) for the Orchestration of Data Center Resources
       	- Cisco ACI is a data center software-defined network (SDN) solution that is used to allow application to dynamically request data center network resources
    3. Cisco DNA (Digital Network Architecture) for the Orchestration of Enterprise Campuse And WAN Resources
        - Intent-based networking (IBN) built on Cisco DNA takes a software-delivered approach to automating and assuring services across your WAN and your campus and branch networks. 		- Cisco DNA enables you to streamline operations and facilitate IT and business innovation.
    
DNA Digita Network Infrastructure
---------------------------------
Cisco Digital Network Architecture
Cisco DNA is an open, extensible, software-driven architecture. Cisco DNA uses five fundamental new design principles for the networking software stack:
    - Virtualize everything
        To give organizations freedom of choice to run any service anywhere, independent of the underlying platform—physical or virtual, on premise, or in the cloud.
    - Designed for automation:
        To make networks and services on those networks easy to deploy, manage, and maintain—fundamentally changing the approach to network management.
    - Pervasive analytics:
        To provide insights on the operation of the network, IT infrastructure, and the business—information that only the network can provide.
    - Service management delivered from the cloud:
        To unify policy and orchestration across the network—enabling the agility of cloud with the security and control of on premises solutions.
    - Open, extensible, and programmable at every layer:
        Integrating Cisco and third-party technology, open API's and a developer platform, to support a rich ecosystem of network-enabled applications.
        
Cisco DNA is delivered across three layers:
    Layer 1 is the network element layer
    Layer 2 is the platform layer
    Layer 3 is the network-enabled applications layer
    
As you build to the design principles, you will use the cloud for the following:
    Cloud managed:
        Using the cloud to security manage all elements in the network through a singl pane view.
    Cloud edge:
        Providing critical network functions at the edge to support business moving their operations to the cloud (likde AWS, Azure)
    Cloud delivered:
        Enabling flexible subscription models where possible, minimizing the infrastructure burden
        
Cisco DNA Center is the network management system, foundational controller, and analytics platform at the heart of Cisco intent-based network. Beyond device management and configuration, Cisco DNA Center is a set of software solutions that provide:
    - A management platform for all your network.
    - An SDN controller for automation of your virtual devices and services.
    - An assurance engine to guarantee the best network experience for all your users
	
Cisco DNA Center software resides on the Cisco DNA Center Appliance and controls all your Cisco devices—both fabric and non-fabric.
    - Complet Network Management System
        - Single Pane of Glass for All Devices
        - End-to-End Health Information in Real Time
        - Granular Visibility
        - Simplified Workflows
    - Automation For Provisioning
        - Zero-Touch Deployment
        - Device Lifecycle Management
        - Ploicy Enforcement
    - Analytics For Assurance
        - Verify Intent of Network Settings
        - Proactively Resolve Issues
        - Reduce Time Spent Troubleshooting
    - Platform for Extensility
        - Integrate APIs with Third-party Solutions
        - Integrate And Customize Servicenow
        - Evolve Operational Tools and Processes
        
ACI Application Centeric Interface
---------------------------------
Cisco ACI (Application Centeric Infrastructure) consists of the data center networking infrastructure that is controlled by a Cisco Application Policy Infrastructure Controller (APIC)
    - Fabric Management and Automaion
    - Network and Application Security
    - Virtualization and Containers
    - Open partner Ecosystem
  - Touchless provisioning
  - Graceful Insertion and Removal
  - Scalability multitenancy
  - Workload mobility
  - Real-time monitoring and troubleshooting
  - Centralized policy enforcement
  - Zero-trust security model
  - Cisco DNA Integration
  - CloudSec (WAN encryption)
  - Open APIs
  
The Cisco Application Policy Infrastructure Controller (Cisco APIC) is the main architectural component of the Cisco ACI solution. It is the unified point of automation and management for the Cisco ACI fabric, policy enforcement, and health monitoring. The controller optimizes performance and manages and operates a scalable multitenant Cisco ACI fabric.

Cisco ACI is a centralized application-level policy engine for physical, virtual, and cloud infrastructures, and it provides the following capabilities:
    - Detailed visibility, telemetry, and health scores by application and by tenant.
    - Designed around open standards and open APIs.
    - Robust implementation of multi-tenant security, quality of service (QoS), and high availability.
    - Integration with management systems such as VMware, Microsoft, and OpenStack.
    - Cloud APIC appliance for Cisco Cloud ACI deployments of public cloud environments.

Cisco APIC is a single point of automation and management for the Cisco ACI fabric:
    - Centralized Controller for ACI Fabric
    - Web HTML5 GUI and RESTful API (XML or JSON)
    - Application-oriented network policy
    - Extensive 3rd-party integration (65+ partners)
    - ACI App Center extends functionality
Designed for automation, programmability, and centralized management, the Cisco APIC itself exposes northbound APIs through XML and JSON. It provides both a CLI and GUI, which utilize the APIs to manage the fabric holistically.

NSO Network Services Orchestrator
--------------------------------
At a very high level, Cisco NSO has three components:
    1. A model-based programmatic interface that allows for control of everything from simple device turn-up and configuration management to sophisticated, full lifecycle service management.
    2. A fast, highly scalable, highly available configuration database (CDB) that serves as a single source of truth.
    3. A device abstraction layer that uses network element drivers (NEDs) to mediate access to both Cisco and more than 150 non-Cisco physical and virtual devices.
    
The three-phase approach makes it easier to get to full DevOps environment in a more controllable manner:
	- Phase 1: Use NSO as a programmable network interface.
    	- Use NSO to provide a single API into the network. Operations gains a network provisioning and configuration power tool, with the ability to perform networkwide command-line interface (CLI) and configuration changes from a single interface, in a single transaction, instead of having to individually touch multiple boxes and use different, device-specific commands.
	- Phase 2: Use NSO for service abstraction.
    	- NSO draws on device and service models to begin more fully automating service activations and changes. You see an end-to-end view of the service as a whole, instead of just seeing the individual device configurations.
	- Phase 3: Use NSO for DevOps infrastructure automation.
    	- As you make the people and process changes to support agile development and Continuous Integration/Continuous Delivery (CI/CD), NSO can support that change by enabling everyone that is involved in the service—product developers, network engineers, provisioning and operations teams—to work together to design and execute new services and changes, quickly and continuously.

Cisco Compute Management Platforms
----------------------------------
Cisco provides three options for managing Cisco compute solutions such as Cisco Unified Computing System (UCS) and Cisco HyperFlex: Cisco UCS Manager, Cisco UCS Director and Cisco Intersight.

There are three levels of management power to deploy on Cisco Unified Computing Platforms:
    - Cisco UCS Manager 	for simple infrastructure management
    - Cisco UCS Director 	for more powerful infrastructure orchestration
    - Cisco Intersight 		for cloud-based management of Cisco UCS and Cisco HyperFlex

Cisco UCS Manager:
    - Automates and treats infrastructure as code to improve agility.
    - Unifies management of Cisco UCS blade and rack servers, Cisco UCS Mini, and Cisco HyperFlex.
    - Speeds up daily operations and reduces risks with policy-driven, model-based architecture.
    - Truly single pane of glass API.
    - Automatic hardware discovery.
    - Flexible, programmable, policy driven setup.
    - Policy driven firmware maintenance.
    - Service profile enabling agile, flexible, simple hardware provisioning, management, monitoring, and maintenance.
    - Cisco UCS Central utilizes Cisco UCS Manager APIs to enable orchestration of multiple Cisco UCS domains.

Cisco UCS Director:
    - Provides the foundation for infrastructure as a service (IaaS), including a self-service portal for end users
    - Supported by independent hardware and software vendors through open APIs
    - Operates across infrastructure stacks in the data center, edge scale, and Mode 2 environments globally

Cisco Intersight:
    - Cloud-hosted management for Cisco UCS and Cisco HyperFlex.
    - Simplifies systems management across data center, remote office/branch office (ROBO), and edge environments.
    - Unique recommendation engine delivers actionable intelligence.
    - Tight integration with Cisco Technical Assistance Center (TAC) makes support easier.

Cisco UCS Central Software provides the following advantages to help make operations and analysis easier compared to a single Cisco UCS Manager
    - Unified control plane for all the elements in the system: centralized logs for compute, network, and storage
    - Single source of truth accessible to tools via API
    - Centralizes global policies, service profiles, inventory, ID pools, and templates for up to 6.000 servers

Cisco UCS Central builds on top of Cisco UCS Manager:
  - Unified control plane for all the elements in the system:
    - Centralized logs for compute, network, and storage
    - Single source of truth accessible to tools via API
  - Centralizes global policies, service profiles, inventory, ID pools and templates for up to 10,000 servers.

Cisco Intersight
The Cisco Intersight software as a service (SaaS) platform makes systems management smarter and simpler. Intelligence and automation make daily activities easier and more efficient. Cisco Intersight delivers efficiency of operations for Cisco UCS, HyperFlex, and third-party infrastructure from the data center to the edge.

Cloud-based data center management:
    - Global, multisite, data center, edge
    - Recommendation engine
    - Real-time analytics and machine learning
    - Forecasting

DevOps enabled:
    - Continuous integration and delivery
    - Continuous monitoring
    - OpenAPI Specification (OAS)
    - Python and PowerShell SDK
    
Open API
    Cisco Intersight includes an API that supports the OpenAPI Specification (formerly known as the Swagger Specification), a powerful definition format to describe RESTful APIs. Support for the OpenAPI specification provides users with access to an interoperable REST API with tools that automate the generation of the Intersight API documentation (intersight.com/apidocs), API schemas, and SDKs. The Intersight API includes fully functional Python and PowerShell SDKs.  The API is an integral part of the broader open connector framework Cisco has established to enable the Intersight ecosystem to evolve. The ecosystem will eventually support a wide range of Cisco and third-party DevOps software.

Cisco Compute Management APIs
Cisco offers a range of the APIs for their compute (data center) portfolio, they can be divided into multiple groups:
    - Cisco Unified Computing System (UCS) Management: Intersight, Cisco UCS Manager, Cisco UCS Director
    - User Integrations: Ruby, PowerShell, Python
    
This technology supports unified, model-based management of the system. All properties and configurations of this infrastructure are programmable through Cisco UCS Manager, including the following:
    - MAC addresses and UUIDs
    - Firmware revisions
    - BIOS and Redundant Array of Independent Disks (RAID) controller settings
    - Host bus adapter (HBA) and network interface card (NIC) settings
    - Network security
    - Quality of service (QoS)

PowerTool Suite
Cisco UCS Manager PowerTool Suite is a library of Microsoft PowerShell Cmdlets that allow you to retrieve and manipulate Cisco UCS Manager-managed objects. The Cisco UCS Manager API interactions can be categorized in several distinct sections:
    - Sessions
    - Methods
    - Queries and Filters
    - Configurations and Transactions
    - Event Subscription
    
Cisco UCS Director offers the following APIs:
    - Custom task development API
    - Cisco UCS Director REST API
	
The Custom task development API uses CloupiaScript scripting language.
With Cisco UCS Director REST API you can:
    - Execute Cisco UCS Director workflows
    - Modify switch configuration
    - Modify adapter configuration
    - Modify policies

Cisco Intersight REST API supports the following methods:
    - GET
    - POST
    - PATCH
    - DELETE
All the data from the application is represented in what is called the Cisco Intersight Management Information Model. Some examples of application areas that you can manage with the API include the following:
    - Cisco UCS Servers
    - Server components such as DIMMs, CPUs, GPUs, storage controllers, and Cisco IMC
    - Cisco UCS Fabric Interconnects
    - Firmware inventory
    - Cisco HyperFlex nodes and HyperFlex clusters
    - Virtual machines
    - VLANs and virtual storage area networks (VSANs)
    - Users, roles, and privileges

Cisco Collaboration Platforms
-----------------------------
Cisco offers various solutions for collaboration. The more traditional telephony-based solutions and the more modern collaboration solutions to support online meetings and teamwork.
On premise managed collaboration platforms:
    Cisco Unified Communication Manager:
        - Desktop phones
        - PC
        - Smart phone apps
    Cisco Finesse:
        - Browser-based agents for call centers
    Cloud managed collaboration platforms:
        - Cisco Webex Meetings:
            - Dedicated collaboration devices
            - PC
            - Smart phones
    Cisco Webex Teams:
        - Dedicated collaboration devices
        - PC
        - Smart phones
        
Cisco Finesse provides the following:
    - An agent and supervisor desktop that integrates traditional contact center functions into a thin-client desktop.
    - A 100-percent browser-based desktop implemented through a web 2.0 interface; no client-side installations required.
    - A single, customizable "cockpit," or interface, that gives customer care providers quick and easy access to multiple assets and information sources.
    - Open web 2.0 APIs that simplify the development and integration of value-added applications and minimize the need for detailed desktop development expertise.

Cisco Webex
-----------
Cisco Webex brings together several enterprise solutions for video conferencing, online meetings, screen share, and webinars:
    - Webex Meetings is a multi-platform video conferencing solution.
    - Webex Teams is a collaboration solution for continuous teamwork with video meetings, group messaging, file sharing, and white boarding.
    - Webex Devices improve team collaboration and the Webex Meetings and Webex Teams experience.
	
The following options are available for Cisco Webex Meetings and Cisco Webex Teams:
    - Cisco Webex Board is an all-in-one whiteboard, wireless presentation screen, and video conferencing system for smarter team collaboration.
    - Cisco Webex Room Devices are intelligent video conferencing devices for meeting rooms of all sizes.
    - Cisco Webex Desk Devices are simple-to-use and compact video conferencing devices designed for desktops.
    - Software running on PC.
Cisco Webex Meetings provides a meetings XML API to integrate Webex Meetings services with your custom web portal or application.
Cisco Webex Teams API to create bots, embed videos, or programmatically create teams.

Cisco Collaboration APIs
------------------------
Cisco offers a wide range of APIs for their collaboration portfolio. Collaboration portfolio and APIs can be divided into multiple groups:
    - Cloud collaboration: Apple IOS, Cisco Webex Teams, Cisco Webex Meetings
    - On-prem collaboration: Cisco Meeting Server
    - Contact center: Cisco Unified Contact Center Express, Finesse, Cisco SocialMiner, Cisco Remote Expert Mobile
    - Contact center enterprise: Computer telephony integration (CTI) Server Protocol, Cisco Computer Telephony Integration Operating System (CTI OS)
    - Audio and video endpoints: Room Devices, Jabber Web SDK, Cisco Jabber Bots SDK
    - Call control: Java Telephony Application Programming Interface (JTAPI), Cisco Telephony Application Programming Interface (TAPI), Cisco Unified Communications Manager Session Initiation Protocol (SIP), Cisco WebDialer, Cisco Unified Routing Rules XML Interface (CURRI)
    - Management: Administrative XML (AXL), User Data Services (UDS), Cisco Emergency Responder
    - Cloud calling: Broadworks, Cisco Hosted Collaboration Solutions (HCS)
    - Instant messaging (IM) & presence: Cisco UC Manager IM & presence
    - Voicemail: Cisco Unity Connection
    
Cisco Unified Communications Manager
------------------------------------
Cisco Unified Communications Manager exposes multiple APIs that can be used by developers to interact with Cisco Unified Communications Manager server and its users and devices:
    - Administrative XML (AXL) API
    - Cisco Emergency Responder API
    - Platform Administrative Web Services (PAWS) API
    - Cisco Unified Communications Manager Serviceability XML (SXML) API
    - User Data Services (UDS) API
	
AXL API:
    - XML/Simple Object Access Protocol (SOAP)-based API
    - Manage Cisco Unified Communications Manager configuration
AXL API uses basic authentication:
    Create new application user in Cisco Unified Communications Manager admin page
AXL API can be used to do the following:
    - Configure devices and directory numbers
    - Configure users
    - Configure voicemail
The AXL API is a XML/Simple Object Access Protocol (SOAP) based API that provides a mechanism for managing configuration of the Cisco Unified Communications Manager. Developers can use AXL to create, read, update, and delete objects such as gateways, users, devices, and much more. Examples of Cisco Unified Communications Manager objects that can be provisioned with AXL include:
    - Cisco Unified Communications Manager groups
    - Call Pickup groups
    - Device pools
    - Dial plans
    - Directory numbers
    - Locations
    - Phones
    - Regions
    - Users
    - Voicemail ports
To authenticate a user, use an end-user account created by the Cisco Unified Communications Manager administrator:
    - It is recommended that a user and group for your application is created, rather than using the admin user.
    - Create a special application user for AXL access.
    - Create a user group for AXL access.
    - Put the AXL user in this user group.
	
The UDS API 
----------
is a REST-based set of operations that provide authenticated access to user resources and entities such as user devices, subscribed services, and much more from the Cisco Unified Communications Manager database. UDS is designed for web applications and can run on any device.
Actions that can be accomplished by using the UDS service include:
    - Directory search for users
    - Manage call forward, do not disturb
    - Set language and locale
    - Subscribe to IP phone service applications
    - Reset PIN/password credentials
    - Configure remote destinations
    
Cisco Finesse
-------------
Cisco Finesse offers multiple REST APIs:
    - Cisco Finesse desktop APIs
    - Cisco Finesse configuration APIs
    - Cisco Finesse serviceability APIs
    - Cisco Finesse notifications
Cisco Finesse desktop APIs are used by agents and supervisors to communicate between the Cisco Finesse desktop, Cisco Finesse server, Cisco Unified Contact Center Enterprise, or Cisco Unified Contact Center Express, to send and receive information about the following entities:
    - Agents and agent states
    - Calls and call states
    - Teams
    - Queues
    - Client logs
Cisco Finesse desktop APIs use basic authentication to authenticate with Cisco Unified Contact Center server.
Cisco Finesse configuration APIs allow the configuration of these entities:
    - System, cluster, and database settings
    - Finesse desktop and call variable layout
    - Reason codes and wrap-up reasons
    - Phonebooks and contacts
    - Team resources
    - Workflow and workflow actions
Cisco Finesse configuration APIs require administrator credentials (the application user ID and password) to be passed in the basic authorization header. If a user repeatedly passes an invalid password in the basic authorization header, Cisco Finesse blocks user access to all configuration.
Cisco Finesse serviceability APIs allow users to perform the following actions:
    - Get system information: Status of the Cisco Finesse system (includes the deployment type), installed licenses, system auth mode, and other information.
    - Diagnostic information: Performance information and product version.
    - Run-time information:   Number of logged-in agents, active tasks, and active dialogs.
    
Cisco Webex Teams
Cisco Webex Teams APIs allow you to do the following:
    - Create conversational bots
    - Embed video calls
    - Manage spaces
Cisco Webex Teams exposes a REST API that can be used by developers to interact with Cisco Webex Teams. Before using a REST API, you must create a Cisco Webex Teams developer account, where you will get an access token, which you can later use when invoking API calls towards Cisco Webex Teams. This access token should be used only for development and testing purposes and not for production usage.
Cisco Webex Teams REST APIs can manage the following:
    - Rooms
    - People
    - Memberships
    - Messages
When making requests to the Cisco Webex REST API, an authentication HTTP header is used to identify the requesting user. This header must include an access token. This access token may be a personal access token, a Bot token, or an OAuth token. Cisco Webex Teams APIs support the following methods:
    - GET
    - POST
    - PUT
    - DELETE
For methods that accept request parameters, the platform accepts either application/json or application/x-www-form-urlencoded content type and application/json as a return type.

Cisco Webex Meetings
Cisco Webex Meetings APIs provide developers the ability to include Webex Meetings functionality into their custom applications:
    - REST API
    - XML API: Complete suite of Cisco Webex functions such as scheduling, user management, recording management, attendee management, reports, and more.
    - URL API: An HTML form-based set of functions that allow developers to offer basic Cisco Webex Meetings functionality from their custom web portals.
    - Teleconference service provider (TSP) API: It provides external TSPs to integrate their teleconferencing service with Cisco Webex Meetings.
With Cisco Webex XML API you can manage the following services (entities):
    - User service: Authenticate, create, delete, and update users.
    - General session service: Create contacts, get API versions, get session information, and list open sessions.
    - Meeting service: Create meetings, create teleconference sessions, delete meetings, and get meetings.
    - Training session service: Check lab availability, create training session, delete training session, and get lab info.
    - Event session service: Create and delete an event, send an invitation email, and upload an event image.
    - Support session service: Create a support session and get feedback info.
    - History service: List the event attendee and event session history.
    - Site service: Get site, list time zone, and set site.
    - Meeting attendee service: Create meeting attendee, get enrollment info, and list meeting attendees.
    - Meeting type service: Get and list meeting type.
With Cisco Webex URL API, you can manage the following services (entities):
    - Manage user accounts: Create a new user account, edit an existing user account, and activate and deactivate user accounts.
    - Login/logout: Use an authentication server to login or logout from Cisco Webex Meetings hosted websites.
    - Managing meetings: Schedule a meeting, edit a meeting, list all scheduled meetings, list all open meetings, and join an open meeting.
    - Modifying my Webex meetings page: Modify user information and manage a user contact list.
    - Using attendee registration forms: Create a registration form, determine current required, optional, or do-not-display settings for a registration page.
    - Managing attendee lists: Add attendees to a list of invited users and remove attendees from a list of invited users.
    - Playing back a recorded event: Allow an attendee to get a list of recorded events for playback.
    - Reporting: Send email notifications with attendee information.
The xAPI supports these protocols:
    - Serial port
    - SSH
    - HTTP
    - WebSocket
The xAPI supports these request formats:
    - Text
    - XML
    - JSON
    
Cisco Security APIs
-------------------
Most Cisco security products provide APIs to enable users to enhance the provided functionality or embed the products into larger automated solutions reducing the need for manual actions.
Security use cases using various security products:
    - Cisco FirePower for firewalling
    - Cisco Umbrella for endpoint protection
    - Cisco Advanced Malware Protection for endpoint protection
    - Cisco Threat Grid for threat intelligence
	
Cisco Firepower
There are three Cisco FirePower devices that provide an API:
    - Firepower Threat Defense (FTD): A Rest API that automates configuration management and execution of operational tasks on Cisco Firepower Threat Defense (FTD) devices.
    - Firepower Management Center (FMC): Context-rich APIs for exchange of network and endpoint security event data and host information.
    - FXOS Firepower Chassis Manager: REST API for the Firepower 9300 Chassis. It includes both Configuration and Monitoring APIs for Platform and Firepower Chassis Services.

Cisco FTD API Use Cases
You can use the FTD API to perform these tasks:
Configure policy and settings:
    - Manage policy objects
    - Manage firewall policy
    - Manage device settings
Configure logging and cloud integrations:
    - Configure syslog logging (IPS, file/malware, connection)
    - Configure connections to the cloud
    - Configure smart licensing
Automate your device configuration:
    - Ansible Automation
    - Python or programming language of your choice
    - Industry standard OAuth authentication
    
Cisco Network Management Platforms in Cloud 
Another way of managing your infrastructure is to make use of cloud-based management tools, which further simplify the solution by not even requiring to maintain the management product. There are two Cisco products that use cloud-based management: Meraki and SD-WAN.

Meraki advantages:
    - Provides speed and agility
    - Simplified infrastructure management
    - Using or planning to introduce WAN circuits such as Cable, DSL, Broadband, 4G/LTE
    - Any SD-WAN project with low to medium routing complexity
    - Mobile device management
	
SD-WAN advantages:
    - More powerful options
    - Traffic segmentation
    - Using existing Cisco infrastructure
    - Needed with high routing complexity
	
Cisco Meraki
    - Cisco Meraki is a complete cloud-managed networking solution:
    - Wireless, switching, security, and mobile device management (MDM), centrally managed over the web.
    - Built from the ground up for cloud management.
    - Integrated hardware, software, and cloud services.

The Meraki Dashboard API is an interface for software to interact directly with the Meraki cloud platform and Meraki managed devices. The API contains a set of tools that are known as endpoints for building software and applications that communicate with the Meraki Dashboard for use cases such as provisioning, bulk configuration changes, monitoring, and role-based access controls. The Dashboard API is a modern, RESTful API using HTTPS requests to a URL and JSON as a human-readable format. The Dashboard API is an open-ended tool can be used for many purposes, and here are some examples of how it is used today by Meraki customers:
    - Add new organizations, admins, networks, devices, VLANs, Service Set Identifiers (SSIDs)
    - Provision thousands of new sites in minutes with an automation script
    - Automatically onboard and off-board new employees' teleworker device
    - Build your own dashboard for store managers, field techs, or unique use case

Cisco SD-WAN
    Through the Cisco SD-WAN vManage console, you can quickly establish an SD-WAN overlay fabric to connect data centers, branches, campuses, and colocation facilities to improve network speed, security, and efficiency. After setting templates and policies, Cisco SD-WAN analytics identifies connectivity and contextual issues to determine optimal paths for users to get to their destination, regardless of their connectivity.

    Whether hosted in the cloud or on premises, Cisco vBond and vSmart orchestration and controller platforms authenticate and provision network infrastructure, verifying that the devices connecting to your SD-WAN are authorized. Once connected, SD -WAN platforms find the best path to bring users closer to the applications they need, managing overlay routing efficiency, adjusting in real time to reflect policy updates, and handling key exchanges in Cisco full-mesh, encrypted delivery.

    Cisco SD-WAN supports third-party API integration, allowing for even greater simplicity, customization, and automation in day-to-day operations. In addition, Cisco SD-WAN includes the common routing protocols that are critical for all enterprise SD-WAN deployments, such as Border Gateway Protocol (BGP), Open Shortest Path First (OSPF), Virtual Router Redundancy Protocol (VRRP) and Internet Protocol version 6 (IPv6).
    
    Through a single dashboard called vManage, Cisco SD-WAN provides:
        - Transport independence: 
        Guaranteeing zero network downtime, Cisco SD-WAN automates application flexibility over multiple connections, such as the Internet, Multiprotocol Label Switching (MPLS), and wireless 4G Long Term Evolution (LTE).
        - Network services: 
        Rich networking and security services are delivered with a few simple clicks. WAN optimization, cloud security, firewalling, IPS, and URL filtering can be deployed wherever needed across the SD-WAN fabric from a single location.
        Endpoint flexibility: 
        Cisco SD-WAN can simplify connectivity across branches, campuses, data centers, or cloud environments, extending the SD-WAN fabric wherever you need it to go. Whether physical or virtual, various Cisco SD-WAN platforms give you unparalleled choice, ensuring that your specific business needs are met.

Automating Cisco Network Operations:
------------------------------------
The value of network programmability, and use cases, suggest possibilities for network programmability solutions. Network automation is used for various common tasks. Several of the most common are:
    - Device provisioning: 
    Device provisioning is likely one of the first things that comes to engineer's minds when they think about network automation. Device provisioning is simply configuring network devices more efficiently, faster, and with fewer errors because human interaction with each network device is decreased.
    - Device software management: 
    Controlling the download and deployment of software updates is a relatively simple task, but it can be time-consuming, and may fall prey to human error. Many automated tools have been created to address this issue, but they can lag behind customer requirements. A simple network programmability solution for device software management is beneficial in many environments.
    - Compliance checks: 
    Network automation methods allow the unique ability to quickly audit large groups of network devices for configuration errors and automatically make the appropriate corrections with built-in regression tests.
    - Reporting: 
    Automation decreases the manual effort that is needed to extract information and coordinate data from disparate information sources to create meaningful and human readable reports.
    - Troubleshooting: 
    Network automation makes troubleshooting easier by making configuration analysis and real-time error checking very fast and simple even with many network devices.
    - Data collection and telemetry: 
    A common part of effectively maintaining a network is collecting data from network devices and telemetry on network behavior. Even the way data is collected is changing as now many devices can push data (and stream) off box in real time in contrast to being polled in regular time intervals.
    
Automating Device Operational Lifecycle
---------------------------------------
Network engineers are tasked with deploying, operating, and monitoring the network as efficiently, securely and reliably as possible.
    - Day 0 Install
    The first challenge is getting a device onto the network. This part is commonly referred to as "Day 0" device onboarding. The key requirement is to get the device connected with as little effort as possible. Depending on the operational mode and security requirements, either a small subset of the configuration or a "full" initial configuration will be deployed.
    - Day 1 Configure and Operate
    Once the device is provisioned, the configuration of the device needs to be maintained and upgraded. "Day 1" device configuration management is responsible for the ongoing configuration. Changes to the configuration need to be reliable, efficient, and auditable.
    - Day 2 Optimize
    The next challenge is monitoring the network and measuring performance. "Day 2" device monitoring provides a view of how the network is operating. Based on this view, some changes may be required.
    - Day N Upgrade
    Lastly, optimizations to the device are made, such as extensions to the device capabilities, enhancements to operational robustness, or patches and software upgrades. Referred to as "Day n," it implies an iterative approach, looping back through Days 0, 1, and 2.

Cisco IOS XE Device-level APIs
Cisco IOS XE is an intent-based network operating system:
    - Optimized for enterprise networks
    - Wired and wireless access, aggregation, core, and WAN
    - Open and flexible
    - Standards-based APIs

Cisco IOS XE Operational Approaches
There are three operational approaches to programmatically integrate a network element:
    - Via a controller such as Cisco DNA Center
    - Via a configuration management tool (that is, DevOps)
    - Directly to the device
Each comes with various benefits and trade-offs. However, the Cisco IOS XE Software has been designed to enable all three integration options.

Cisco NX-OS Device-Level APIs
Programmability Options on Cisco Nexus Devices
Programmability Options on Cisco Nexus devices include a few different options.
    - On Board Python
    - EEM
    - NX-API CLI and REST
    - NX-SDK = Currently N9K only
    - NX-Toolkit = Currently N9K/N3K only
    - Guest Shell and Bash
    - Configuration Management Tools (Puppet/Chef/Ansible)

On Board Python
Cisco Nexus Series Switches have various APIs to enhance off box scripting capabilities, but there is also the ability to run Python scripts directly on each switch as well. There is a native Python execution engine that allows you to use the dynamic Python interpreter directly from the switch command line. Also, you can also run standalone scripts from the command line.

On Board Python characteristics include:
Use it for event-based activity, where polling may not be possible
Python on Cisco Nexus is useful for automating tasks
    - CLI commands
    - Generate syslogs
    - Process information and act upon it quickly
Integrate with EEM, Scheduler—get some data from the box and work on it

Embedded Event Manager
EEM monitors events that occur on your device and takes action to recover or troubleshoot these events, based on your configuration. EEM has the following characteristics:
    - A subsystem to automate tasks and customize the device behavior
           - Event > Notification > Action
    - Many built-in system policies
    - Useful for collecting more data and debugging issues especially when unpredictable
    - Can be scheduled at a specific time or intervals
Example of EEM-High CPU
    event manager applet HIGH-CPU
      even snmp oid 1.3.6.1.4.1.9.9.109.1.1.1.1.6.1 get-type exact entry-op ge entry-val 60 poll-interval 1
        action 1.0 syslog msg High CPU hit $_event_pub_tum
        action 2.0 cli enable
        action 3.0 cli show clock >> bootflash:high-cpu.txt
        action 4.0 cli show proc cpu sort >> bootflash:high-cpu.txt
        action 5.0 cli local python bootflash:my_python.py
EEM consists of three major components:
    - Event statements: Events to monitor from another Cisco NX-OS component that might require some action, workaround, or notification.
    - Action statements: An action that EEM can take, such as sending an email, or disabling an interface, to recover from an event.
    - Policies: An event paired with one or more actions to troubleshoot or recover from the event.
EEM feature can be used to repeatedly execute scripts on a given schedule—possibly to examine interface counters or cyclic redundancy check (CRC) errors, or you can even use EEM to dynamically execute a script when a given CLI command is executed as shown in the figure.

Cisco Contollers APIs
The software-defined networking (SDN) architecture slightly differs from the architecture of traditional networks. The different software programs represented in the SDN architecture are composed of three stacked layers called the infrastructure layer, control layer, and application layer:
    - Infrastructure layer: 
        contains network elements (physical and virtual devices that deal with customer traffic).
    - Control layer: 
        represents the core layer of the SDN architecture. It contains SDN controllers, which provide centralized control of the devices in the infrastructure layer.
    - Application layer: 
        contains SDN applications that communicate their network requirements towards the controller.
The SDN controller uses APIs to communicate with the applications and infrastructure layer. Communication with the infrastructure layer is defined with the southbound interfaces, while services are offered to the application layer using the northbound interfaces.
Solutions that contain an SDN controller:
    - Cisco Meraki
    - Cisco Digital Network Architecture Center
    - Cisco application-centric infrastructure (ACI)
    - Cisco software-defined WAN (SD-WAN)
    - Cisco Network Services Orchestrator (NSO)

Cisco Meraki
Cisco Meraki Cloud-Based Platform is a solution that offers customers a single user interface to manage all of their Meraki supported devices. When a new device from the Meraki product family is installed on the network, it requires minimal to no efforts to connect it to the management platform. The web user interface has very limited options that can be configured compared to the Catalyst product family making it simpler to deploy and manage. After the device comes online, it initiates a Secure Sockets Layer (SSL) tunnel to the management platform letting it know that it is available to be claimed and configured. From there, the device will be managed via the cloud-based management platform.
Cisco Meraki offers a wide portfolio of API capabilities. It offers a Representation State Transfer (REST) API with which you can:
    - Retrieve data about the Meraki infrastructure
    - Configure Meraki devices
    - Cisco Meraki offers five primary types of APIs:
    - Dashboard API
    - Scanning API
    - mV Sense API
    - External Captive Portal API
    - Wireless Health API
Dashboard API provides methods to interact directly with the Meraki cloud platform and Meraki managed devices. Using the API, some of the use cases are the following:
    - Add new organizations, admins, networks, devices, VLANs, and more
    - Onboard and off-boards employees
    - Build your dashboard for store managers or fields technicians
    
Cisco DNA Center
Cisco DNA Center is the network management and command center for Cisco DNA, an intent-based network for the enterprise. It supports the expression of business intent for network use cases, such as base automation capabilities in the enterprise network.

Cisco DNA Center provides open programmability APIs for policy-based management and security through a single controller. It provides an abstraction of the network, which leads to simplification of the management of network services. This approach automates what has typically been a tedious manual configuration.

The Analytics and Assurance features of Cisco DNA Center provide end-to-end visibility into the network with full context through data and insights. Cisco customers and partners can use the Cisco DNA Center platform to create value-added applications that use the native capabilities of Cisco DNA Center. You can use Cisco DNA Center Intent APIs, Integration Flows, Events and Notification Services, and the optional Cisco DNA Center Multivendor SDK to enhance the overall network experience by optimizing end-to-end IT processes, reducing Total Cost of Ownership (TCO), and developing new value-added networks.
Cisco DNA offers the following REST APIs:
    - Intent API
    - Software Image Management (SWIM) API
    - Plug and Play (PnP) API
    - Operational Tools
    - Authentication API
    - Integration API

The Intent API is a Northbound REST API that exposes specific capabilities of the Cisco DNA Center platform. It provides policy-based abstraction of business intent, allowing you to focus on an outcome to achieve instead of struggling with the mechanisms that implement that outcome. The RESTful Cisco DNA Center Intent API lets you use HTTPS verbs (GET, POST, PUT, and DELETE) and JSON syntax to discover and control your network. Intent API can be divided into multiple groups:
    - Site Hierarchy Intent API: 
        Retrieves site hierarchy with network health information
    - Network Health Intent API: 
        Retrieves network devices by category, with health information on each of the devices returned. Additional request paths retrieve physical and virtual topologies.
    - Network Device Detail Intent API: 
        Retrieves detailed information about devices retrieved by time stamp, MAC address, universally unique identifier (UUID), name, or nwDeviceName. Additional REST request paths allow you to retrieve additional information, such as functional capabilities, interfaces, device config, certificate validation status, values of specified fields, modules, and VLAN data associated with specified interfaces. You can also add, delete, update, or sync specified devices.
    - Client Health Intent API: 
        Returns overall client health organized as wired and wireless categories. It returns detailed information about a single client.

SWIM API enables you to retrieve information about available software images, import images into Cisco DNA Center, distribute images to network devices, and activate images that have been installed on devices.

PnP API enables you to manage PnP projects, settings, workflows, virtual accounts, and PnP-managed devices.

Operational Tools enable you to configure and manage CLI templates, discover network devices, configure network settings, and trace paths through the network. Operational tools can be divided into multiple groups:
    Command Runner API: 
        Enables you to retrieve the keywords of all CLIs that Command Runner accepts, and it lets you run read-only commands on a network device to retrieve its real-time configuration.
    - Network Discovery API: 
        Provides programmatic access to the Discovery functionality of Cisco DNA Center. You can use this API to create, update, delete, and manage discoveries and their associated credentials. You can also use this API to retrieve the network devices that a particular discovery job acquired.
    - Template Programmer API: 
        Enables you to perform Create, Read, Update, and Delete (CRUD) operations on templates and projects that the template programmer uses to facilitate design and provisioning workflows in Cisco DNA Center. You can use this API to create, view, edit, delete, and version templates. You can also add interactive commands to templates, check the contents of templates for syntactical errors or blacklisted commands, deploy templates, and check the status of template deployments.
    - Path Trace API: 
        Simplifies resolution of network performance issues by tracing application paths through the network and providing statistics for each hop along the path. You can use this API to analyze the flow between two endpoints on the network, retrieve the results of a previous flow analysis, summarize all stored flow analyses, or delete a saved flow analysis.
    - Task API: 
        Queries Cisco DNA Center for more information about a specific task that your RESTful request initiated. Often, a network action may take several seconds or minutes to complete, so Cisco DNA Center completes most tasks asynchronously. You can use the Task API to determine whether a task completed successfully; if so, you can then retrieve more information from the task itself, such as a list of devices provisioned.
    - File API: 
        Enables you to retrieve files from Cisco DNA Center; for example, you might use this API to get a software image or a digital certificate from Cisco DNA Center.

All Cisco DNA Center platform REST requests require proof of identity. The Authentication API generates a security token that encapsulates the privileges of an authenticated REST caller. Cisco DNA Center authorizes each requested operation according to the access privileges associated with the security token that accompanies the request.

The role of the Integration API is to allow Cisco DNA Center to connect to other systems. Integration capabilities are part of westbound interfaces. To meet the need to scale and accelerate operations in modern data centers, IT operators require intelligent, end-to-end workflows built with open APIs. The Cisco DNA Center platform provides mechanisms for integrating Cisco DNA Assurance workflows and data with third-party IT Service Management (ITSM) solutions.

Cisco DNA Center—List Devices
The Cisco DNA Center's Intent API is a northbound REST API that provides a consistently structured way to access Cisco DNA Center workflows for automation and assurance. The Intent API is hierarchically structured into functional domains and subdomains. To retrieve a list of devices on the network, you need to examine the Devices subdomain of the Know Your Network domain.

The Devices subdomain enables API clients to perform CRUD operations on the devices in the network. Wide range of parameters for filtering the response are supported, such as hostname, management Ip address, MAC address, and software version.

The complete Intent API reference is available at https://developer.cisco.com/docs/dna-center/api/1-3-1-x/. The documentation provides example requests with detailed descriptions, available parameters, response codes and examples of response data.

One of the most common scenarios is to gather information about all devices connected to the network, that are in this case managed by Cisco DNA Center. You might need that to get a general overview of the operational status for the devices or you require a detailed view of the performance of any device or client over time and from any application context. Another example could include a software image upgrade. In all of these cases, you first need a list of all devices.

Cisco ACI provides programmability for the data center fabric as a whole, including hardware and software devices, by using an integrated protocol and device packages with scripts for third-party devices:
    - Built-in programmability in both software and hardware
    - Entire data center switching infrastructure can be programmed as a single fabric
    - Declarative model enforces desired state
    
Cisco ACI offers different SDKs:
Cobra—Cisco ACI Python SDK:
    - Python implementation of the API
    - Provides native bindings for all the REST functions
    - Objects in Cobra are 1:1 representations of the Management Information Tree (MIT)
    - Provides methods or performing lookups and queries and for object creation, modification, and deletion
    - Offers full functionality, better suited for more complex queries / incorporating L4-7 devices, Initial Fabric Builds and so on
Cisco ACI toolkit:
    - Python libraries for basic configuration of the APIC
    - Exposes a small subset of the Cisco APIC object model
    - Not full functionality as Cobra SDK
Cisco APIC REST to Python adapter (ARYA):
    - Converter for XML/JSON code to Python
    - Often used with API Inspector
ACIrb:
    - Ruby implementation of the Cisco APIC REST API
    - Enables direct manipulation of the MIT through the REST API using standard Ruby language options
Besides REST API, ACI also offers an NX-OS style CLI to configure and manage ACI in a traditional CLI way. Moquery is a CLI Object Model query tool, while Visore is Object Store Browser (GUI).

Note:
When you perform a task in the Cisco APIC GUI, the GUI creates and sends internal API messages to the operating system to execute the task. By using the API Inspector, which is a built-in tool of the Cisco APIC, you can view and copy these API messages. An administrator can replicate these messages to automate key operations, or you can use the messages as examples to develop external applications that will use the API.

The URL format used can be represented as follows:
    - http:// | https://: By default, only HTTPS is enabled.
    - host: This component is the hostname or IP address of the APIC controller, for example, "APIC."
    - :port: This component is the port number for communicating with the APIC controller if a nonstandard port configured.
    - /api/: This component specifies that the message is directed to the API.
    - mo | class: This component specifies the target of the operation as a managed object or an object class.
    - DN: This component is the DN of the targeted managed object, for example, topology/pod-1/node-201.
    - className: This component is the name of the targeted class, concatenated from the package and the class in the context of the package, for example, dhcp:Client is dhcpClient. The className can be defined in the content of a DN, for example, topology/pod-1/node-1.
    - json | xml: This component specifies the encoding format of the command or response body as JSON or XML.
    - ?options: This component includes optional filters, selectors, or modifiers to a query. Multiple option statements are joined by & sign.

You can use the Cobra SDK to manipulate the MIT generally though this workflow:
    - Identify the object to be manipulated.
    - Build a request to read, change attributes, or add or remove children.
    - Commit the changes made to the object.
    
A common workflow for retrieving data using Cobra is as follows:
    - Create a Session Object
    - Log in to Cisco APIC
    - Perform Lookup by Class or DN
In the same fashion, a common workflow for configuring the Cisco ACI Fabric is as follows:
    - Create a Session Object
    - Log in to Cisco APIC
    - Create Configuration Object by first looking for an object by DN or Class and then creating the new object. you also need to reference the parent object when building a configuration object.
    - Create a Configuration Request Object
    - Add your Config Object to the Request
    - Commit
    
Cisco SD-WAN
The SD-WAN software provides a REST API, which is a programmatic interface for controlling, configuring, and monitoring the SD-WAN devices in an overlay network. You access the REST API through the vManage web server.

When you use a program or script to transfer data from a vManage web server or perform operations on the server, you must first establish an HTTPS session to the server. To do it, you send a call to log in to the server with the following parameters:
    - URL to send the request to—Use https://{vmanage-ip-/
    - Request method—Specify a type of request
    - API call input—The input is an application, so for example for the Content Type, specify application/x-www-form-urlencoded
    - API call payload
REST API URL consists of three parts:
    - Server (hostname or IP Address)
    - Resource (location of the data or object of interest)
    - Parameters (details of scope, filter, often optional)

All REST API calls to vManage contains the root /dataservice.
In the vManage REST API, resources are grouped into collections, which are present at the top level of the API. There are multiple categories:
    - Device actions: Manage device actions like reboot, upgrade, lxcinstall
        - Example URI: /device/action/
    - Device inventory: Retrieve device inventory information, including serial numbers
        - Example URI: /system/device/
    - Device configuration: Create feature and device configuration templates, create, and configuring vManage clusters
        - Example URI: /template/
    - Certificate managements: Manage certificates and security keys
        - Example URI: /certificate
    - Monitoring: View status, statistics, and other information about operational services in the overlay network
        - Example URIs: /alarms, /statistics, /event
    - Real-Time monitoring: Retrieve, view, and manage real-time statistics and traffic information
        - Example URIs: /device/app-route/statistics, /device/bfd/status
    - Troubleshooting: Troubleshoot devices, determine the effect of the policy, update software, retrieve software version information
        - Example URIs: /device/action/software, /device/tools/ping
    - Cross-Domain Integration: APIs to integrate with Selsius Digital Access (SDA) and ACI
        - Example URI: /partner

Cisco NSO
Cisco NSO exposes a northbound REST API which can be used by developers to perform operations. It supports the following methods:
    - GET: Retrieve information from Cisco NSO
    - POST: Add/modify configuration on Cisco NSO
    - DELETE: Remove configuration from Cisco NSO
Cisco NSO REST API allows you to manage service and device configurations.
Although REST API is easy to use, you cannot modify data in the Cisco NSO configuration database (CDB) directly. For this purpose two more APIs are available in Cisco NSO:
    - Python API
    - Java API
These APIs are not northbound APIs (cannot be consumed by third-party applications) but are APIs that can be used as a mapping logic between service and device configuration. While configuration templates in Cisco NSO offer only static binding of variables from a service instance, using one of the APIs you can also perform calculations or connect to external system.
Using the NSO Python API, the following Python versions are supported:
    - Python 2.7.5 or later
    - Python 3.4 or later
    
Among the main benefits of deploying Cisco NSO are faster service deployment and the deployment of configuration management systems. Cisco NSO also makes networks more scalable, because new devices can be added and configured with minimal effort. The same is true for device replacement: Devices can be replaced quickly with little to no additional configuration.

Cisco NSO provides rapid deployment of provisioning and configuration management systems (for example: network-wide end-to-end configuration of VPN Service).

Automating Cisco Webex Teams Operations
Chatbot service providers can automatically supply chatbots with access to some of the following information:
    - Member activity
    - Channel information
    - Chatbot information
    - Metadata
    - Statistics
Listed are the benefits of using chatbots in operational workflows:
    - Centralized infrastructure management
    - Conversational approach to operations
    - Tighter collaboration
    - Transparent workflow
A collaboration platform can make bot-based interactions possible by providing a northbound API and implementing event notifications. Cisco Webex Teams assures it by providing a REST API and enabling webhooks.

The Webex Teams SDK has the following functions:
    - Simplifies authentication
    - Provides default arguments
    - Supports automatic pagination and rate-limiting
    - Manages file attachments
    - Provides comprehensive error reporting

There are two types of spaces (rooms):
    - Direct: A private space used by two participants.
    - Group: Space shared by a team. To create a team room, you need to specify a teamId parameter key and value in the power-on self-test (POST) payload.
    
DevNet Developer Resources 
Cisco DevNet is a developer program which provides tools that help you produce applications build on top of Cisco products. DevNet is much more than a simple website, but rather a fully integrated developer program consisting of a website, an interactive developer community, coordinated developer tools, integrated discussion forums, and sandboxes.
Cisco DevNet is a developer program consisting of:
    - DevNet Sandbox
    - DevNet Learning Labs
    - Cisco DevNet GitHub organization
    - DevNet API Information
    - DevNet Support Forums, chat with Cisco Webex Teams, and case-based support
The DevNet developer program includes support for much more than networking. It includes content on automation that pertains to Cisco networking, collaboration, compatibility testing, Internet of Things (IoT), cloud, security, and data center. There is a plethora of content that is geared towards hands-on labs.

One of the most widely used Cisco DevNet resources is DevNet API information page. On these pages, you can find all the APIs that are exposed on Cisco devices.
APIs are grouped into multiple sections:
    - IoT
    - Cloud
    - Networking
    - Data Center
    - Security
    - Mobility
    - Open Source
    - Collaboration Services
DevNet Support options offer multiple types of support for developer who are creating solutions using Cisco APIs:
    - Knowledge Base: Free articles that cover a wide range of topics. It is a good place to check if you question is already answered.
    - Chat Room Support: This resource is always available. You can enter your email and ask the community a question regarding APIs.
    - Forum Support: Free resource to any DevNet member. You need to be logged in to post your questions.
    - Case Support: Developers can open support tickets for a specific technology. These tickets can be purchased and are also provided as a part of the Solution Partner program.

Basic Networking Concepts:
Standards-based, layered models provide several benefits:
    - Make complexity manageable by breaking communication tasks into smaller, simpler functional groups.
    - Define and specify communication tasks to provide the same basis for everyone to develop their own solutions.
    - Facilitate modular engineering, allowing different types of network hardware and software to communicate with one another.
    - Prevent changes in one layer from affecting the other layers.
    - Accelerate evolution, providing for effective updates and improvements to individual components without affecting other components or having to rewrite the entire protocol.
    - Simplify teaching and learning.
Note
Knowledge of layers and the networking functions that they describe assists in troubleshooting network issues, making it possible to narrow the problem to a layer or a set of layers.

Some important characteristics of switches follow:
    High port density: 
        Switches have high port densities: 24-, 32- and 48-port switches operate at speeds of 100 Mbps, 1 Gbps, 10 Gbps 25 Gbps, 40 Gbps and 100 Gbps. Large enterprise switches may support hundreds of ports.
    Large frame buffers: 
        The ability to store more received frames before having to start dropping them is useful, particularly when there may be congested ports connected to servers or other heavily used parts of the network.
    Port speed: 
        Depending on the switch, it may be possible to support a range of bandwidths. Ports of 100 Mbps, 1Gbps, and 10 Gbps are expected, but 40- or 100-Gbps ports allow even more flexibility.
    Fast internal switching: 
        Having fast internal switching using dedicated hardware allows for higher bandwidths: 100 Mbps, 1 Gbps, 10 Gbps, 25 Gbps, 40 Gbps and 100 Gbps.
    Low per-port cost: 
        Switches provide high port density at a lower cost. For this reason, LAN switches can accommodate network designs that feature fewer users per segment. This feature, therefore, increases the average available bandwidth per user.

Ethernet II frame are:
    Preamble: 
        This field consists of 8 bytes of alternating 1s and 0s that are used to synchronize the signals of the communicating computers.
    Destination Address (DA): 
        The DA field contains the MAC address of the network interface card (NIC) on the local network to which the frame is being sent.
    Source Address (SA): 
        The SA field contains the MAC address of the NIC of the sending computer.
    Type: 
        This field contains a code that identifies the network layer protocol.
    Payload: T
        his field contains the network layer data. If the data is shorter than the minimum length of 46 bytes, a string of extraneous bits is used to pad the field. This field is also known as "data and padding".
    FCS: 
        The FCS field includes a checking mechanism to ensure that the frame of data has been transmitted without corruption. The checking mechanism that is being used is the cyclic redundancy check (CRC).

A MAC address is composed of 12 hexadecimal numbers, which means it has 48 bits. There are two main components of a MAC address. The first 24 bits constitute the Organizationally Unique Identifier (OUI). The last 24 bits constitute the vendor-assigned, end-station address.
- 24-bit OUI: 
    The OUI identifies the manufacturer of the NIC. The IEEE regulates the assignment of OUI numbers. Within the OUI, there are 2 bits that have meaning only when used in the destination address field:
        - Broadcast or multicast bit: 
            When the least significant bit in the first octet of the MAC address is 1, it indicates to the receiving interface that the frame is destined for all (broadcast) or a group of (multicast) end stations on the LAN segment. This bit is referred to as the Individual/Group (I/G) address bit.
            - Locally administered address bit: 
            The second least significant bit of the first octet of the MAC address is referred as a universally or locally (U/L) administered address bit. Normally, the combination of the OUI and a 24-bit station address is universally unique. However, if the address is modified locally, this bit should be set to 1.
 - 24-bit, vendor-assigned, end-station address: This portion uniquely identifies the Ethernet hardware.

The procedure below describes a specific example, when PC A sends a frame to PC B, and the switch starts with an empty MAC address table.
Switching Frames Procedure
	- 1 The switch receives a frame from PC A on port 1.
	- 2 The switch enters the source MAC address (of PC A) and the switch port that received the frame into the MAC table.
	- 3 The switch checks the table for the destination MAC address (of PC B). Because the destination address is not known, the switch floods the frame to all the ports except the port on which it received the frame. In this example, both PC B and PC C will receive the frame.
	- 4 The destination device with the matching MAC address (PC B) replies with a unicast frame addressed to PC A.
	- 5 The switch enters the source MAC address of PC B and the port number of the switch port that received the frame into the MAC table. The destination address of the frame (PC A) and its associated port is found in the MAC table.
	- 6 The switch can now forward frames between the source and destination devices (PC A and PC B) without flooding because it has entries in the MAC table that identify the associated ports. Not having to send to other ports is called filtering.
    
Instead, you can use one connection configured as a trunk:
	- Combining many VLANs on the same port is called trunking.
	- A trunk allows the transport of frames from different VLANs.
	- Each frame has a tag that specifies the VLAN that it belongs to.
	- The receiving device forwards the frames to the corresponding VLAN based on the tag information.

A trunk is a point-to-point link between two network devices, such as a server, router and a switch. Ethernet trunks carry the traffic of multiple VLANs over a single link and allow you to extend the VLANs across an entire network. A trunk does not belong to a specific VLAN. Rather, it is a conduit for VLANs between devices. By default, on a Cisco Catalyst switch, all configured VLANs are carried over a trunk interface.
Note: A trunk could also be used between a network device and a server or another device that is equipped with an appropriate trunk capable NIC.

If your network includes VLANs that span multiple interconnected switches, the switches must use VLAN trunking on the connections between them. Switches use a process called VLAN tagging in which the sending switch adds another header to the frame before sending it over the trunk. This extra header is called a tag and includes a VLAN ID (VID) field so that the sending switch can list the VLAN ID and the receiving switch can identify the VLAN that each frame belongs to. The switch does so by using the 802.1Q encapsulation header. IEEE 802.1Q uses an internal tagging mechanism that inserts an extra 4-byte tag field into the original Ethernet frame between the Source Address and Type or Length fields. As a result, the frame still has the original source and destination MAC addresses. Also, because the original header has been expanded, 802.1Q encapsulation forces a recalculation of the original FCS field in the Ethernet trailer, because the FCS is based on the content of the entire frame. It is the responsibility of the receiving Ethernet switch to look at the 4-byte tag field and determine where to deliver the frame.

IP has these characteristics:
	- IP operates at Layer 3 or the network layer of the OSI reference model (network layer) and at the Internet layer of the TCP/IP stack.
	- IP is a connectionless protocol, in which a one-way packet is sent to the destination without advance notification to the destination device. The destination device receives the data and does not return any status information to the sending device.
	- Each packet is treated independently, which means that each packet can travel a different way to the destination.
	- IP uses hierarchical addressing, in which the network identification (ID) is the equivalent of a street, and the host ID is the equivalent of a house or an office building on that street.
	- IP provides service on a best-effort basis and does not guarantee packet delivery. A packet can be misdirected, duplicated, or lost on the way to its destination.
	- IP does not provide any special features that recover corrupted packets. Instead, the end systems of the network provide these services.
	- IP operates independently of the medium that is carrying the data.
	- There are two types of IP addresses: IPv4 and IPv6, the latter becoming increasingly important in modern networks.

The IPv4 header has several fields. First you will learn about these four fields:
	- Service type: Provides information on the desired quality of service
	- TTL: Limits the lifetime of a packet
	- Source address: Specifies the 32-bit binary value that represents the IPv4 address of the sending endpoint
	- Destination address: Specifies the 32-bit binary value that represents the IPv4 address of the receiving endpoint
Note:  The TTL value does not use time measurement units. It is a value between 1 and 255. The packet source sets the value, and each router that receives the packet decrements the value by 1. If the value remains above 0, the router forwards the packet. If the value reaches 0, the packet is dropped. This mechanism keeps undeliverable packets from traveling between networks for an indefinite amount of time.

Other fields in the header include:
	- Version: Describes the version of IP
	- IHL: Internet Header Length (IHL) describes the length of the header
	- Total Length: Describes the length of a packet, including header and data
	- Identification: Used for unique fragment identification
	- Flag: Sets various control flags regarding fragmentation
	- Fragment Offset: Indicates where a specific fragment belongs
	- Protocol: Indicates the upper-layer protocol that is used in the data portion of an IPv4 packet. For example, a protocol value of 6 indicates this packet carries a TCP segment.
	- Header Checksum: Used for header error detection
	- Options: Includes optional parameters
	- Padding: Used to ensure that the header ends on a 32-bit boundary
	
LIRs (Locacl Internet Registry) obtain IP address pools from their regional internet registry (RIRs):
	- African Network Information Center (AfriNIC)
	- Asia Pacific Network Information Center (APNIC)
	- American Registry for Internet Numbers (ARIN)
	- Latin American and Caribbean Network Information Center (LACNIC)
	- Réseaux IP Européens Network Coordination Centre (RIPE NCC)

Pv6 includes several features that make it attractive for building global-scale, highly effective networks:
Larger address space: The expanded address space includes several IP addressing enhancements:
	- It provides improved global reachability and flexibility.
	- A better aggregation of IP prefixes is announced in the routing tables. The aggregation of routing prefixes limits the number of routing table entries, which creates efficient and scalable routing tables.
	- Multihoming increases the reliability of the internet connection of an IP network. With IPv6, a host can have multiple IP addresses over one physical upstream link. For example, a host can connect to several ISPs.
	- Autoconfiguration is available.
	- There are more "plug-and-play" options for more devices.
	- Simplified mechanisms are available for address renumbering and modification.
  - Simpler header: Streamlined fixed header structures make the processing of IPv6 packets faster and more efficient for intermediate routers within the network. This fact is especially true when large numbers of packets are routed in the core of the IPv6 internet.
  - Security and mobility: Features that were not part of the original IPv4 specification, such as security and mobility, are now built into IPv6. IP Security (IPsec) is available in IPv6, allowing the IPv6 networks to be secure. Mobility enables mobile network devices to move around in networks without breaks in established network connections.
  - Transition richness: IPv6 also includes a rich set of tools to aid in transitioning networks from IPv4, to allow an easy, nondisruptive transition over time to IPv6-dominant networks. An example is dual stacking, in which devices run both IPv4 and IPv6.

IPv6 addresses consist of 128 bits and are represented as a series of eight 16-bit hexadecimal fields that are separated by colons. Although upper and lower case are permitted, it is best practice to use lower case for IPv6 representation:

Address representation:
	- Format is x:x:x:x:x:x:x:x, where x is a 16-bit hexadecimal field:
	- Example: 2001:0db8:010f:0001:0000:0000:0000:0acd
	- Leading zeros in a field can be omitted:
		- Example: 2001:db8:10f:1:0:0:0:acd
	- Successive fields of 0 are represented as "::" but only once in an address:
		- Example: 2001:db8:10f:1::acd
Note: The a, b, c, d, e, and f in hexadecimal fields can be either uppercase or lowercase, but it is best practice to use lower case for IPv6 representation.

IPv6 Address Types
	- IPv6 supports three basic types of addresses. Each address type has specific rules regarding its construction and use. These types of addresses are:
	- Unicast: Unicast addresses are used in a one-to-one context.
	- Multicast: A multicast address identifies a group of interfaces. Traffic that is sent to a multicast address is sent to multiple destinations at the same time. An interface may belong to any number of multicast groups.
	- Anycast: An IPv6 anycast address is assigned to an interface on more than one node. When a packet is sent to an anycast address, it is routed to the nearest interface that has this address. The nearest interface is found according to the measure of metric of the particular routing protocol that is running. All nodes that share the same address should behave the same way so that the service is offered similarly, regardless of the node that services the request.
Note:  IPv6 does not support broadcast addresses in the way that they are used in IPv4. Instead, specific multicast addresses (such as the all-nodes multicast address) are used.

Router Functions
Routers have these two important functions:
	- Path determination: Routers use their routing tables to determine how to forward packets. Each router must maintain its own local routing table, which contains a list of all destinations that are known to the router, and information about how to reach those destinations. When a router receives an incoming packet, it examines the destination IP address in the packet and searches for the best match between the destination address and the network addresses in the routing table. A matching entry may indicate that the destination is directly connected to the router or that it can be reached via another router. This router is called the next-hop router and is on the path to the final destination. If there is no matching entry, the router sends the packet to the default route. If there is no default route, the router drops the packet.
	- Packet forwarding: After a router determines the appropriate path for a packet, it forwards the packet through a network interface toward the destination network. Routers can have interfaces of different types. When forwarding a packet, routers perform encapsulation following the OSI Layer 2 protocol implemented at the exit interface.

A router must perform these actions to route data:
	- Identify the destination of the packet: Determine the destination network address of the packet that needs to be routed by using the subnet mask.
	- Identify the sources of routing information: Determine from which sources a router can learn paths to network destinations.
	- Identify routes: Determine sources from which a router can learn paths to network destinations.
	- Select routes: Select the best path to the intended destination.
	- Maintain and verify routing information: Update known routes and the selected route according to network conditions.
If the destination network is directly connected—that is, if there is an interface on the router that belongs to that network—the router already knows which interface to use when forwarding packets. If destination networks are not directly attached, the router must learn which route to use when forwarding packets.
The destination information can be learned in two ways:
	- You can enter destination network information manually, also known as a static route.
	- Routers can learn destination network information dynamically through a dynamic routing protocol process that is running on the router.
The routing information that a router learns is offered to the routing table. The router relies on this table to tell it which interfaces to use when forwarding packets.
A routing table may contain four types of entries:
	- Directly connected networks
	- Dynamic routes
	- Static routes
	- Default routes
	
Static and Dynamic Routing Comparison
There are two ways that a router can learn where to forward packets to destination networks that are not directly connected.
	- Static routing: 
		The router learns routes when an administrator manually configures the static route. The administrator must manually update this static route entry whenever an internetwork topology change requires an update. Static routes are user-defined routes that specify the outgoing interface on the router when packets should be sent to a specific destination. These administrator-defined routes allow a very precise control over the routing behavior of the IP internetwork.
		- Dynamic routing: 
			The router dynamically learns routes after an administrator configures a routing protocol that determines routes to remote networks. Unlike the situation with static routes, after the network administrator enables dynamic routing, the routing process automatically updates the routing table whenever the device receives new topology information. The router learns and maintains routes to the remote destinations by exchanging routing updates with other routers in the internetwork.

Here are the characteristics of static and dynamic routes:
	- Static routes:
		- A network administrator manually enters static routes into the router.
		- A network topology change requires a manual update to the route.
		- Routing behavior can be precisely controlled.
	- Dynamic routes:
		- A network routing protocol automatically adjusts dynamic routes when the topology or traffic changes.
		- Routers learn and maintain routes to the remote destinations by exchanging routing updates.
		- Routers discover new networks or other changes in the topology by sharing routing table information.

Default Gateways
A source host is able to communicate directly (without a router) with a destination host only if the two hosts are on the same subnet. If the two hosts are on different subnets, the sending host must send the data to its default gateway, which will forward the data to the destination. The default gateway is an address on a router (or Layer 3 switch) connected to the same subnet that the source host is on.
Therefore, before a host can send a packet to its destination, it must first determine if the destination address is on its local subnet or not. It uses the subnet mask in this determination. The subnet mask describes which portion of an IPv4 address refers to the network or subnet and which part refers to the host.

The source host first does an AND operation between its own IPv4 address and subnet mask to arrive at its local subnet address. To determine if the destination address is on the same subnet, the source host then does an AND operation between the destination IPv4 address and the source’s subnet mask. This is because it doesn’t know the subnet mask of the destination address, and if the devices are on the same subnet they must have the same mask. If the resulting subnet address is the same, then it knows the source and destination are on the same subnet. Otherwise, they are on different subnets.

If the source and destination devices are on the same subnet, then the source can deliver the packet directly. If they are on different subnets, then the packet must be forwarded to the default gateway, which will forward it to its destination. The default gateway address must have the same network and subnet portion as the local host address; in other words, the default gateway must be on the same subnet as the local host.

TCP/IP Transport Layer Functions
The basic service that the transport layer provides is tracking communication between applications on the source and destination hosts. This service is called session multiplexing, and it is performed by both UDP and TCP. A major difference between TCP and UDP is that TCP can ensure that the data is delivered, while UDP does not ensure delivery.
	- Session Multiplexing
	- Identification of Different Applications
	- segmentation
	- Flow Control
	- Connection-Oriented
	- Reliability

Note: Review of Open Systems Interconnection (OSI) and TCP/IP reference models: The transport layer of the TCP/IP protocol stack maps to the transport layer of the OSI model. The protocols that operate at this layer are said to operate at Layer 4 of the OSI model. If you hear someone use the term "Layer 4," they are referring to the transport layer of the OSI model.

Session Multiplexing
Session multiplexing is the process by which an IP host is able to support multiple sessions simultaneously and manage the individual traffic streams over a single link. A session is created when a source machine needs to send data to a destination machine. Most often, this process involves a reply, but a reply is not mandatory.
Note:  Session multiplexing service provided by the transport layer supports multiple TCP or UDP sessions, and not just one TCP and one UDP session respectively over a single link 

Identifying the Applications
To pass data to the proper applications, the transport layer must identify the target application. TCP/IP transport protocols use port numbers to accomplish this task. The connection is established from a source port to a destination port. Each application process that needs to access the network is assigned a port number that is unique in that host. The destination port number is used in the transport layer header to indicate which target application that piece of data is associated with. The source port is used by the sending host to help keep track of existing data streams and new connections it initiates. The source and destination port numbers are not usually the same.

Segmentation
TCP takes variably sized data chunks from the application layer and prepares them for transport onto the network. The application relies on TCP to ensure that each chunk is broken up into smaller segments that will fit the maximum transmission unit (MTU) of the underlying network layers. UDP does not provide segmentation services. UDP instead expects the application process to perform any necessary segmentation and supply it with data chunks that do not exceed the MTU of lower layers.
Note:  The MTU of the Ethernet protocol is 1500 bytes. Larger MTUs are possible, but 1500 bytes is the normal size.

Flow Control
If a sender transmits packets faster than the receiver can receive them, the receiver drops some of the packets and requires them to be retransmitted. TCP is responsible for detecting dropped packets and sending replacements. A high rate of retransmissions introduces latency in the communication channel. To reduce the impact of retransmission-related latency, flow control methods work to maximize the transfer rate and minimize the required retransmissions.

Basic TCP flow control relies on acknowledgments that are generated by the receiver. The sender sends some data while waiting for an acknowledgment from the receiver before sending the next part. However, if the round-trip time (RTT) is significant, the overall transmission rate may slow to an unacceptable level. To increase network efficiency, a mechanism called windowing is combined with basic flow control. Windowing allows a receiving computer to advertise how much data it is able to receive before transmitting an acknowledgment to the sending computer. Windowing enables avoidance of congestion in the network.

Connection-Oriented Transport Protocol
Within the transport layer, a connection-oriented protocol establishes a session connection between two IP hosts and then maintains the connection during the entire transmission. When the transmission is complete, the session is terminated. TCP provides connection-oriented reliable transport for application data.

Reliability
TCP reliability has these three main objectives:
	- Detection and retransmission of dropped packets
	- Detection and remediation of duplicate or out-of-order data
	- Avoidance of congestion in the network
	
Reliable vs. Best-Effort Transport
The terms reliable and best effort are terms that describe two types of connections between computers. TCP is a connection-oriented protocol that is designed to ensure reliable transport, flow control, and guaranteed delivery of IP packets. For this reason, it is labeled a "reliable" protocol. UDP is a connectionless protocol that relies on the application layer for sequencing and detection of dropped packets and is considered "best effort." Each protocol has strengths that make them useful for particular applications.

					TCP								UDP
					---								---
Conntection Type	Connection-Oriented				Connectionless
Sequencing			Yes								No
Uses				Email, FTP, Web, downloading	Voice, DHCP, TFTP

Reliable (Connection-Oriented)
Some types of applications require a guarantee that packets arrive safely and in order. Any missing packets could cause the data stream to be corrupted. Consider the example of using your web browser to download an application. Every piece of that application must be assembled on the receiver in the proper binary order, or it will not execute. FTP is an application where the use of a connection-oriented protocol like TCP is indicated.

TCP uses a three-way handshake when setting up a connection. You can think of it as being similar to a phone call. The phone rings, the called party says "hello," and the caller says "hello." Here are the actual steps:
	- The source of the connection sends a synchronization (SYN) segment to the destination requesting a session. The SYN segment includes the Sequence Number (or SN).
	- The destination responds to the SYN with a synchronization-acknowledgment (SYN-ACK) and increments the initiator SN by 1.
	- If the source accepts the SYN-ACK, it sends an acknowledgment (ACK) segment to complete the handshake.

Here are some common applications that use TCP:
	- Web browsers
	- Email
	- FTP
	- Network printing
	- Database transactions
	
To support reliability, a connection is established between the IP source and destination to ensure that the application is ready to receive data. During the initial process of connection establishment, information is exchanged about the capabilities of the receiver, and starting parameters are negotiated. These parameters are then used for tracking data transfer during the connection.

When the sending computer transmits data, it assigns a sequence number to each packet. The receiver then responds with an acknowledgment number that is equal to the next expected sequence number. This exchange of sequence and acknowledgment numbers allows the protocol to recognize when data has been lost, or duplicated, or has arrived out of order.

Best Effort (Connectionless)
Reliability (guaranteed delivery) is not always necessary, or even desirable. For example, if one or two segments of a VoIP stream fail to arrive, it would only create a momentary disruption in the stream. This disruption might appear as a momentary distortion of the voice quality, but the user may not even notice. In real-time applications, such as voice streaming, dropped packets can be tolerated as long as the overall percentage of dropped packets is low.

Here are some common applications that use UDP:
	- Domain Name System (DNS)
	- VoIP
	- TFTP

UDP provides applications with best-effort delivery and does not need to maintain state information about previously sent data. Also, UDP does not need to establish any connection with the receiver and is termed connectionless. There are many situations in which best-effort delivery is more desirable than reliable delivery. A connectionless protocol is desirable for applications that require faster communication without verification of receipt.
UDP is also better for transaction type services, such as DNS or DHCP. In transaction type services, there is only a simple query and response. If the client does not receive a response, it simple sends another query, which is more efficient and consumes less resources than using TCP.

TCP Characteristics
Applications use the connection-oriented services of TCP to provide data reliability between hosts. TCP includes several important features that provide reliable data transmission.
TCP can be characterized as follows:
	- TCP operates at the transport layer of the TCP/IP stack (OSI Layer 4).
	- TCP provides application access to the Internet layer (OSI Layer 3, the network layer), where application data is routed from the source IP host to the destination IP host.
	- TCP is connection-oriented and requires that network devices set up a connection to exchange data. The end systems synchronize with one another to manage packet flows and adapt to congestion in the network.
	- TCP provides error checking by including a checksum in the TCP segment to verify that the TCP header information is not corrupt.
	- TCP establishes two connections between the source and destination. The pair of connections operates in full-duplex mode, one in each direction. These connections are often called a virtual circuit because, at the transport layer, the source and destination have no knowledge of the network.
	- TCP segments are numbered and sequenced so that the destination can reorder segments and determine if data is missing or arrives out of order.
	- Upon receipt of one or more TCP segments, the receiver returns an acknowledgment to the sender to indicate that it received the segment. Acknowledgments form the basis of reliability within the TCP session. When the source receives an acknowledgment, it knows that the data has been successfully delivered. If the source does not receive an acknowledgment within a predetermined period, it retransmits that data to the destination. The source may also terminate the connection if it determines that the receiver is no longer on the connection.
	- TCP provides mechanisms for flow control. Flow control assists the reliability of TCP transmission by adjusting the effective rate of data flow between the two services in the session.

Reliable data delivery services are critical for applications such as file transfers, database services, transaction processing, and other applications in which delivery of every packet must be guaranteed. TCP segments are sent by using IP packets. The TCP header follows the IP header and supplies information that is specific to the TCP protocol. Flow control, reliability, and other TCP characteristics are achieved by using fields in the TCP header. Each field has a specific function.

The TCP header is a minimum of 20 bytes; the fields in the TCP header are as follows:
	- Source Port: Calling port number (16 bits)
	- Destination Port: Called port number (16 bits)
	- Sequence Number and Acknowledgment Number: Used for reliability and congestion avoidance (32 bits each)
	- Header Length: Size of the TCP header (4 bits)
	- Reserved: For future use (3 bits)
	- Flags or control bits (9 bits)  --> Important to remember these TCP flags
		- Nonce Sum (NS): Enables the receiver to demonstrate to the sender that segments are being acknowledged.
		- Congestion Window Reduced (CWR): Acknowledge that the congestion-indication echoing was received
		- Explicit Congestion Notification Echo (ECE): Indication of congestion
		- Urgent (URG): This data should be prioritized over other data
		- Acknowledgment (ACK): Used for acknowledgment
		- Push (PSH): Indicates that application data should be transmitted immediately and not wait for the entire TCP segment
		- Reset (RST): Indicates that the connection should be reset
		- Synchronize (SYN): Synchronize sequence numbers
		- Finish (FIN): Indicates there is no more data from sender
	- Window size: Window size value, used for flow control (16 bits)
	- Checksum: Calculated checksum from a constructed pseudo header (containing the source address, destination address, and protocol from the IP header, TCP segment length, and reserved bits) and the TCP segment (TCP header and payload) for error-checking (16 bits)
	- Urgent Pointer: If the URG flag is set, this field is an offset from the sequence number indicating the last urgent data byte (16 bits)
	- Options: The length of this field is determined by the data offset field (from 0 to 320 bits)
	- Data: upper-layer protocol (ULP) data (varies in size)

UDP Characteristics
Applications use the connectionless services of UDP to provide high-performance, low-overhead data communications between hosts. UDP includes several features that provide for low-latency data transmission.

UDP is a simple protocol that provides basic transport layer functions:
	- UDP operates at the transport layer of the TCP/IP stack (OSI Layer 4).
	- UDP provides applications with access to the Internet layer (OSI Layer 3, the network layer), without the overhead of reliability mechanisms.
	- UDP is a connectionless protocol in which a one-way datagram is sent to a destination without advance notification to the destination device.
	- UDP performs only limited error checking. A UDP datagram includes a checksum value, which the receiving device can use to test the integrity of the data.
	- UDP provides service on a best-effort basis and does not guarantee data delivery, because packets can be misdirected, duplicated, or lost on the way to their destination.
	- UDP does not provide any special features that recover lost or corrupted packets. UDP relies on applications that are using its transport services to provide recovery.
	- Because of its low overhead, UDP is ideal for applications like DNS and Network Time Protocol (NTP), where there is a simple request-and-response transaction.

The low overhead of UDP is evident when you review the UDP header length of only 64 bits (8 bytes). The UDP header length is significantly smaller compared with the TCP minimum header length of 20 bytes.
The following list describes the field definitions in the UDP segment:
	- Source port: Calling port number (16 bits)
	- Destination port: Called port number (16 bits)
	- Length: Length of UDP header and UDP data (16 bits)
	- Checksum: Calculated checksum of the header and data fields (16 bits)
	- Data: ULP data (varies in size)
Application layer protocols that use UDP include DNS, Simple Network Management Protocol (SNMP), DHCP, Routing Information Protocol (RIP), TFTP, Network File System (NFS), online games, and voice streaming.

Host-To-Host Packet Delivery
Host-to-host packet delivery consists of an interesting series of processes. In this multipart example, you will discover what happens "behind the scenes" when an IPv4 host communicates with another IPv4 host, first when a router is used and second when a switch is responsible for host-to-host packet delivery process.

Address Resolution Protocol
When a device sends a packet to a destination, it encapsulates the packet into a frame. The packet contains IPv4 addresses, and the frame contains MAC addresses. Therefore, there must be a way to map an IPv4 address to a MAC address. For example, if you enter the ping 10.1.1.3 command, the MAC address of 10.1.1.3 must be included in the destination MAC address field of the frame that is sent. To determine the MAC address of the device with an IPv4 address 10.1.1.3, a process is performed by a Layer 2 protocol called ARP.

ARP provides two essential services:
	- Address resolution: Mapping IPv4 addresses to MAC addresses on a network
	- Caching: Locally storing MAC addresses that are learned via ARP

The term address resolution in ARP refers to the process of binding or mapping the IPv4 address of a remote device to its MAC address. ARP sends a broadcast message to all devices on the local network. This message includes its own IPv4 address and the destination IPv4 address. The message is asking the device on which the destination IPv4 address resides to respond with its MAC address. The address resolution procedure is completed when the originator receives the reply frame, which contains the required MAC address, and updates its table containing all the current bindings.
Note:The Layer 2 broadcast MAC address is FF:FF:FF:FF:FF:FF.

Using ARP to Resolve the MAC of a Local IPv4 Address
Because ARP is a Layer 2 protocol, its scope is limited to the local LAN. If the source and destination devices are on the same subnet, then the source can use ARP to determine the destination’s MAC address.
For example, IPv4 host 10.10.1.241/24 is on the 10.10.1.0/24 subnet. If the host that it wants to communicate with is 10.10.1.175, it knows that this IPv4 host is also on the local 10.10.1.0/24 subnet, and it can use ARP to determine its MAC address directly.

Using ARP to Resolve the MAC of a Remote IPv4 Address
If the source and destination devices are not on the same subnet, then the source uses ARP to determine the default gateway’s MAC address.
For example, when the source host 10.10.1.241 wants to communicate with the destination host 10.10.2.55, it compares this IPv4 address against its subnet mask and discovers that the host is on a different IPv4 subnet (10.10.2.0/24). When a host wants to send data to a device that is on another network or subnet, it encapsulates the packet in a frame addressed to its default gateway. So, the destination MAC address in the frame needs to be the MAC address of the default gateway. In this situation, the source must send an ARP request to find the MAC address of the default gateway. In the example, host 10.10.1.241 sends a broadcast with an ARP Request for the MAC address of 10.10.1.1.

Understanding the ARP Cache
Each IPv4 device on a network segment maintains a table in memory—the ARP table or ARP cache. The purpose of this table is to cache recent IPv4 addresses to MAC address bindings. When a host wants to transmit data to another host on the same subnet, it searches the ARP table to see if there is an entry. If there is an entry, the host uses it. If there is no entry, the IPv4 host sends an ARP broadcast requesting resolution.
Note:  By caching recent bindings, ARP broadcasts can be avoided for any mappings in the cache. Without the ARP cache, each IPv4 host would have to send an ARP broadcast each time it wanted to communicate with another IPv4 host.

Each entry, or row, of the ARP table has a pair of values—an IPv4 address and a MAC address. The relationship between the two values is a map, which simply means that you can locate an IPv4 address in the table and discover the corresponding MAC address. The ARP table caches the mapping for the devices on the local LAN, including the default gateway.

The device creates and maintains the ARP table dynamically, adding and changing address relationships as they are used on the local host. The entries in an ARP table expire after a while; the default expiry time for Cisco devices is 4 hours. Other operating systems (Windows, Mac OS) might have a different value – Windows OS for example uses a random value between 15 - 45 seconds. This timeout ensures that the table does not contain information for systems that may be switched off or that have been moved. When the local host wants to transmit data again, the entry in the ARP table is regenerated through the ARP process.

If no device responds to the ARP request, then the original packet is dropped, because a frame to put the packet in cannot be created without the destination MAC address.  On a Microsoft Windows PC, the arp -a command displays the current ARP table for all interfaces on the PC.  To limit the output of the arp command to a single interface, use the arp -a -N ip_address command.

Network Device Planes 
Network devices implement processes that can be broken down into three functional planes: the management plane, control plane, and data plane. Under normal network operating conditions, the network traffic consists mostly of data plane transit packets. Network devices are optimized to handle these packets efficiently. Typically, there are considerably fewer control and management plane packets.

Network Devices consist of the following tree planes:
	- Management Plane:  Exchange of management information
	- Control Plane:  Exchange of Routing Informaton, Routing protocol table, RIB (routing Information Base), Routing information Process, and Policies
	- Data Planes: Incomming ang forwarding of network traffic (packets, frame, segements, etc)
	
The data plane: 
The primary purpose of routers and switches is to forward packets and frames through the device onward to final destinations. The data plane, also called the forwarding plane, is responsible for the high-speed forwarding of data through a network device. Its logic is kept simple so that it can be implemented by hardware to achieve fast packet forwarding. The forwarding engine processes the arrived packet and then forwards it out of the device. Data plane forwarding is very fast. It is performed in hardware. To achieve the efficient forwarding, routers and switches create and utilize data structures, usually called tables, which facilitate the forwarding process. The control plane dictates the creation of these data structures. Examples of data plane structures are CAM (Content-Addressable Memory) table, Ternary CAM (TCAM) table, Forwarding Information Base (FIB) table, and Adjacency table.

Cisco routers and switches also offer many features to secure the data plane. Almost every network device has the ability to utilize ACLs, which are processed in hardware, to limit allowed traffic to only well-known traffic and desirable traffic.

Note:  Data plane forwarding is implemented in specialized hardware. The actual implementation depends on the switching platform. High-speed forwarding hardware implementations can be based on specialized integrated circuits called ASICs, Field-Programmable Gate Arrays (FPGAs), or specialized Network Processors (NPs). Each of the hardware solutions is designed to perform a particular operation in a highly efficient way. Operations performed by ASIC may vary from compression and decompression of data, or computing and verifying checksums to filter or forward frames based on their MAC address.

The control plane:
consists of protocols and processes that communicate between network devices to determine how data is to be forwarded. When packets that require control plane processing arrive at the device, the data plane forwards them to the device’s processor, where the control plane processes them.

In cases of Layer 3 devices, the control plane sets up the forwarding information based on the information from routing protocols. The control plane is responsible for building the routing table or Routing Information Base (RIB). The RIB in turn determines the content of the forwarding tables, such as the FIB and the adjacency table, used by the data plane. In Layer 2 devices, the control plane processes information from Layer 2 control protocols, such as STP and Cisco Discovery Protocol, and processes Layer 2 keepalives. It also processes info from incoming frames (such as the source MAC address to fill in MAC address table).

When high packet rates overload the control or management plane (or both), device processor resources can be overwhelmed, reducing the availability of these resources for tasks that are critical to the operation and maintenance of the network. Cisco networking devices support features that facilitate control of traffic that is sent to the device processor to prevent the processor itself from being overwhelmed and affecting system performance.

The control plane processes the traffic that is directly or indirectly destined to the device itself. Control plane packets are handled directly by the device processor, which is why control plane traffic is called processed switched.

There are generally two types of process switched traffic. The first type of traffic is directed, or addressed, to the device itself and must be handled directly by the device processor. Examples include routing protocol data exchange. The second type of traffic that is handled by the CPU is data plane traffic with a destination beyond the device itself, but which requires special processing by the device processor. One example of such traffic is IPv4 packets that have a TTL value, or IPv6 packets that have a Hop Limit value of less than or equal to one. They require Internet Control Message Protocol (ICMP) Time Exceeded messages to be sent, which results in CPU processing.

The management plane:
consists of functions that achieve the management goals of the network, which include interactive configuration sessions, and statistics gathering and monitoring. The management plane performs management functions for a network and coordinates functions among all the planes (management, control, and data). In addition, the management plane is used to manage a device through its connection to the network.

The management plane is associated with traffic related to the management of the network or the device. From the device point of view, management traffic can be destined to the device itself or intended for other devices. The management plane encompasses applications and protocols such as Secure Shell (SSH), Simple Network Management Protocol (SNMP), HTTP, HTTPS, Network Time Protocol (NTP), TFTP, FTP, and others that are used to manage the device and the network.

From the perspective of a network device, there are three general types of packets as related to the functional planes:
	- Transit packets and frames include packets and frames that are subjected to standard, destination IP, and MAC based forwarding functions. In most networks and under normal operating conditions, transit packets are typically forwarded with minimal CPU involvement or within specialized high-speed forwarding hardware.
	- Receive, or for-us packets include control plane and management plane packets that are destined for the network device itself. Receive packets must be handled by the CPU within the device processor, because they are ultimately destined for and handled by applications running at the process level within device operating system.
	- Exception IP and non-IP information include IP packets that differ from standard IP packets, for instance, IPv4 packets containing Options field in the IPv4 header, or IPv4 packets with TTL that expires, and IPv4 packets with unreachable destinations. Examples of non-IP packets are Layer 2 keepalives, ARP frames, and Cisco Discovery Protocol frames. All packets and frames in this set must be handled by the device processor.

In traditional networking, the control and data planes exist within one device. With the introduction of software-defined networking (SDN), the management and control planes are separated from the data plane into separate devices. Data plane devices, such as switches and routers, focus on forwarding data. The management and control planes are abstracted into a centralized solution, a specialized network controller, which implements a virtualized, software orchestration to provide network management and control functions.

DHCP:
DHCP server is used to assign IP addresses and IP configuration parameters. Examples of IP configuration parameters that are automatically set by DHCP would be the subnet mask, default router, and DNS servers. This protocol is also used to provide other configuration information that is needed, including the length of time the address has been allocated to the host.

DHCP is built on a client/server model. The DHCP sends configuration parameters to dynamically configured hosts that request them. The term "client" refers to a host that is requesting initialization parameters from a DHCP server.  The DHCP client and the DHCP server exchange the following packets:
	- DHCP Discover: 
		  The DHCP client boots up and sends this message on its local physical subnet to the subnet's broadcast (destination IPv4 address of 255.255.255.255 and MAC address of ff:ff:ff:ff:ff:ff), with a source IPv4 address of 0.0.0.0 and its MAC address.
	- DHCP Offer: 
		  The DHCP server responds and fills the yiaddr (your IPv4 address) field of the message with the requested IPv4 address. The DHCP server sends the DHCP Offer to the broadcast address, but includes the client's hardware address in the chaddr (client hardware address) field of the offer, so the client knows that it is the intended destination.
	- DHCP Request: 
		    The DHCP client may receive multiple DHCP Offer messages, but chooses one and accepts only that DHCP server’s offer, implicitly declining all other DHCP Offer messages. The client identifies the selected server by populating the Server Identifier option field with the DHCP server's IPv4 address. The DHCP Request is also a broadcast, so all DHCP servers that sent a DHCP Offer will receive it, and each will know whether it was accepted or declined. Even though the client has been offered an IPv4 address, it will send the DHCP Request message with a source IPv4 address of 0.0.0.0.
	- DHCP ACK: 
			  The DHCP server acknowledges the request and completes the initialization process. DHCP ACK (acknowledge) message has a source IPv4 address of the DHCP server, and the destination address is once again a broadcast and contains all the parameters that the client requested in the DHCP Request message. When the client receives the DHCP ACK, it enters into the Bound state, and is now free to use the IPv4 address to communicate on the network.

NAT:
NAT may be configured in the following ways:
	- Static NAT: 
		A translation type or entry on the translating device that is statically configured and will always translate between the same pre-NAT and post-NAT addresses. This is commonly used for Servers providing a service on a public network. For example A DMZ Web Server that is accessible from the Internet.
	- Dynamic NAT: 
		A translation type where the Client address is the pre-NAT address and is dynamically given an address from a pre-configured pool of addresses.
	- PAT: Port Address 
		Translation (PAT) will translate the Source IP address of the Clients to a single IP address on the perimeter device. Often this address represents an outside-facing interface. Because the Client addresses are translated to the same address, the Port is also changed to uniquely identify the client's session. This is the most common type of NAT, sometimes also called Network Address and Port Translation (NAPT).

Benefits and Drawbacks of NAT
Here are the benefits of NAT:
	- NAT conserves public addresses by enabling multiple privately addressed hosts to communicate using a limited, small number of public addresses instead of acquiring a public address for each host that needs to connect to internet. The conserving effect of NAT is most pronounced with PAT, where internal hosts can share a single registered IPv4 address for all external communication.
	- NAT increases the flexibility of connections to the public network.
	- NAT provides consistency for internal network addressing schemes. When a public IPv4 address scheme changes, NAT eliminates the need to readdress all hosts that require external access, saving time and money. The changes are applied to the NAT configuration only. Therefore, an organization could change ISPs and not need to change any of its inside clients.
	- NAT can be configured to translate all private addresses to only one public address or to a smaller pool of public addresses. When NAT is configured, the entire internal network hides behind one address or a few addresses. To the outside, it seems that there is only one or a limited number of devices in the inside network. This hiding of the internal network helps provide additional security as a side benefit of NAT.
	
Here are the disadvantages of NAT:
	- End-to-end functionality is lost. Many applications depend on the end-to-end property of IPv4-based communication. Some applications expect the IPv4 header parameters to be determined only at end points of communication. NAT interferes by changing the IPv4 address and sometimes transport protocol port (PAT) numbers at network intermediary points. Changed header information can block applications. For instance, call signaling application protocols include the information about the device's IPv4 address in its headers. Although the application protocol information is going to be encapsulated in the IPv4 header as data is passed down the Transmission Control Protocol (TCP)/IP stack, the application protocol header still includes the device's IPv4 address as part of its own information. The transmitted packet will include the senders IPv4 address twice: in the IPv4 header and in the application header. When NAT makes changes to the source IPv4 address (along the path of the packet), it will change only the address in the IPv4 header. NAT will not change IPv4 address information that is included in the application header. At the recipient, the application protocol will rely only on the information in the application header. Other headers will be removed in the de-encapsulation process. Therefore, the recipient application protocol will not be aware of the change NAT has made and it will perform its functions and create response packets using the information in the application header. This process results in creating responses for unroutable IPv4 addresses and ultimately prevents calls from being established. Besides signaling protocols, some security applications, such as digital signatures, fail because the source IPv4 address changes. Sometimes, you can avoid this problem by implementing static NAT mappings.
	- End-to-end IPv4 traceability is also lost. It becomes much more difficult to trace packets that undergo numerous packet address changes over multiple NAT hops, so troubleshooting is challenging. On the other hand, for malicious users, it becomes more difficult to trace or obtain the original source or destination addresses.
	- Using NAT also creates difficulties for the tunneling protocols, such as IP Security (IPsec), because NAT modifies the values in the headers. Integrity checks declare packets invalid if anything changes in them along the path. NAT changes interfere with the integrity checking mechanisms that IPsec and other tunneling protocols perform.
	- Services that require the initiation of TCP connections from an outside network (or stateless protocols, such as those using User Datagram Protocol [UDP]) can be disrupted. Unless the NAT router makes a specific effort to support such protocols, inbound packets cannot reach their destination. Some protocols can accommodate one instance of NAT between participating hosts (passive mode FTP, for example) but fail when NAT is performed at multiple points between communicating systems, for instance both in the source and in the destination network.
	- NAT can degrade network performance. It increases forwarding delays because the translation of each IPv4 address within the packet headers takes time. For each packet, the router must determine whether it should undergo translation. If translation is performed, the router alters the IPv4 header and possibly the TCP or UDP header. All checksums must be recalculated for packets in order for packets to pass the integrity checks at the destination. This processing is most time consuming for the first packet of each defined mapping. The performance degradation of NAT is particularly disadvantageous for real-time applications, such as Voice over IP (VoIP).
	
Common applications using well-known, registered ports include the following:
	- FTP (port 20 and 21, TCP): 
		FTP is a reliable, connection-oriented service that uses TCP to transfer files between systems that support FTP. FTP supports bidirectional binary and ASCII file transfers. Besides using port 21 for exchange of control, FTP also uses one additional port, 20 for data transmission.
	- SSH (port 22, TCP): 
		Secure Shell (SSH) provides the capability to remotely access other computers, servers, and networking devices over an encrypted connection.
	- Telnet (port 23, TCP): 
		Telnet is a predecessor to SSH. It sends messages in unencrypted cleartext. As a security best practice, most organizations now use SSH for remote communications.
	- SMTP (port 25, TCP): 
		Simple Message Transfer Protocol (SMTP) is used for exchanging emails on the Internet.
	- HTTP (port 80, TCP): 
		HTTP, that defines how messages are formatted and transmitted by web servers, communicates over unencrypted TCP sessions.
	- HTTPS (port 443, TCP): 
		HTTPS provides HTTP over an encrypted connection. Since 2018 it is more often used than HTTP.
	- DNS (port 53, TCP and UDP): 
		Domain Name System (DNS) uses TCP for zone transfer between DNS servers and UDP for resolving Internet names to IP addresses.
	- TFTP (port 69, UDP): 
		Trivial File Transfer Protocol (TFTP) is a connectionless service. Routers and switches use TFTP to transfer configuration files and Cisco IOS images and other files between systems that support TFTP.
	- SNMP (port 161 and 162, UDP): 
		Simple Network Management Protocol (SNMP) facilitates the exchange of management information between network devices.
	- NETCONF (port 830, TCP): 
		Network Configuration Protocol (NETCONF) is a successor to SNMP, using a reliable TCP transport instead of UDP.

Secure Shell
The SSH protocol provides a secure virtual terminal for a user to log in to a remote host and execute commands. SSH uses strong encryption for securing the exchange of information over an insecure network.
The main features of SSH are:
	- Industry-standard management protocol
	- Secure, bi-directional authentication with passwords and/or keys
	- Supports file transfer with scp or sftp command
	- Many Operating Systems have support included
	- Publicly and commercially available clients
SSH can be simply used from the command line (ssh hostname-or-ip) or through GUI clients, such as PuTTY.

Telnet
Telnet protocol was designed to enable remote shell or console sessions across the network. While it was popular years ago, its major disadvantage is unencrypted communication. This allowed hackers to steal credentials. As a security best practice, SSH is now the recommended protocol.
Made obsolete as virtual terminal by SSH, the telnet command is still useful today to test connectivity.
$ telnet cisco.com 80
Trying 72.163.4.185...
Connected to cisco.com.
Escape character is '^]'.
GET / HTTP/1.0

HTTP/1.0 301 Moved permanently
Location: https://www.cisco.com/
Connection: close
Cache-Control: no-cache
Pragma: no-cache

Connection closed by foreign host.
When you see a Connected (or Open) response, you can assume that remote device is reachable and it listens on a given TCP port (80 in the example).
Taking a closer look at the output, you will see it is an HTTP protocol response. Don't you think this is interesting? Telnet uses the TCP session almost as-is (without additional encoding) and this allows us to read raw HTTP messages. While this trick will only work with some services, you can test for an open port with any TCP-based protocol.

SNMP
SNMP consists of these three components:
	- SNMP manager: Polls agents on the network and displays information.
	- SNMP agent: Runs on managed devices, collects device information, and translates it into MIB-compatible format. It also generates traps that are unsolicited messages
	- Management Information Base (MIB): Database of management objects (information variables).

The snmpwalk command is available for Linux systems. It pulls data (e.g. interface names) from the device's MIB tree.

root@NMS:~$ snmpwalk -v 3 -u test-user -l authPriv -a SHA -A auth-pass -x AES -X priv-pass 10.1.10.1 1.3.6.1.2.1.2.2.1.2
IF-MIB::ifDescr.1 = STRING: Ethernet0/0
IF-MIB::ifDescr.2 = STRING: Ethernet0/1
IF-MIB::ifDescr.3 = STRING: Ethernet0/2
IF-MIB::ifDescr.4 = STRING: Ethernet0/3
IF-MIB::ifDescr.5 = STRING: Ethernet1/0
IF-MIB::ifDescr.6 = STRING: Ethernet1/1
IF-MIB::ifDescr.7 = STRING: Ethernet1/2
IF-MIB::ifDescr.8 = STRING: Ethernet1/3
IF-MIB::ifDescr.9 = STRING: Serial2/0
IF-MIB::ifDescr.10 = STRING: Serial2/1
IF-MIB::ifDescr.11 = STRING: Serial2/2
IF-MIB::ifDescr.12 = STRING: Serial2/3

SMTP
SMTP is also known as Simple Mail Transfer Protocol. Simple in its name signifies the protocol design used—a client-server protocol, with features and message types being simple in nature.
SMTP is used for sending email in the following three use cases:
	- Email servers on the Internet exchange email using SMTP protocol.
	- Users use SMTP together with POP and IMAP configured within their email client.
	- Applications use SMTP when handing off email alerts and notifications to the configured email server.

DNS Malfunction
Name resolution issues often manifest as lost host connectivity and authentication failures.
Common problems that contribute to DNS malfunction include:
	- No DNS Server was defined or the incorrect DNS Server was defined
	- Missing or incorrect DNS entry
	- Wrong host name used (e.g. server01.example.net instead of server01.example.com)
	- Invalid DNS Server configuration

Default Gateway Malfunction
The Default Gateway takes on the role of providing the routing function, to get packets off the current network and on to or on its way to the destination network of interest. It is more generally known as a Layer 3 device and does not necessarily need to be a physical router; it could also be a Layer 3 or Multi-Layer Switch, a Firewall, or even a Server with multiple NICs installed. These multiple NICs would be the equivalent to multiple interfaces on a router or firewall.
Note: The name, Layer 3 device, originates from the 3rd (Network) layer of the OSI model.

A source host is able to communicate directly (without a router) with a destination host only if the two hosts are on the same subnet. If the two hosts are on different subnets, the sending host must send the data to its default gateway, which will forward the data to the destination. Once again, the default gateway is an address on a router (or Layer 3 switch) connected to the same subnet that the source host is on.

Therefore, before a host can send a packet to its destination, it must first determine if the destination address is on its local subnet or not. It uses the subnet mask in this determination. The subnet mask describes which portion of an IPv4 address refers to the network or subnet and which part refers to the host.

If the default gateway configuration is incorrect or the default gateway is experiencing problems, then the host or hosts using that gateway are isolated to that network only.

Possible points of failure to investigate would be:
	- Host or Default Gateway with wrong subnet
	- Host or Default Gateway with wrong subnet mask
	- Interface on Default Gateway is not up
	- Default Gateway is missing a route to destination and does not know where to forward traffic
	- Missing default route on Host
Sometimes, even though you configure addressing correctly, there is no connectivity due to a faulty link between devices.
The three main categories of issues are:
	- Hardware failure
	- Software failure (bug)
	- Configuration error (e.g. disabled interface)
	
To verify the interface status, use the show interfaces (or equivalent) command.

Cisco# show interfaces GigabitEthernet0/1
GigabitEthernet0/1 is up, line protocol is up 

Linux$ ip link show eth0
2: eth0: <BROADCAST,MULTICAST,UP> mtu 1500 group default qlen 1
    link/ether 08:00:27:f2:74:be

Link Utilization
Dropped packets due to link oversubscription will cause the following:
	- Decreased bandwidth
	- Increased latency
	- Timeouts
	
Switch# show interface fast 0/10  
FastEthernet0/10 is up, line protocol is up (connected)  
5 minute input rate 12329000 bits/sec, 1707 packets/sec  
5 minute output rate 2933000 bits/sec, 1331 packets/sec

Modern operating systems provide network utilization statistics. For example, on Cisco devices, you can check the current link utilization with the show interface command. From the output, focus on the bit rate, which is displayed in average bits per sec (bits/sec or bps). The interval shown defaults to the last 5 minutes, which can be changed with the load-interval command. Keep in mind this is average bit rate, so there may still be short peaks of fully utilized link, causing packet drops.

NAT Issues
As useful and ubiquitous as NAT may be, it has disadvantages. Changes to address and port values have been known to cause problems with applications and network devices.
	- End-to-end functionality is lost.
		- Port forwarding may be required to establish connections.
	- Address and port values change.
		- Applications should not encode or rely on these values-unlike FTP, which requires Deep Packet Inspection on NAT devices to function.
	- Traceability is lost.
		- You cannot ping hosts behind NAT to verify basic connectivity.
	- Initiating connections can be disrupted.
		- Broken NAT devices may not translate all required packets, such as ICMP 'Datagram too big"
	- Performance is degraded.
		- Many modern NAT devices are stateful, performing additional processing of packets that increases delay, most noticeable on session establishment.

Since hosts behind NAT use private addresses, they cannot be reached directly and require additional configuration of the NAT device to be able to accept new connections. While workarounds exist, they are complex to implement. One such example is NAT-Traversal (NAT-T). NAT-T is a tunneling technique that uses UDP frames to discover if NAT is being performed upstream from the tunnel endpoint, to re-establish end-to-end connectivity, which is required by IPsec. IPsec encrypts whole packets, including IP addresses, and would otherwise break under NAT.

VPN Blocking
It is not too hard for unwary internet users today to fall prey to various computer viruses and malware. This can incur significant cost in lost productivity and assets. Therefore, organizations deploy firewalls with deep packet inspection as a best practice.
As more traffic on the internet is encrypted, so is malware and other unwanted traffic. This makes it harder for network security appliances, such as firewalls, to separate normal and malicious traffic. One option is to use additional end-client protection (antivirus) software, for example, but there are others as well.
Some organizations opt to inspect all traffic in their network-for example to prevent users from downloading malware.
This may interfere with encrypted traffic, like HTTPS or VPN connections
	- Direct connections are blocked; use of a proxy server is required.
	- Traffic is routed through a "next-generation" firewall device, impersonating the destination server and enabling it to decrypt traffic.
In the last case, you may have to install additional root CA or configure certificate pinning on clients.

Certificate pinning instructs the system to trust a given certificate for a specified host, regardless of the certificate signing hierarchy. It is, simply put, a way to manually trust a host's certificate.  These measures are required because, to applications, this setup looks exactly like a man-in-the-middle attack. In turn, applications will refuse connections due to wrong server identity. If you encounter this situation, you will typically see error messages such as server certificate mismatch and the like.

Tools for Troubleshooting Connectivity Issues 
As network connectivity can be disrupted in various ways, you will have to use different tools to locate and troubleshoot the problem.
Facing a connectivity issue, you should create a troubleshooting plan:
	- Verify DNS
	- Verify first hop
	- Verify path connectivity
	- Check firewalls
	- Verify traffic reaching host
This plan is just an example. You can also create your own. The plan's purpose is to help you keep track of what you have already checked and the list of remaining possible causes to examine.  A good plan can significantly cut down the time you need to locate the issue. Starting by verifying that name resolution is a good choice, since you want to be sure that you are connecting to the right host.

Verify DNS
You can check DNS name resolution using the following two commands:

nslookup: Displays the resolved IP address (if any), allowing you to make sure that you are connecting to the right address. If there are DNS failures, this tool also allows you to examine the internals of the DNS system.
ping: Also displays the resolved IP address. In addition, a successful ping to an IPv4 address means that the endpoints have basic IPv4 connectivity between them.

PS C:\> ping dns.google
Pinging dns.google [8.8.4.4] with 32 bytes of data:
Reply from 8.8.4.4: bytes=32 time=39ms TTL=52

Many networking engineers have come to rely on the ping tool for troubleshooting and with good reason. Originally standing for Packet INternet Groper, the ping command is available on all major devices and operating systems. It uses a series of ICMP echo request and ICMP echo response messages to determine if a remote host is active or inactive, the round-trip time (RTT) in communicating with the host and (if any) packet loss. The ping command first sends an echo request packet to an address, then waits for a reply. The ping is successful only if the echo request gets to the destination, and the destination is able to send an echo reply to the source within a predetermined time called a timeout. The default value of this timeout is two seconds on Cisco devices.

As you can see in the command output, ping also measures round-trip time. You can use it as an approximate measurement of delay. Furthermore, looking at how the times vary will give you a rough estimate of jitter, which is the variance of time delay in packets on the network.

Therefore, ping will allow you to do multiple checks with a single command. Regarding connectivity, if ping is successful, you know that both first hop and the full path to destination are operational. On the down side, if ping command fails, that may not really tell you much. As some administrators choose to disable ping functionality on their servers, you cannot distinguish this case from broken connectivity.  If ping fails, you will have to verify connectivity in a different way and it makes sense to verify first hop as the next step.

Verify First Hop
In networking terminology first hop signifies the first router or Layer 3 switch that a host uses to forward packets. For end hosts, it represents a default gateway. If a host lacks connectivity to this router, it will only be able to communicate with other hosts on the local subnet and nothing else. This is the reason that we usually check first hop right after DNS. What is more, it is perhaps one of the most common network problems you will encounter.

For first hop verification use the commands in the following order:
	- show ip route to display default gateway configuration.
	- ping towards default gateway to verify reachability and populate ARP cache.
	- show ip arp to display the mapping of IP addresses to MAC addresses to verify connected devices.
	- show ip interface brief to display IPv4 network configuration of the interfaces.

Cisco Devices					Windows / macOS					Linux
-------------					---------------					------------
show ip route					netstat -r						ip route list
show ip arp						arp -a							ip neigh list
show ip interface brief			ipconfig / ifconfig				ip address list
-------------------------------------------------------------------------------

You can verify first hop reachability from either side of the connection; the target host or the first hop router. Some engineers prefer to do it from the router, as hosts may run different operating systems and you have to be familiar with the commands specific to that OS. The table lists the alternatives as used in different operating systems.

To discover MAC address information the Address Resolution Protocol (ARP) is used on ethernet-connected networks. Using a series of ARP request and ARP response messages, the initially unknown MAC address of the known destination IP address is discovered.

If ARP is operating correctly, MAC and IP address of the first hop router (default gateway) will show up in ARP cache. Then you know that you have a working ethernet connection to the default gateway.

Verify Path Connectivity
Once you have ensured first hop connectivity and the problem persists, the next step is to verify the path to destination host (or a proxy server if it is being used).

Traceroute is used to test the path that packets take through the network. It sends out either ICMP echo request (Microsoft Windows) or UDP messages (most implementations, e.g. Cisco IOS Router) with gradually increasing IPv4 Time to Live (TTL) values to probe the path by which a packet traverses the network. The first packet with the TTL set to 1 will be discarded by the first hop router, which will send an ICMP 'time exceeded" message that is sourced from its IPv4 address. The device that initiated the traceroute therefore knows the address the first hop router. When the TTL is set to 2, the packets will arrive at the second router, which will respond with an ICMP "time exceeded" message from its IPv4 address. This process continues until the message reaches its final destination; the destination device will return either an ICMP echo reply (Windows) or an ICMP port unreachable, indicating that the request or message has reached its destination.

Use traceroute (or tracert on Microsoft Windows) to determine how far along the path data can successfully reach. But note the faulty hops (marked by the asterisk) may be filtering ICMP messages and still forward traffic.

Cisco traceroute works by sending a sequence of three packets for each TTL value, with different destination UDP ports, which allows it to report routers that have multiple, equal-cost paths to the destination.

Another approach that can be used to verify that path connectivity is inspecting individual routing tables hop-by-hop. However, this approach is not only more tedious, it also requires management access to routers, which is rarely available.

Check Firewalls
Any security devices and VPN connections along the data path need to be reviewed separately and with special consideration. Because they were designed for filtering, legitimate traffic can easily be blocked by some default rule or a rule that is unintentionally too broad. Firewalls are also one of the most common sources of connectivity issues.
What makes troubleshooting security devices especially challenging is:
	-They often have huge rulesets
	-Network Address Translation (NAT) adds additional complexity
	-Commands like traceroute and ping may not work through NAT
	-Limited personnel have access

If available, make use of tools like the packet-tracer command on Cisco ASA devices to help you understand what happens to packets.
Many firewalls also act as routers. If you believe that NAT and filtering are not an issue, you should be able to use tools like traceroute and ping on the device itself (if you have access).


Verify Traffic Reaching Host
As the last step in troubleshooting network issues, you can verify that traffic is reaching the intended host, making sure that network indeed provides connectivity. Available on majority of Linux distributions, tcpdump is an invaluable tool that is used to capture and display ("dump") packets on the command line. An alternative for Microsoft Windows systems is a GUI packet capture program called Wireshark.
The tcpdump and Wireshark programs allow you to:
	- Capture and display IP packets
	- See transmitted and received traffic
	- Observe network behavior, performance, and traffic source
	- Analyze network protocols
	- Filter only relevant traffic with various options and filters

Observe a packet capture of an unsuccessful telnet connection attempt. Reply to the TCP SYN was a TCP RST packet because no Telnet service is running on this host.
# tcpdump -i any port 22 or port 23
21:38:57.969164 IP6 localhost.47744 > localhost.telnet: Flags [S], seq 1970484646, win 43690, options [mss 65476,sackOK,TS val 484369352 ecr 0,nop,wscale 7], length 0
21:38:57.969174 IP6 localhost.telnet > localhost.47744: Flags [R.], seq 0, ack 1970484647, win 0, length 0
Command arguments that are used instruct tcpdump to show traffic from any interface (-i any) and having source or destination ports either 22 or 23. Other useful filters include source and destination (src, dst) and also specific protocol (udp, tcp, icmp).

Network Constraints
In networks, you will find a mix of data, voice, and video, as well as network control traffic. Each traffic type has different properties:
	- Bandwidth: The amount of free capacity needed on a link, in order for the service to function smoothly.
	- Delay (latency): The time it takes for a packet to reach its destination. One-Way delay/latency is from the sender to the receiver.
	- Round-Trip delay/latency: The combined time of packets and their replies to come back to the sender.
	- Jitter: The time variance between the highest and the lowest value for delay/latency.
	- Packet Loss: Packets that are not received, or lost in transit, for whatever causes.
	
Network Constraint		Affected By
------------------		-----------
Bandwidth				The ability or capacity of an interface (Layer 3) or port (Layer 2) to process frames due to features such as IPsec or MACsec encryption
Speed					The rate at which the sending device can encode data, calculate required checksums and so on.
Latency					An amount of time a device in the forwarding path (ex. Router or Switch) needs to receive a frame on a port, process the frame against all features and settings (ex. QoS, ACLs) and retransmit the frame
Jitter					A variation in the delay may be due to configuration errors (ex. QoS), network congestion, incorrect queuing.
Packet Loss				Number of packets lost depends upon configuration errors, device failure, link loss.

The impact of latency is most felt with:
	- Voice traffic
	- Video conferencing
	- Real-time applications, such as Remote Desktop (RDP)
Insufficient bandwidth affects the most:
	- Video conferencing
	- Streaming
	- Interactive web applications

Like bandwidth, packet loss has the most effect on video conferencing and streaming:
	- Degraded audio and video quality
	- Loss of connection
	- But buffers can be used to "stay ahead" of missing packets
Too much jitter presents a problem for the following:
	- Network Time Protocol (NTP) clients
	- Voice and video traffic
	- Real-time applications
	
Jitter results from variation of delay between packets. But more specifically when this variation of delay exceeds a certain threshold that applications can still handle. That is when variation of delay (jitter) takes place for a longer period of time than the receiving device can compensate. NTP clients are an example where jitter makes accurate time calculation very hard, if not impossible.

Model-Driven Programmability Stack 
As Nextgen programmatic interfaces are built, products must meet a few key requirements:
	- Efficient and easy-to-use tooling to consume the new application programming interfaces (APIs) (programming libraries)
	- Extensible and open APIs (Representational State Transfer [REST], Representational State Transfer Configuration Protocol [RESTCONF], Network Configuration Protocol [NETCONF], gRPC)
	- Flexibility and support for different types of encoding formats (XML and JavaScript Object Notation [JSON])
	- Support for different types of transport
The core components of the complete device API include:
	- Data models: The foundation of the API consists of data models. Data models define the syntax and semantics including constraints of working with the API.
	- Transport: Model-driven APIs support one or more transport methods including Secure Shell (SSH), Transport Layer Security (TLS), and HTTP or HTTPS.
	- Encoding: Model-driven APIs support the choice of encoding including XML and JSON, but also custom encodings such as Google protocol buffers.
	- Protocols: Model-driven APIs also support multiple options for protocol with the three core protocols being NETCONF, RESTCONF, and gRPC. Remember, they are the core protocols that work with model driven APIs. REST is not explicitly listed because when REST is used in a modeled device, it becomes RESTCONF.

There are two main data encoding formats that are commonly used in APIs. They are XML and JSON. Each of them provides a structured way using data formatting to send data between two computer systems. It is in stark contrast to using SSH issuing CLI commands, in which data is sent as raw text, such as, strings over the wire.
XML and JSON are used for the data transmission and they have the following features:
	- Human readable, because they are self-describing
	- Hierarchical, because they store values within values
	- Parsable and used by lots of programming languages

Data Models
The industry is migrating from having no framework (no modeling) when using CLI commands and text output, to a fully modeled device. In other words, a device that has a JSON and XML representation of its full configuration and that is fully driven from a robust model such as YANG. Models also define operational data and statistics on devices.

The following are the main characteristics of the data models:
	- Data models describe a data structure, type, and constraints in the form of a schema language.
	- Use well-defined parameters to standardize the representation of data from a network device so the output among various platforms is the same.
	- Not used to actually send information to devices and instead rely on protocols such as NETCONF and RESTCONF.
	- Device configuration can be validated against a data model to check if the changes are a valid for the device before committing the changes.
Data models are used to describe syntax and semantics of working with specific data objects. They can define attributes and answers such as the following:
	- What is the range of a valid VLAN ID?
	- Can a VLAN name have spaces in it?
	- Should the value be a string or an integer?
	- Should the values be enumerated and only support up or down for an admin state?
One misconception is that data models are used to send data to/from a device, which is not the case. Instead, protocols such as NETCONF/RESTCONF send JSON and XML encoded documents that are governed by a given model.

Network Automation and NETCONF 
The basic create, read, update, and delete (CRUD) mechanics for network device configuration and operational state are encapsulated in NETCONF. NETCONF provides formal and generally interoperable means of opening a secure management session with a network device. It offers basic operations to act on the configuration data and get operational state, mechanisms for notifications, and a set of framework definitions and operations to tie these functions together.
The basic purpose of NETCONF is to do the following:
	- Transport configuration payloads to a device, which is targeted at a specified configuration datastore
	- Retrieve configuration data when queried
	- Support notifications, often based on SNMP trap definitions
NETCONF is defined in RFC 6241. It is a protocol that is transported over TLS and SSH, which includes operations and configuration datastore concepts that allow management of a network device. The protocol is exposed as an API that applications can use to send and receive full and partial configuration data sets, and receive state and operational data sets. An example of a client-side tool for NETCONF is the ncclient Python tool
Here are some key features of NETCONF (RFC 6241):
	- Network Configuration Protocol
	- Defines framework for session management
	- RPC messages to put and get configuration data
	- Transaction-based communication
	- Network service activation with networkwide transaction
	- Datastores of configuration data
	- Configuration can be CLI (1.0) or YANG (1.1)

Configuration Datastores
NETCONF defines the existence of one or more configuration datastores and allows configuration operations on those datastores. A configuration datastore is defined as the complete set of configuration data that is required to get a device from its initial default state into a desired operational state. The configuration datastore does not include state data or executive commands.

Name		Description
------		-----------
running		This datastore holds the complete configuration currently active on the network device. Only one running configuration datastore exists on the device, and it is always present. This datastore is referenced by using the <running> element. See RFC 6241, Section 5.1.

candidate	This datastore holds configuration data that serves as a workplace for creating and manipulating configuration data. A <commit> operation causes the device's running configuration to be set to the value of the candidate configuration. Any uncommitted changes to the candidate configuration can be removed by executing the <discard-changes> operation. See RFC 6241, Section 8.3.

startup		Configuration loaded by the device when it boots. Operations that affect the running configuration will not be automatically copied to the startup configuration. An explicit <copy-config> operation from the <running> to the <startup> is used to update the startup configuration to the current contents of the running configuration. Referred to using the <startup> element. RFC6241 section 8.7.

The existence of different datastores on a given device is advertised via capabilities, as defined in Section 8 of the RFC 6241, Network Configuration Protocol (NETCONF). Capabilities are exchanged when you start a NETCONF session with a device, and you can see these capabilities in the initial message from the NETCONF server on the device.

The running configuration datastore holds the complete configuration that is currently active on the network device. Only one running configuration datastore exists on the device, and it is always present. NETCONF protocol operations refer to this datastore using the <running> XML element. The running datastore capability is supported only in Cisco IOS XE and Cisco Nexus Operating System (NX-OS) Software.

This string identifies the candidate datastore capability in the capabilities that the device advertises.  Cisco IOS XE Software supports a candidate datastore, which needs to be enabled in the CLI. When using NETCONF with Cisco IOS XE Software, you must explicitly state that it is the candidate datastore that is the target of operations, and you must explicitly commit changes.

Operational Data
NETCONF provides a set of low-level operations to manage device configurations and retrieve device state information. These operations include retrieving, configuring, copying, and deleting configuration datastores and retrieving state and operational data. Additional operations can be provided, based on the capabilities that the device advertises.
The information that can be retrieved from a running system is separated into two classes:
	- Configuration data is the set of writable data that starts the system operations.
	- State data is the nonconfiguration data on a system, or operational data, such as read-only status information and collected statistics.
	
Operations
Here are base NETCONF operations:
	- get: 			 retrieves running configuration and device state information, that is, operational data.
	- get-config: 	 retrieves all or part of a specified configuration datastore.
	- edit-config:   loads all or part of a specified configuration to the specified target configuration datastore.
	- copy-config:   creates or replaces an entire configuration datastore with the contents of another complete configuration datastore.
	- delete-config: deletes a configuration datastore. The <running> configuration datastore cannot be deleted.
	- lock: 		 allows the client to lock the entire configuration datastore system of a device. Such locks are intended to be short-lived. They allow a client to make a change without fear of interaction with other NETCONF clients, non-NETCONF clients (for example, SNMP and CLI scripts), and human users.
	- unlock: 		 releases a configuration lock that was previously obtained with the <lock> operation.
	- close-session: requests graceful termination of a NETCONF session.
	- kill-session:  forces the termination of a NETCONF session.
	
NETCONF over SSH
There are a few different steps that occur during a NETCONF session. It can be summarized as follows:
	- Client connects to the NETCONF SSH sub-system.
	- Server responds with Hello that includes NETCONF supported capabilities.
	- Client responds with supported capabilities to establish connection.
	- Client issues NETCONF request (rpc/operation/content).
	- Server issues response or performs operation.

The first step is to connect to the NETCONF server by using the following Linux command:
$ ssh -p 830 cisco@csr1kv -s netconf
When you connect to the network device and establish a connection, the device sends a hello and it includes all its supported NETCONF capabilities. The Cisco Cloud Services Router (CSR) 1000V includes hundreds of capabilities.

Note: All messages to and from the NETCONF server or client must end with ]]>]]>. This way, the client, and server know that the other side is done sending the message.

When the server sends its hello, the client needs to send a hello with its supported capabilities. You can respond back with everything the server supports (assuming the client does too), or just with the bare minimum to do edit and get commands.

Once the client sends its capabilities, it can then send NETCONF requests. In this example, the client is sending a request to perform the NETCONF get operation (denoted by <get>). It is then asking for a given section of configuration using a filter. Based on the devices/servers being used, you often have to be aware of which model is being used (denoted by the XML namespace).
This filter is selectively asking for configuration information of GigabitEthernet1.
<?xml version="1.0"?>
<rpc message-id="101" xmlns="urn:ietf:params:xml:ns:netconf:base:1.0">
    <get>
        <filter type="subtree">
            <native xmlns="http://cisco.com/ns/yang/ned/ios">
             <interface>
              <GigabitEthernet>
               <name>1</name>
              </GigabitEthernet>
             </interface>
            </native>
        </filter>
    </get>
</rpc>
]]>]]>

The server processes the client’s request and responds with the configuration as expected.
<?xml version="1.0" encoding="UTF-8"?>
<rpc-reply xmlns="urn:ietf:params:xml:ns:netconf:base:1.0" message-id="101">
    <data>
        <native xmlns="http://cisco.com/ns/yang/ned/ios">
          <interface>
           <GigabitEthernet>
            <name>1</name>
            <negotiation>
             <auto>true</auto>
            </negotiation>
            <ip>
             <address>
              <primary>
               <address>192.168.1.11</address>
               <mask>255.255.255.0</mask>
              </primary>
             </address>
            </ip>
           </GigabitEthernet>
          </interface>
        </native>
    </data>
</rpc-reply>]]>]]>

NETCONF also supports capability discovery and model downloads. Supported models are discovered using the ietf-netconf-monitoring model. You can see the revision dates for each model in the capabilities response. Data models are available for optional download from a device using the get-schema RPC.

YANG (Yet Another Next Genration)
Yet Another Next Generation (YANG) is a modeling language that is used with NETCONF and in an increasing number of other domains. It is used to define configuration and operational state data models, and additional operational functions, and notifications that are transported within NETCONF commands.
	- Here are some key features of YANG (RFC 6020):
	- Language for data modeling and RPCs
	- Originally intended for NETCONF payloads
Now used for many purposes as an interface definition language, for example, ODL (OpenDayLight)

YANG is a data modeling language that was originally defined in RFC 6020, YANG—A Data Modeling Language for the Network Configuration Protocol (NETCONF). It is used to model configuration and state data that NETCONF manipulates, and the NETCONF RPCs and notifications. Beyond its roots in NETCONF, YANG is now used as a general-purpose interface definition and data modeling language in different programming environments, including Cisco Network Services Orchestrator (NSO) and OpenDaylight (ODL) and with tools like YDK.

YANG Structure
YANG was originally defined in RFC 6020, YANG—A Data Modeling Language for the Network Configuration Protocol (NETCONF) (circa 2010). It was defined as "a data modeling language that is used to model configuration and state data manipulated by NETCONF…. YANG is used to model the operations and content layers of NETCONF."

A YANG module defines a hierarchy of data that supports a complete description of all data that is sent between a NETCONF client and server. YANG models the hierarchical organization of data as a tree in which each node has a name and a value or a set of child nodes. YANG provides clear and concise descriptions of the nodes and the interaction between those nodes.

Data models are structured with modules and submodules. A module can import data from other external modules and include data from submodules. The hierarchy can be augmented, allowing one module to add data nodes to the hierarchy that are defined in another module. This augmentation can be conditional, with new nodes appearing only if certain conditions are met.

Some of the IETF models include the following:
	- RFC: 7223: A YANG Data Model for Interface Management
	- RFC: 7277: A YANG Data Model for IP Management
	- RFC 7317: A YANG Data Model for System Management
	- RFC 7407: A YANG Data Model for SNMP Configuration
	- RFC 6728: Configuration Data Model for the IP Flow Information Export (IPFIX) and Packet Sampling (PSAMP) Protocols
	- RFC 8519: Network Access Control List (ACL) YANG Data Model
	- RFC 8345: A Data Model for Network Topologies
	- RFC 8022: A YANG Data Model for Routing Management
	- IETF draft-ietf-ospf-yang-29: Yang Data Model for OSPF Protocol
	- RFC 6022: YANG Module for NETCONF Monitoring
Other YANG model work includes the following:
	- CableLabs: DOCSIS CCAP
	- Open Networking Foundation: OF-CONFIG
	- Vendor YANG models: Cisco IOS, IOS XR, IOS XE models
	- OpenConfig Group: Publishes models, documentation, and other material for the community

Data Nodes Generically Mapped to XML
YANG is expressed in XML, that is, an instance of something that is modeled in YANG is an XML document. The illustration shows the generic rules for mapping YANG model elements to XML document elements.

Individual data items:
	- Single Indatanse: Leaf
		<leaf>value</leaf>
	- Multiple Instnces: Leaf-list
		<leaf>value1</leaf>
		<leaf>value2</leaf>
		<leaf>value3</leaf>
Structure multiple data items:
	- Containing elements include other data nodes to build a data hierarchy
	- Resemble containement structure in an XML instance document
	- Single instance: Container
		<container>
			<node1>value1</node2>
			<node1>value2</node2>
		</container>
	- Multiple instances: List
		<list>
			<key>value1</key>
			<morenodes>....</morenodes>
		</list>
		<list>
			<key>value2</key>
			<morenodes>....</morenodes>
		</list>

XML namespaces are widely used and have the following traits:
	- Provide a means to mitigate element name conflicts.
	- It is defined with the attribute xmlns:prefix="URI", prefix is used as abbreviation of the namespace in the tag.
	- You can have a default namespace using xmlns=url, eliminating the need to have an attribute in each tag.
	
The operation attribute has one of the following values:
merge: 		The configuration data that the element containing this attribute identifies are merged with the configuration. Data are merged at the corresponding level in the configuration datastore that the target parameter identifies. This behavior is the default.

replace: 	The configuration data that the element containing this attribute identifies replace any related configuration in the configuration datastore that is identified by the target parameter. Only the configuration that is actually present in the "config" parameter is affected.

create: 	The configuration data that are identified by the element containing this attribute are added to the configuration if and only if the configuration data does not exist on the device. If the configuration data exists, an <rpc-error> element is returned with an <error-tag> value of "data-exists."

delete: 	The configuration data that are identified by the element containing this attribute are deleted in the configuration datastore that is identified by the target parameter

Utilizing Data Models with RESTCONF Protocol 
HTTP-based RESTCONF provides a programmatic interface that is based on standard mechanisms for accessing configuration data, state data. You can also access data-model-specific RPC operations and events that are defined in the YANG model. It is defined in RFC 8040.
RESTCONF offers these characteristics:
	- Functional sub-set of NETCONF
	- Exposes YANG models via a REST API (URL)
	- Uses HTTP(S) as transport
	- Uses XML or JSON for encoding
	- Developed to use HTTP tools and programming libraries
	- Uses common HTTP verbs in REST APIs
	
RESTCONF is a REST-like protocol that provides a mechanism over HTTP for accessing data that is defined in NETCONF datastores and modeled in YANG.

RESTCONF combines the HTTP protocol simplicity with the predictability and automation potential of a schema-driven API. A client can determine all management resources for YANG models and NETCONF capabilities. Therefore, the URIs for custom protocol operations and datastore content are predictable, based on the YANG module definitions. The generation of the code to support RESTCONF APIs, and the mapping of these API calls to NETCONF, can be automated because the mapping from a YANG model to a RESTCONF URI is well-defined.

RESTCONF helps support a common, REST-based programming model for network programming in general. This model aligns with the wider trend in infrastructure programming to support REST APIs.

Operation			Description
---------			-----------
GET					Retrieve data from a resource (config/operational)
POST				Create a configuration data resource
PUT					Create or replace a configuration data resource
PATCH				Merge configuration data with target resource
DELETE				Delete a configuration data resource

This mapping follows the general pattern of most REST APIs. Resources representing configuration data can be modified with the DELETE, PATCH, POST, and PUT methods. Data is encoded with either XML or JSON.

The HTTP GET operation represents the same semantics as the NETCONF GET and get-config operations, and can also be used for notifications. The HTTP PATCH operation supports partial configuration updates in a way that is similar to the NETCONF edit-config operation with operation=merge. The HTTP PUT operation is similar to PATCH, but it is typically used to replace the contents of a named resource, rather than changing attribute values of a resource.

The HTTP POST operation is used for NETCONF remote procedure calls (RPCs), and in some circumstances, to create a resource. The HTTP DELETE operation is equivalent to the NETCONF edit-config with operation=delete.  The client can also access the YANG libraries that the server implements, that is, the capabilities.

The API resource contains the entry points for the RESTCONF datastore and operation resources. It is the top-level resource that is referred to by the notation {+restconf}. It has the media type application/yang.api+xml or application/yang.api+json, depending on whether the encoding of the payload document is XML or JSON.

The RESTCONF does not support all the NETCONF operations. Specifically, these operations are not supported:
	- Config locking
	- Candidate config
	- Startup config
	- Validate
	- Confirmed commit
You can perform more granular operations when doing a change within NETCONF, such as specifying whether you want to replace an object, update it, or delete it. This example shows how RESTCONF operations map directly back to their counterparts in NETCONF.

Examples:
GET http://csr1kv/restconf/api/config/native
	Retrieve a full running configuration as an object
GET http://csr1kv/restconf/api/config/native/interface
	Retrieve interface-specific attributes
GET http://csr1kv/restconf/api/config/native/interface/GigabitEthernet/1
	Retrieve interface-specific attributes for GigabitEthernet1

RESTCONF utilities and tools:
Same tools that are used for native REST interfaces are used for RESTCONF:
	- Python requests module
	- Postman
	- Firefox RESTClient
Note:  There are no API docs, so YANG tools will be used to generate the URL and request body.

Using Python Scripts and Cisco SDKs 
A Software Development Kit (SDK) is a collection of tools that are used for developing software or applications that are supplied by software and hardware suppliers. It mostly consists of API’s, sample code, documentation and much more. The YANG Development Kit (YDK) is an SDK that provides APIs modeled in YANG. The main goal of YDK is to reduce the learning curve of YANG data models by expressing the model semantics in an API and abstracting protocol/encoding details.

The main benefits of the Model-Driven APIs:
	- Simplify application development
	- Abstract transport, encoding, modeling language
	- API generated from YANG model
	- One-to-one correspondence between model and class hierarchy
	- Multi-language (Python, C++, Ruby, Go, and so on)

The following list are main features of the YDK:
	- Python SDKs that are built from YANG models
	- Ydk-py
		- Pre-built bindings for common models
	- Ydk-gen
		- Generate your own bindings

YDK is composed of a core package that defines services and providers, plus one or more module bundles that are based on YANG models. Each module bundle is generated using a bundle profile and the ydk-gen tool. It comprises two Open Source projects, the ydk-py and ydk-gen are available on Cisco DevNet GitHub community at https://github.com/CiscoDevNet

Ydk-py is a set of pregenerated Python modules that adhere to common YANG models. For example, you can use ydk-py to use Python to configure a device that adheres to a given model such as OpenConfig’s BGP model. For more advanced users who are developing custom data models, you can autogenerate your own python bindings using ydk-gen.

One of the huge benefits of model-driven APIs is that you get client-side validation. It means your application that is written using ydk-py bindings already understand the constraints that are embedded in the YANG model. Thus, if you go to push a change to a device, an error is raised in Python (or given language) before an API call is even made to the device.

Client-side validation:
	- YDK service will automatically perform local (client-side) validation
	- Type check (enum, string, and so on)
	- Value check (range, pattern, and so on)
	- Semantic check (key uniqueness/presence, mandatory leafs, and so on)

Model Driven Programmability in a Cisco Environment 
One of the major byproducts of using models is model-driven programmability. What this means is that model-driven programmability fully de-couples transport, protocol, and encoding from the model being used. Over the past few years, YANG was predominantly used for NETCONF, which encodes only in XML. However, since models were first written in YANG, it was easy to build new tooling that autogenerated URLs and bodies that took advantage of them using a REST API in both JSON and XML. The model then becomes the definition of what can be done on a device that is completely decoupled from the encoding method.

An example of an OpenConfig BGP YANG model:
	- Model is the source of truth
	- rw represents configuration data
	- ro represents operational data/state
	
There are other Cisco platforms and solutions that are all notable and worth mentioning because they do not use YANG but are fully driven by an object model.
Examples of the Cisco platforms and solutions that do not use YANG are:
	- Cisco Nexus 9000/3000 with NX-API REST
	- Cisco Application Centric Infrastructure (ACI)
	
The main characteristics of the Cisco ACI are:
	- Model-based, structured, computer friendly
	- Choice of transport, protocol, and encoding
	- Model-driven APIs for abstraction and simplification
	- Wide standard support while using Open Source
	- Deploy services faster and more simply
	- Simplify application development
	- Models manage abstractions of the underlying network device data structures (configurations, state data, and so on)

Each of these platforms was built using a custom object model that offers the same properties as if it was built using YANG models. For example, with Cisco ACI everything is an object. Every object has associated properties and constraints. These constraints are defined in the Cisco ACI Management Information Model as opposed to a YANG model. The most important point to note is that YANG is not the only way to model network devices. If YANG is not used, any associated YANG tooling are not be supported. In the case of Cisco ACI, they already have a robust toolset of libraries and an object model browser that is already on par or better with what is emerging from the YANG ecosystem.

Cisco NX-OS Programmability
The Cisco NX-OS open platform allows for programmatic access to Cisco Nexus platforms providing network administrators with increased scale, agility, and efficiency. Open NX-OS on Cisco Nexus platforms offers a rich software suite that is built on a Linux foundation that exposes APIs, data models, and other programmatic constructs.

This support is delivered by several means including two versions of NX-API including NX-API CLI and NX-API REST, NETCONF, on-box Linux and Python scripting capabilities, and several traditional means already familiar to network administrators including the Scheduler feature, Embedded Event Manager, Power on Provisioning.

Cisco Nexus Programmability Features
	- Day-0 Provisioning				---> POAP, iPXE
	- Base Features						---> SNMP, Native Python, EEM
	- APIs								---> NETCCONF, NX-API CLI, NX-API REST, XMPP
	- Linux on the Switch				---> Linux Container Guest Shell, Native Shell
	- COnfiguration Management			---> Puppet, Chef, Ansible
	
In the following list are the most common scenarios for device configuration and usage of device APIs:
 Day-0 provisioning: 
 	Zero-touch device provisioning is commonly associated with compute devices, but network devices have had this capability for years. Power on Auto Provisioning was designed to provide advanced Day-0 provisioning capabilities using an extensible framework. POAP includes the ability to execute Python scripts as part of its workflow. Today, POAP can download and install additional management agents and apply specific configurations that are based on information such as location in a network topology.
	-A similar approach is achieved by using Preboot Execution Environment. PXE has extended its presence into the network as infrastructure devices are increasingly managed more like servers. Cisco NX-OS uses iPXE, which utilizes an Open Source network firmware that is based on gPXE/Etherboot.

	Base features: 
		In addition to providing traditional SNMP support, the Cisco Nexus platform also provides Python scripting capability on the devices to provide programmatic access to the switch CLI to perform various tasks including Power-On Auto Provisioning and Embedded Event Manager actions. The Python interpreter is included in the Cisco NX-OS Software.

		APIs: 
			Cisco NX-OS provides a built-in web server to respond to HTTP calls to the switch to improve accessibility, extend capability, and improve manageability of Cisco NX-OS. APIs include NETCONF, NX-API CLI, NX-API REST, and XMPP.

Linux on the switch: 
	Cisco Nexus switches have always been built upon a Linux foundation making available a native Linux Bash shell, but today NX-OS also provides a Linux guest shell, which is a separate Linux environment running on the switch inside a container. Currently utilizing a CentOS distribution, a key benefit for the guest shell is the ability to securely run third-party applications that monitor and control the switch.

	Configuration management: 
		Cisco NX-OS incorporates a set of tools, features, and capabilities that enable automation. Modern configuration management tools like Puppet, Chef, and Ansible drive programmability.

NX-API CLI
The following are main features of the NX-API CLI:
	- REST-like API that enables programmatic access to Cisco Nexus devices
	- Improves accessibility of the CLI by making them available off box
	- Supports show commands, configurations, and Linux Bash

On Cisco Nexus devices, CLIs are run only on the device and used far too often to manage data center networks. NX-API improves the accessibility of these CLIs by making them available outside of the switch by using HTTP/HTTPS. You can use the NX-API as an extension to the existing Cisco Nexus CLI. The NX-API CLI API is great for network engineers getting started with the API because it still makes use of commands. It sends commands to the device, wrapped in HTTP/HTTPS, but receives structured data back. You have the ability to send show commands, configuration commands, and Linux commands directly to the switches using NX-API CLI.
The following are transports are supported for NX-API:
	- Runs on HTTP/HTTPS
	- CLI commands are encoded into the HTTP/HTTPS POST body
	- The request/response format is encoded with JSON-RPC, JSON, or XML
	- NGINX (pronounced "engine-ex") HTTP back-end web server to listen for HTTP requests

The NX-API back end uses the NGINX HTTP server. The Nginx process, and all its children processes, are under Linux cgroup protection where the CPU and memory usage are capped. If the NGINX resource usage exceeds the cgroup limitations, the Nginx process is restarted and restored. The NX-API back end uses a lightweight on-box web server to listen for HTTP requests, which are converted to CLI and used to retrieve data or push configurations. The request/response format is encoded with JSON-RPC, JSON, or XML. NX-API supports XML, JSON, and JSON-RPC, and commands are sent to a single HTTP request within a CLI wrapper.

NX-API supports HTTPS, therefore all communication to the Cisco Nexus device can be encrypted. By default, NX-API is integrated into the authentication system on the device. You access the device through the NX-API using user accounts containing a username and password, which are contained in the HTTP header.

NX-API is disabled by default and can be enabled by using the feature manager CLI command. NX-API provides a session-based cookie, nxapi_auth, when users first successfully authenticate. The session cookie expires in 600 seconds, which is a fixed value that cannot be modified.

With the session cookie, the username and password are included in all subsequent NX-API requests that are sent to the device. If the session cookie is not included with subsequent requests, another session cookie is required and is provided by the authentication process.

The NX-API feature must be enabled via the CLI.
	- Enable the feature via the command line
	- Identify the alternate port being used (if any)
	- Identify an HTTPS certificate file to use
	- Enable the nxapi sandbox
The main attributes of the NX-API Sandbox are:
	- NX-API is available on the switch itself and accessed via web browser
	- There are helpful buttons available with commonly used built-in scripts
	- Supported on all Cisco Nexus platforms

When using JSON-RPC, you are always sending a list of JSON objects (dictionaries) even if it’s a list of one (single command). Here is an example of sending show version with the associated Request and Response. The Response is similar in that it’s always a list of dictionaries for JSON-RPC.

Note: When using JSON encoding, you only receive a list in the response when you send more than one command. On the other hand, JSON-RPC always replays with a list of dictionaries.
Note: JSON sends a dictionary when sending one command but sends a list of dictionaries when sending more than one command.

NX-API REST
NX-API REST is supported on Cisco Nexus 9000 and 3000 series switches starting with 7.0(3)I2(2). NX-API REST is the Nextgen API for the Cisco Nexus platform in that it supports sending and receiving objects in the API payload. If you recall, NX-API CLI supports sending commands to the device while receiving structured data back (JSON, XML). With NX-API REST, it is completely based on structured data. Therefore, JSON/XML payloads are sent to the device in the HTTP request and received from the device in the response.

The following list outlines the NX-API REST implementation:
	- NX-API REST is an evolved version NX-API CLI
	- Complete REST interface that brings Model Driven Programmability to standalone Cisco Nexus family switches
	- Configuration and state information of the switch is stored in a hierarchical tree structure that is known as the Management Information Tree (MIT)

The implementation of NX-API REST is similar to the model used by Cisco ACI. All information about the switch, including configuration and state data, is stored in a hierarchical tree. This tree is called the Management Information Tree. Every object in the tree can be directly accessed via the REST API.

The following are the main features of the NX-API REST:
	- Object instances are referred to as managed objects (MOs)
	- Managed Objects (MOs) are also of a certain type of Class
	- A unique distinguished name (DN) can identify every managed object in the system
	- URLs and URIs map directly to distinguished names identifying objects on the tree
	- Data can be encoded in XML or JSON

Every object in the Management Information Tree is referred to as a Managed Object. Each Managed Object (MO) is also of a certain type of class. For example, Ethernet interfaces are of type l1PhysIf and SVI interfaces are of type sviIf, but all interfaces are of type intf. These types are called classes.

It is important to understand the relationship, between DNs and classes because you can make an API to the Cisco Nexus switch using NX-API REST using a DN-based query or Class-based query. Example: query a single interface (DN) or query all interfaces of a given type (Class).

The NX-API-REST API operates in forgiving mode, which means that missing attributes are substituted with default values (if applicable) that are maintained in the internal data management engine. The Data Management Engine (DME) validates and rejects incorrect attributes. The API is also atomic. If multiple MOs are being configured, and any of the MOs cannot be configured, the API stops its operation. It returns the configuration to its prior state, stops the API operation that listens for API requests, and returns an error code.

URLs and URIs map directly to distinguished names identifying objects on the tree. Any data on the MIT can be described as a self-contained structured text tree document encoded in XML or JSON.

The NX-API REST API supports three methods:
	- GET: Used to retrieve and read information from the Management Information Tree.
	- DELETE: Used to delete and remove an object from the Management Information Tree.
	- POST: Used to create or update an object within the Management Information Tree. In the NX-API REST API, POSTs to the API are idempotent meaning that the change is made just once no matter how many times the API is called.

There is a dedicated API to handle authorization when using the API. The device returns a token that is then sent in subsequent API calls that perform CRUD operations.

Regarding Content-Type and Accept, Cisco Nexus switches ignore them. It is the case because the payload format must be specified as a file extension. All API calls using NX-API REST will have either ".json" or ".xml" appended to them. This notation lets the switch know what the content-type is and how to response, for example, with which encoding format.

The URL format that is used can be represented as follows:
	/api/[mo|class]/[dn|class][:method].[xml|json]?{options}
The various building blocks of the preceding URL are as follows:
	- System: 		System identifier; an IP address or DNS-resolvable hostname
	- mo | class: 	Indication of whether it is a managed object or tree or class-level query
	- class: 		Managed-object class (as specified in the information model) of the objects queried; the class name is represented
	- dn: 			Distinguished name (unique hierarchical name of the object in the MIT tree) of the object queried
	- method: 		Optional indication of the method being invoked on the object; applies only to HTTP POST requests
	- XML | json: 	Encoding format
	- options: 		Query options, filters, and arguments

In a list are the GET method examples for NX-API REST:
	- http://n9k/api/mo/sys/intf/phys-[eth2/5].json
	- http://n9k/api/mo/sys/bgp/inst.json

The typical sequence of configuration is:
	- Authenticate: 
		Call https://<IP of Nexus switch>/api/mo/aaaLogin.xml with a payload that in XML. This call returns a cookie value that the browser uses for the next calls.
	- Send HTTP POST to apply the configuration: 
		The URL of the POST message varies depending on the object. The following is an example: https://<IP of Nexus switch>/api/mo/sys/bgp/inst.json. Api indicates that this call is to the API. Mo indicates that this call is to modify a managed object. Bgp/inst refers to the BGP instance, and .json indicates that the payload is in JSON format. If the end of URL is .xml, that would mean that the payload is in XML format.
	- Verify the HTTP status code: 
		One should want a response of 200 OK. With the capability to address and access an individual object or a class of objects with the REST URL, you can achieve complete programmatic access to the entire object tree and to the entire system.

NXToolkit
Cisco has published a Python library that is called NXToolkit that is a set of Python modules that simplify getting started to use Python to program against the NX-API REST API. The library can be used to perform common operations such as collect information from Cisco Nexus devices but also to make configuration changes. Using NXToolkit eliminates the need to worry about the underlying URLs being used as they are abstracted away from the developer and built-into the library itself.

While NXToolkit is a Python library that simplifies working with the NX-API REST API on Cisco Nexus switches, it also comes with a collection of pre-built scripts that perform common operational tasks. These pre-built scripts can be used immediately to eliminate the need to write any code. All that you need to do is enter in specific device information such as credentials and IP address information and execute the script.

Visore 
is a tool for NX-API REST that is built-into each switch. It is a managed object browser. It allows you to browse and navigate the Management Information Tree in real time and inspect the state of each and all objects. Visore is a great tool to understand and learn about the relationship between all objects of the system. You can access Visore by navigating to http(s)://<nexus>/visore.html and authenticating using standard device credentials.

Visore is helpful as it has two links in the center that say "Display URI of last query" and "Display last response." By clicking "Display URI of last query," you get to see the URI, which for this API call was:
	/api/node/class/rmonEtherStats.xml?query-target-filter=and(eq(rmonEtherStats.collisions,"0"))
Once you have the URI, you are able to make native REST calls using Postman or equivalent. As you can see, the API also supports filters and using Visore simplifies learning how to use them by seeing the URIs generated automatically. This API call queried the device for all objects of class type rmonEtherStats but then added a filter to only return those objects that had 0 collisions.

If wanted to get the results in JSON, you could use the URI generated and add ".json" to it like so:
	/api/node/mo/sys/intf/phys-[eth1/3]/dbgEtherStats.json 
This response would include the Ethernet stats just for Ethernet1/3 since it is a DN-based query.

You can also query a single SVI such as ‘interface vlan200’ with the following DN-based query:
	/api/node/mo/sys/intf/svi-[vlan200].xml
Note:  The addition of the /node in the resource is optional making API calls back into the system

Application Deployment Types
The purpose of a computer server is to serve or share data between different nodes. Today, multiple types of servers exist:
	- Application servers
	- Catalog servers
	- Computing servers
	- Database servers
	- File servers
	- Network servers
	- Web servers
	
Virtual machines have the following advantages:
	- Snapshots: A state of a virtual machine can be recorded at any point of time.
	- Migrations: A snapshot of a virtual machine can be moved to another host machine with its own hypervisor.
	- Failover: A virtual machine can be run on any other host server if the primary server fails.
Backup and restore: A snapshot of a virtual machine can be created at any time and restored when needed.
	- Virtual machines also have some disadvantages:
	- Underutilized hardware resources: Some of the hardware resources are used for running hypervisors, so not all the resources are available for virtual machines.
	- Large resource usage: Each virtual machine runs its own operating system, which also decreases the amount of the resources that are available for the server to perform tasks.

To compensate for the disadvantages of virtual machines, container technology was developed. Container technology removes the abstraction layer and directly uses the host operating system to provide an environment for multiple services (containers) to run on the same physical server. Container technology has the following advantages:
	- Direct access to bare-metal hardware
	- Optimal use of system resources
Container technology on bare metal servers also shares advantages with virtual machines:
	- Deploy applications inside portable environments.
	- Provide resource isolation between containers.
However, container technology on bare metal servers also has some disadvantages:
	- Physical server replacement is difficult. When replacing a bare metal server, the container environment must be recreated from scratch on new server.
	- Container platforms do not support all hardware configurations.
	- Bare-metal servers do not offer rollback features.
As already discussed, multiple deployment types are available, each with their own advantages and disadvantages. It is your choice to select the appropriate type based on the application needs and your company policies and processes.

Bare-metal servers have these advantages:
	- Performance: Physical server resources can be optimized for a specific workload.
	- Security: Data, applications and other resources are physically isolated.
	- Reliability: Physical resources are dedicated to a specific workload and are not shared.

Virtual Machines
A virtual machine is an emulation of a computer system running on a shared host. Each virtual machine consists of its own environment (including operating system, libraries, and applications) and is not aware of other virtual machines running on the same physical host. Communication between applications inside a virtual machine and physical resources is through an abstraction layer called a hypervisor. This abstraction layer is responsible both for resource allocation and isolation.
Virtualization consists of multiple layers:
	- Host machine: 	A physical server that supports virtualization.
	- Hypervisor: 		Computer software that runs on the host machine and manages the virtual machines, also known as the virtualization layer.
	- Virtual machine:  A virtual computer that emulates a physical computer system. It has its own operating systems as well as dedicated software to perform a specific task. Software that is executed on these virtual machines is separated from the underlying hardware resources.

Containers
Container technology uses host operating system features to provide an isolated environment for multiple applications to run on the same server. An early example of the technology behind containers is the chroot command, which provides isolation in Unix-based operating systems. chroot (change root) allows users to change the root directory for a running process, which makes it possible to isolate system processes into separate file systems, without impacting the global system environment. The chroot command was added to the 7th edition of Unix operating system in 1982. The environment created with the chroot command is called chroot jail.

Control groups later named cgroups provide the following:
	- Resource limiting: Limit CPU, memory, disk I/O, and network for a specific group
	- Prioritization: Some groups might have a larger share of resources than others.
	- Accounting: Measurement of group resource usage

In 2008, the Linux Containers (LXC) technology was developed. LXC provides virtualization at the operating system level by allowing multiple Linux environments to run on a shared Linux kernel, where each environment has its own process and network space.
The operating system has these features:
	- chroot
	- process and network space isolation
	- cgroups
	
The most widely-used container solution used today is Docker. Docker was released in 2013 and enables users to package containers so that they can be moved between environments. Docker initially relied on LXC technology, which was replaced by the libcontainer component in 2014.
Docker is popular because it contains more features than its predecessor, LXC:
	- Portable deployments across machines: You can use Docker to create a single object (image) containing all your bundled applications. The image can then be installed on any other Docker-enabled host.
	- Versioning: Docker can track versions of containers, inspect differences between versions, and commit new versions.
	- Component reuse: Docker allows building and stacking of already created packages.
	- Shared images: Anyone can upload new images to a public registry of Docker images.

Application Deployment Models 
Cloud computing, often referred to simply as cloud, represents delivery of computing resources (servers, storage, databases, software, networking, and more) over the internet on a pay-as-you-go basis. Cloud computing has become popular in recent years, but the history of cloud computing dates back to 1950s, where large scale mainframes were built to be used by different corporations and schools. The mainframe's hardware was installed in server room and users were able to access it via terminals, dedicated stations for accessing the mainframe. Mainframes were large and expensive and organizations could not afford a mainframe for each user, so it became common practice to allow multiple users to access the same data storage from any station.
Cloud computing can be divided into three groups:
	- Public clouds
	- Private clouds
	- Hybrid clouds
	
Public Cloud								Private Cloud													Hybrid Cloud
------------								-------------													------------
Resources shared between organizations		Resources dedicated to one organization							Combination of public and private cloud
Resources owned by a service provider		Resources owned by your organization and/or a service provider	Offers cloud bursting
AWS											Used by government and financial institutions					Combination of public and private cloud vendors
Microsoft Azure																								Cisco Meraki
Google Cloud Platform

What Is a Public Cloud?
Public clouds are the most popular cloud deployment solution. Resources are owned by a third-party cloud service provider and are physically located in the provider’s data center. No resources are located at the customer data center. Physical and virtual resources in the provider’s data center are also shared between multiple organizations. Customers and end users access these services over the Internet using a web browser.
Public clouds have multiple advantages:
	- No capital 		expenditures: There is no need to purchase hardware or software; services are on a pay-as-you-go basis.
	- No maintenance: 	Maintenance is provided by a cloud service provider.
	- Scalability: 		Resources in the data center can be scaled easily.
	- Reliability: 	High availability of services offers greater reliability.
Multiple vendors offer public cloud solutions:
	- Amazon Web Services
	- Microsoft Azure
	- Google Cloud Platform
	
What Is a Private Cloud?
Resources of a private cloud are used exclusively by a single organization. They can be physically located at an organization’s data center or in a cloud provider data center. Services in private cloud are always maintained on a private network and the hardware and software are dedicated to a single organization. Private clouds are often used by financial institutions, government agencies, and other organizations with business-critical operations.
Private clouds have multiple advantages:
	- Flexibility: 		Resources are dedicated only to your organization, so you can customize the environment based on your business needs.
	- Security: 		A higher level of security is available because resources are not shared with other organizations.
	- Scalability: 		Private clouds can still afford the scalability of the public clouds.

What Is a Hybrid Cloud?
Hybrid cloud is a combination of public and private cloud, often referred as “the best of both clouds”. Hybrid clouds combine on-premise infrastructure (private cloud) with public infrastructure (public cloud). In a hybrid cloud, data can be moved between private and public cloud for greater flexibility. An organization can use public cloud for low-security use cases and private cloud for high-security use cases. Hybrid clouds also offer so-called “cloud bursting.” Cloud bursting enables organizations to expand services from private to public cloud in high-demand situations.
Hybrid clouds have multiple advantages:
	- Cost effectiveness: 	With the scalability available in public clouds, you can pay additional fees only when needed.
	- Flexibility: 			You can use a public cloud for additional resources only when needed.
	- Control: 				Sensitive data can be kept in a private cloud, while other data can be kept is a public cloud.
Most vendors that offer public and private clouds also offer hybrid cloud deployment models.

Application Deployment Options
Most cloud computing services belong into one of these categories:
	- Infrastructure as a Service (IaaS)
		- IaaS is the most basic category of cloud computing services. You to rent the IT infrastructure (network devices, storage systems, servers and virtual machines) from a cloud provider.
	- Platform as a Service (PaaS)
		- PaaS refers to an on-demand environment for developing and testing software applications. It is designed to save time for developers when creating and deploying web or mobile applications.
	- Software as a Service (SaaS)
		- SaaS is a way of delivering software applications over the Internet, typically on a subscription basis. End users are only provided an application while service providers manage the underlying infrastructure and the application.

Edge Computing
Edge computing is a network solution that brings computing resources as close to the source of data as possible to reduce latency and bandwidth use. Today, more applications are moving to the cloud and multiple clouds are being deployed. The increased number of endpoints dramatically increases the volumes of data that need to be processed, and transporting the data to central locations for processing becomes expensive. At the same time, users want high-quality experiences, best possible application performance, and security across data. To solve these issues, a new service architecture is being introduced: edge computing, which is based on distributing computing capacity to the edge of the network.

Edge computing focuses on:
	- Lowering the latency between the end user device and a processing and storage unit to get better performance
	- Implementing edge offloading for greater network efficiency
	- Performing computations and reducing transport costs
Two examples of using edge computing include:
	- Radio access network (RAN)
	- 5G Core (5GC) network

Edge computing is a distributed network architecture that moves your compute, storage, communication, control, and decision making closer to the network edge or where the data is being produced to mitigate the limitations in the current infrastructure.
You do not necessarily have to do all tasks at the edge of your network, but there are some good use cases for this approach:
	- Bandwidth 				reduction: The cost that is incurred in sending large quantities of data can be reduced.
	- Filtering: 				Filtering allows you to capture only relevant data flows and transport.
	- Latency optimization: 	Some types of data are susceptible to latency and require more real-time data flows.
	- Partitioning: 			Partitioning helps to balance and allocate resources across the network.
	- Simplified applications:  Simplified applications help normalize data and the data organization process.
	- Dynamic changes: 			Data can be redirected based on content and priority.
	- Analytic support: 		Data can be used for analytics and higher-level systems.
	- Network efficiency: 		Edge computing lets you use the network more efficiently.
	
Three major architectural shifts underpin the emergence of the edge computing network infrastructure:
	- Decomposition: Network functions are separated (control/signaling and user/data) for optimization of resources.
	- Disaggregation into software and hardware: Software-centric solutions use off-the-shelf or white-box hardware, which can be procured separately.
	- Infrastructure convergence: Fixed and mobile networks share a common 5GC-based infrastructure for efficient operational practices.

Some organizations are testing edge computing at the cell site itself. At first glance, this approach might appear reasonable because it puts computing as close as possible to the mobile subscribers. However, several issues result:
	- It is operationally complex because of the typically large number of cell sites.
	- It is expensive because of enclosures, power, and HVAC needs. Specialized servers may be needed instead of tapping mass-scale production servers.
	- New trends in radio are for leaner cell-site architectures composed primarily of lean elements such as remote radio heads.
Note: Cloud radio access networks (C-RANs) do not have packet awareness at the cell site.

To determine the location of the edge computing node, consider these questions:
	- Can the more efficient location be the customer premises equipment (CPE)?
	- Can the more efficient location be located on the customer premises?
	- If the edge location is optimally in the network, is there enough low-latency queuing to the endpoint device?
	- For mobile access use cases, can the locations be mapped to a present or future C-RAN central unit location?

DevOps Practices and Principles
What is DevOps?
	- A change in operational approach and mindset
	- A cultural movement
What are some DevOps principles?
	- Iterative
	- Incremental
	- Continuous
	- Automated
	- Self-service
	- Collaborative
	- Holistic
	
The following are sole DevOps principles and their characteristics:
	- Iterative: This type breaks the working process of DevOps into smaller bits. This allows tests to be included in the early stages of DevOps and helps with faster error checks.
	- Incremental: 		Projects need to be developed in small and rapid incremental cycles.
	- Continuous: 		Merge the development (testing) and deployment into a single improved and simpler process.
	- Automated: 		Everything that can be automated should be automated. This adds speed and precision to the process.
	- Self-service: 	Every IT engineer should have the same development environment to develop and test projects.
	- Collaborative: 	the DevOps team needs to be united, work together and help each other during the entire DevOps life cycle.
	- Holistic: 		the DevOps process needs to be treated as a whole process, rather than just a couple of smaller tasks.

Some primary characteristics of these organizations that have adopted DevOps are:
	- Embracing new technologies
	- Embracing a collaborative culture
	- Maintaining a well-defined common goal among teams

Practicing DevOps
DevOps professionals come up with many practices and principles. Whatever these practices and principles are in the end, they should all help an IT organization deliver high-quality software and apps to end users.
The core practices are:
	- Automated provisioning
	- Automated release management
	- Continuous build
	- Continuous delivery
	- Continuous integration
	- Incremental testing
	- Self-service configuration

Life Cycle of DevOps:
	Place -> Code -> Build -> Test -> Release -> Deploy -> Operation -> Monitor - Repelate
	
To boost the productivity and the solidity of existing operations that utilize enablers like containerization, Software Defined Networking (SDN), Network Functions Virtualization (NFV), and much more, DevOps use a practice called automated provisioning. This practice helps them save time and decreases errors when deploying services with automated procedures that do not need human interventions.

To plan, manage, schedule, and control software built in different stages and environments, development teams use the release management process. The goal is to automate whatever you can automate, so DevOps should automate this process as well.

Continuous build is the first step in the continuous integration and continuous delivery (CI-CD) process. It is an automated build on every check-in, but it does not prove functional integration of the code that has been added or updated yet.

The next step is continuous integration (CI), which includes the first step (continuous build) and adds end-to-end integration tests and unit tests to prove the integration of the added code. Each change in code is built and checked by tests and other verifications to detect any integration errors as quickly as possible. The best practice is to implement small changes in code and to version control repositories frequently.

Continuous delivery (CD) is the final step in this ongoing process. It automates the delivery of software and applications to chosen infrastructure environments. DevOps mostly works in development and testing environments that are different from the production environment (for example, development and testing). CD provides an automated way to push the code changes to production.

Incremental testing is one of many integration tests. It has the advantage that errors are found undeveloped in a smaller collection. This approach makes it easier to detect the root cause of the errors. A test is done after each of the steps. There are many ways to approach this practice (top-down, bottom-up, functional incremental, and so on).

Self-service configuration is DevOps permits developers to deploy applications by themselves at their own pace. A task that was previously performed by the IT team. This approach reduces operational costs and increases efficiency, accelerates releases, and reduces the cycle times.

Implementing DevOps to resolve the gap between Dev and Ops is the right idea. However, shifting to DevOps comes with challenges of its own.

Organizations will probably deploy (projects, code, etc.), using the DevOps approach, onto some existing infrastructure populated with some legacy code. To avoid the manual alternatives of provisioning and modeling, organizations need to provide end users with self-service access to both legacy and new applications in a form of cloud sandbox platform. This ensures that every end user has the same clean environment to work in.

Secure applications and software are critical. A frequent problem in IT organizations is that security is usually addressed at runtime instead of addressing it as a part of the whole DevOps workflow. It becomes a secondary component of the DevOps lifecycle. Often organizations put security in second place because of the delivery time and a shortage of people working on a project. Whatever the reason may be, organizations should not overlook security, regardless of the situation. The security team should be engaged early in the development and be present at every step of the DevOps workflow. This is the only way to ensure the application or software is secure.

Documentation is one of the least favorite things for IT engineers, but when implemented properly it can be of great value. Source code, design, and user documentation need to be consistent on every platform. A solution can be provided in a form of web pages where documentation is regularly updated and maintained and is accessible to everyone working on the project. Writing documentation can also be automated with the right kind of tools. DevOps-based documentation needs to provide all DevOps teams with a familiar and confident source of information about the project.

Sometimes organizations implement multiple tools at once, just because they are new. This is also known as the Shiny Object Syndrome. Although new tools can appear very useful at first, they still need to be chosen carefully. Therefore DevOps tools should be adopted only if and when they are needed. Proper tool management can be very helpful. Adopt only the tools that you need for a specific project and always keep the focus on your team and goals over the new and shiny tools.

DevOps teams sometimes have issues with executive support. Properly structured teams can function by themselves (almost) unassisted, but they still require help from management, particularly in the early parts of the DevOps lifecycle.. Sometimes they cannot obtain the right tools, get proper training or even get adequate support from the leadership. Changes in structure must happen at all positions and include enhanced levels of communication and commitment from everyone. Since there is no single formula to implement DevOps at an organization, management needs to discover the best strategic plan for their team.

A CI-CD pipeline consists of:
	- Source code repository
	- Build stage (source code, dependencies, and static code analytics)
	- Test stage (unit and integration tests [end-to-end tests])
	- Deploy stage (packaging, staging, and production)

CI/CD Pipeline process:
Step 1:
The pipeline can be triggered manually and scheduled or automated with a commit by DevOps engineer. The source code repository notifies the CI/CD tool, which then runs the matching pipeline.
Step 2:
A DevOps engineer commits new (or updated) code with the help of Git, which then triggers a CI/CD pipeline. Git is the most commonly-used modern version control system. It has a distributed architecture, instead of only one single point for the full version history of software (as was the case with the older version control systems like Apache subversion [SVN]).
Step 3:
A runnable instance is built during the build stage from the source code, along with all its dependencies that the code needs. Code written in statically typed programming languages (such as C, C++, Java, or Go) needs to be compiled first. Code written in dynamically typed programming languages (such as Python, PHP, and JavaScript) does not require this step. Before the code is first executed, a code analytics tool, such as a linter analyzes the source code to signal any errors, bugs, stylistic errors, and suspicious constructs. This process is called the static code analytics. If the build stage does not pass for any reason whatsoever, the core configuration of the project needs to be addressed instantly.

The next steps in the pipeline process are the test and deploy stages.
	
To inspect the behavior of the software, code needs to be validated with different tests. Depending on the scale of the project, the tests can be run either in a single stage or in multiple stages. The first tests should be unit tests, although the developers should test their code as much as they can before they commit it. Unit tests are faster and demand less resources than integration tests. Execute end-to-end tests only after the unit tests are successfully completed. It is important that this stage provides useful and fast feedback. These feedback ensures that the DevOps team is more efficient and needs less time to find any bugs and errors. Note that unit and end-to-end testing can also run in parallel if they do not consume too much time and resources.

The deploy stage begins after the pipeline has built a runnable instance of the software that has passed all tests. There can be many different deploy environments, but most pipelines consist of just two:
	- Staging server: Used by the DevOps team internally
	- Production server: Used by end users or customers

The staging environment is designed to be as close to the real-life production environment as possible. This characteristic enables DevOps to deploy work-in-progress applications or software to staging for additional tests and reviews. The production environment is for the final working product only and should only contain that.

Before deployment is finished, the project can be packaged. For example, a Docker image can be built and pushed to the docker registry, or for open-source projects, the whole project can be zipped into a single file for easier downloading. Packaging can be done in multiple stages (from build to deploy), depending on the projects and needs.

The deployment to both environments can be automated, but because the production server needs to be handled with extreme caution, many DevOps teams choose to deploy to production manually.

Good practices for a CI-CD pipeline:
	- Always use the same environment.
	- The master repository should hold only up-to-date, documented, and working code.
	- Use code reviews to merge requests to the test and master repositories.
	- Developers should maintain separate repositories.

CI-CD Pipeline Configuration
There are a lot of tools that can help you implement a CI-CD pipeline. Tools based on Git (GitLab, GitHub, and Bitbucket) all provide you with a code repository as well as with the CI-CD pipeline, while tools like Jenkins or CircleCI do not provide software repositories. Tools such as Jenkins, CircleCI, and so on rely on code repositories, hosted somewhere else, to trigger the pipeline with the help of application programming interfaces (APIs).

Two of the most common CI/CD pipeline tools are Jenkins and GitLab.
Jenkins is an open-source automation server with a lot of plugins for the CI/CD pipeline. Jenkins uses a file called Jenkinsfile for the configuration. Jenkinsfiles are written in a programming language called (Apache) Groovy. Groovy takes the syntax of Java and combines the features of Python (and other scripting languages), such as defining a variable without specifying a type.

Jenkins main difference from GitLab is that the extensions of native functionalities are done with the help of plugins (which usually translates to expensive maintenance). GitLab on the other hand is open-core, which means that any changes done to the GitLab's codebase are automatically tested and maintained.

GitLab also provides a pipeline as code, which is an upgrade from the GUI type of pipeline. This approach gives DevOps teams more freedom in the pipeline process. It also makes rollbacks a lot easier, and it provides a built-in lint tool that makes sure the YAML file is valid, version control, and audit trails.

A GitLab pipeline consists of:
	- Commit: 		A change in the code
	- Job: 			Runner instructions
	- Pipeline: 	A group of jobs divided into different stages
	- Runner: 		A server or agent that implements each job separately, which can spin up or down if needed
	- Stages: 		Parts of a job (for example, build or tests). Multiple jobs inside the same stage are executed in parallel.
Whenever a build fails at any step of the pipeline, a notification mail is automatically sent to the committer.

Essential Bash Commands for Development and Operations
Bash provides that CLI for Unix-like operating systems. It is both an interactive command language and a scripting language that contains mechanisms found in other computer programming languages such as:
	- Loops
	- Variables
	- Conditional statements
	- Functions
	- Input and output (I/O) commands

Output of the env command:
	- Type of shell (for example, Bash)
	- Current user
	- Default paths
	- etc.
	
By default, any variables that are created in a parent process are not available to the child process. You will need to use the export command for that to be possible. The command export ensures that exported variables in the parent process can be used in the child process (or subshells for that matter).

--> The following is a collection of Bash Programming for practice (important to read and follow):

#!/bin/bash

# -- Special Characters Section
# ===============================
#echo "Hi there, how is it going? What sort of day are you having? Any goo stuff going on?" # This will print whatever to the screen echo "Do YOU SEE THIS"
#echo "This is a Differenttt line"
#echo "tallahdsalgjal;ghasl;gkal; # blalalbalala"

#name=tea
#echo "The word $name contains ${#name} chars"
#echo $(( 2#111 ))
#echo "hey there"; echo "Are you there"

#var=10
#if [ "$var" -gt 0 ]; then echo "YES"; else echo "NO"
#fi

#colors="red black white"
#echo $colors
#for col in $colors
#do
#	echo $col
#done

#let "y=((x=20, 10/2))"
#echo $y

#var=DSLCoNnEctioN
#echo ${var,}
#echo ${var,,}

#echo ""Linux is Awesome""
#echo "/"Linux is Awesome""
#echo "\"Linux is Awesome"\"

#let val=500/2
#val2=`echo $val`
#echo $val2

#var=20
#if [ "$var" -lt 15 ]
#then :
#else 
#    echo "$var"
#fi

#var=100
#if [ "$var" -gt 99 ]
#then :
#else
#	echo $var
#fi

#var=10
#if [ "$var" != 0 ]
#then
#	echo "NOT"
#else
#	echo "Yes"
#fi

#let var=100*10
#let var2=100**3
#echo "$var $var2"

#var1=10
#echo $(( var2 = var1<20?1:0))  # This line can also be written as the following if statement:

#if [ "$var1" -lt 20 ]
#then
#	var2=1
#else
#	var2=0
#fi
#echo $var2

#var=5
#(var=10;)
#echo $var

#Colors=(red white brown purple blue)
#echo \${test1,test2,test3}\$
#echo \"{test1,test2,test3}\"
#echo {0..10}

#var1=1
#var2=2
#{
#var1=11
#var2=12
#}
#echo "$var1 $var2"

#var=10
#if [ "$var" -gt 0 ] || [ "$var" -eq 10 ]
#if [ "$var" -gt 0 ] && [ "$var" -eq 10 ]
#if [ "$var" -gt 0 ] & [ "$var" -eq 10 ]
#if [ "$var" -gt 0 ] or [ "$var" -eq 10 ]

#then
#	echo "One or both conditions are true"
#else
#	echo "Neither one of the conditions is true"
#fi

#let var=5%4
#echo $var

#someWord=tEsT
#echo ${someWord^}
#echo ${someWord^^}

# -- Variables and Parameters Section
# ===================================
#var=10
#echo var
#echo $var

#var=10
#unset var 
#echo $var

#var1=4
#echo "type in some value"
#read var2
#echo $var2

#for var in 1 2 3
#do
#	echo " Value of var is $var"
#done

#var="T r a l a la      lalala l"
#echo $var  	# This will not print the with spaces before lalala
#echo "$var" 	# The quotation will keep and print the spaces before lalala

#var="test1 test2 test3"
#echo "$var"
# Also the following will be correct but if you remove "\" will get an error
#var=test1\ test2\ test3
#echo "$var"
# The following in NULL but it is also correct it just does not print anything
#var=
#echo "$var"

#var1=11 var2=22 var3=33 # Not recommended, you can do them saparetly as follows:
#var1=11
#var2=22
#var3=22
#echo "$var1 $var2 $var3"
# The following will assing NULL to Var, the 9, then changed to 10, then remove the value of var
#var=
#echo "$var"
#var=9
#echo "$var"
#var=10
#echo "$var"
#unset var
#echo "$var"

#var=			# This is a 0
#let "var += 10"		# This is 0 + 10 = 10
#echo "$var"		# This echoes 10
#let "var = var + 10"	# This is 10 = 10 + 10
#echo "$var"		# This echos 20

#hi=`echo test`
#echo "$hi"

#hello=`ls -larps /home/ubuntu/`
#echo "$hello"

# You will need to add "$" if you want to put your varibles inside ()
#hello2=$(ls -larps /home/ubuntu/)
#echo "$hello2"

#num=1100
#let "num -= 10"
#echo "$num"

#var=${num/10/B}
#echo "$var"

#num=10
#var=${num/10/A}
#echo "$var"

#var=A0
#let "var += 1"
#echo "$var"

#var=hey1100
#echo "$var"
#num=${var/hey/200}
#echo "$num"

#var1=
#echo "var1=$var1"
#let "var1 += 10"
#echo $var1

#MIN=10
#if [ -n "$1" ]
#then
#	echo "1st one is $1"
#fi
#
#if [ -n "$2" ]
#then
#	echo "2nd one in $2"
#fi
#
#if [ -n "$3" ]
#then
#	echo "3rd one is $3"
#fi
#
#if [ -n "$4" ]
#then
#	echo "4th one is $4"
#fi
#
#if [ -n "$5" ]
#then
#	echo "5th one is $5"
#fi
#
#if [ -n "$6" ]
#then
#	echo "6th one is $6"
#fi
#
#if [ -n "$7" ]
#then
#	echo "7th one is $7"
#fi
#
#if [ -n "$8" ]
#then
#	echo "8th one is $8"
#fi
#
#if [ -n "$9" ]
#then
#	echo "9th one is $9"
#fi
#
#if [ -n "${10}" ]
#then
#	echo "10th one is ${10}"
#fi
#
#echo "List of arguments: "$*""
#echo "Name of Script: \""$0"\""
#if [ $# -lt "$MIN" ]
#then
#	echo "Not enough arguments, need 10 arguments to run"
#fi

# -- Return Values Section
# ========================
#echo "tralalala"
#ThisisNasser  # 127 error code command not found because it is not a command
#echo $?       # Get the status of last command execution

#NO_OF_ARGS=2
#E_BADARGS=85
#E_UNREADABLE=86
#if [ $# -ne "$NO_OF_ARGS" ]	#if the number of command line argument not equal to NO_OF_ARGS, we want something to happen which if following:
#then
#	echo "Usage: `basename $0` testFile1 testFile2"  # "$0" will print the name of the script
#exit $E_BADARGS
#fi
#
#if [[ ! -r "$1" || ! -r "$2" ]]
#then
#	echo "Files are not real"
#	exit "$E_UNREADABLE"
#fi
#
#cmp $1 $2 &> /dev/null
#
#if [ $? -eq 0 ]
#then
#	echo "Files are the same"
#else
#	echo "Files are NOT the same"
#fi
#exit 0

# -- Condition Statements
# =======================
#num=1
#if [ "$num" -gt 0 ]
#then
#	if [ "$num" -lt 5 ]
#		then
#			if [ "$num" -gt 3 ]
#				echo "GT 0, LT 5, GT 3"
#			fi
#	fi
#elif [ "$num" -eq 0 ]
#	echo "EQ 0"
#else
#	echo "I HAVE NO IDEA"
#fi

#var=/home/ubuntu/wood.txt
#if [[ -e $var ]]
#then
#	echo "File exists"
#else
#	echo "File does not exist"
#fi

#(( 2 < 1 ))
#echo "Exit status is $?"

#NO_OF_ARGS=2

#E_NOARGS=65
#E_BADARGS=85
#E_UNREADABLE=86
#E_NOFILE=87
#E_SIZE=89
#
#if [[ -z "$1" && -z "$2" ]]		# -z = zero.  if zero or no argument were givin
#then
#	echo "No Arguments given"
#	exit $E_NOARGS
#fi
#
#if [ $# -ne "$NO_OF_ARGS" ]
#then
#	echo "USAGE:`basename $0` file1 file2"
#exit $E_BADARGS
#fi
#
#if [[ ! -e "$1" || ! -e "$2" ]] 	# -e = exist.  if file 1 or 2 is not exist. ! -e means if NOT exist
#then
#	echo "Files do not exist"
#	exit $E_NOFILE
#elif [[ ! -f "$1" || ! -f "$2" ]]	# -f = real file.  ! -f means if there is no regular ril or a device file.
#then
#	echo "Files need to be regular files"
#	exit "$E_NOFILE"
#elif [[ ! -r "$1" || ! -r "$2" ]]	# -r = readable file 
#then
#	echo "No read permission"
#	exit "$E_UNREADABLE"
#
#elif [[ ! -s "$1" || ! -s "$2" ]]	# -s = if the file is not a zero size
#then
#	echo "Files are zero-size"
#	exit $E_SIZE
#fi
#
#cat $1 $2 | sort > file.txt
#
#if [ $? -eq 0 ]			# $? = last command.  Testing if last command was executed successfully or not. $? -eq 0 means if the execution of the script resulted in "0" means successful, print the execution was successful.
#then
#	echo "Execution of script was successful"
#	cat file.txt

#else
#	echo "Execution of the script failed"
#fi

## Note.  You can use "man command" to read about a command.
## better. You can use "info command" to read more precise information about a command

# Built in Variables Section
# ==========================
#echo $BASH
#/bin/bash
#echo $BASH_ENV

#echo "$$"  			# Echos the process ID of the script
#echo $BASH_VERSION
#
#for n in {0..5}
#do
#	echo "BASH_VERSINFO[$n] = ${BASH_VERSINFO[$n]}"
#done
#
#echo $PATH
#echo $CDPATH
#echo $EDITOR
#echo $UID  	# The ID number of the current user
#echo $EUID	# The effective user ID, It can be the same as the above but does not have to be the same
#
#ROOT_UID=0
#echo -n "YOU ARE: "
#if [ "$UID" -eq "$ROOT_UID" ]
#then
#	echo "root. YOUR \$UID = $UID"
#else
#	echo "user. YOUR \$UID = $UID"
#fi

#someFunction()
#{
#	echo "Function name is $FUNCNAME"
#}
#someFunction
#echo "Outside, \$FUNCNAME = $FUNCNAME"

#echo $GROUPS
#echo ${GROUPS[0]}
#echo $HOME
#echo $HOSTNAME
#
#if [[ $HOSTNAME && $USER && $HOME ]]
#then
#	echo "HOSTNAME: $HOSTNAME"
#	echo "USER: $USER"
#	echo "HOME: $HOME"
#	echo "Variable are set"
#else
#	echo "Variables are not set"
#fi
#
#echo $HOSTTYPE
#echo $MACHTYPE

# IFS: Internal Field Separator to reconise where word boundaries are.  The default value is a whitespace character
# You can define where the word begins and where it ends
#colors1="red-brown-orange"
#colors2="red+brown+orange"
#echo "IFS=-"
#IFS=-		# Removes the word separator "-" form colors1 varialble
#echo "IFS=+"
#IFS=+		# Removes the word separator "+" from colors2 variable
#echo $colors1
#echo $colors2

#colors1="red:::::::::brown:::orange"
#colors2="red:brown:orange"
#echo "IFS=:"
#IFS=:		# The machine removes out the first : from colors1 and print out space for the rest of the :s. for colors2, the machine prints out space for the :s 1 and 2
#echo $colors1
#echo $colors2

#colors1="red:::::::::brown:::orange"
#colors2="red:brown:orange"
#echo "IFS=:"
#IFS=:		# The machine removes out the first : from colors1 and print out space for the rest of the :s. for colors2, the machine prints out space for the :s 1 and 2
#echo $LINENO	# Prints out the line numbers up to this echo $LINENO command in the all the script
#echo $colors1
#echo $colors2

#echo $PWD	# Current working directory
#echo $OLDPWD	# Echoing out the previous working directory
#echo $OSTYPE	# Type of the OS like (linux-gnu)
#cat logfile | sort	# Piping sort with cat.  for more infor in sort, type man sort OR better info sort
#echo ${PIPESTATUS[*]}   # Prints out the exit status of the Pip and Cat command

#echo "Some Question"
#read 
#echo "The answer to the question is:  $REPLY"
#
#echo "Some other question"
#read var
#echo "your answer is:  $var"

#LIMIT_TIME=12
#TIME_INTERVAL=3
#echo "This script will run for $LIMIT_TIME seconds."
#echo "Press Ctrl-c to exit before the time limit."
#while [ "$SECONDS" -le "$LIMIT_TIME" ]
#do
#	echo "This script has been running for $SECONDS seconds"
#	sleep $TIME_INTERVAL
#done

#declare -r var_r=5		# Declare Read Only variable
#echo "\$var_r = $var_r"
#declare -i var_i=10		# Declare integer variable
#echo "\$var_i = $var_i"
#var_i=blue
#echo "\$var_i = $var_i"

#declare -i x			# Declare x as an intefer variable
#x=8/3
#echo "\$x = $x"

#declare -a x=(1 2 3 4 5)	# Declare x as an array of five iterations
#for i in {0..4}
#do
	#echo "x[$i]"		# This will print the arrau as index such as x[0] to x[4]
#	echo "${x[$i]}"		# This will print the array without indexing such as 1 2 3 4 5
#	let "i += 1"
#done

#declare -f someFunction		# Declaring a function named someFunction
#someFunction()
#{
#echo "Hey! Are you having a good time? Yes, no, maybe? Or you just do not care at all"
#}
#someFunction

# Declaring a variable that can be exported out from the script with -X
#declare -x var_x=10
#echo "$var_x which is exported variable has been exported out of the script"

# Function that generates random numbers between 0 and 32767 which is 10**2 - 1
#MAX=100
#i=1
#while [ "$i" -le $MAX ]
#do
#	n=$RANDOM
#	echo $n
#	let "i += 1"
#done

# For Loop Section
# ================
# There are three different loops type in BASH (For Loop, While Loop, and Until Loop)
# Breakes and continue commands control loops
# Control the flow of loops command execution with case and select_constructs.
# Nested loops
#for i in 1 2 3 4 5
#do
#	echo "<---- Outer Loop $i --------------------------------"
#	for j in 1 2 3
#	do
#		echo "----> Inner Loop $j !!!!!!! Outer loop iteration $i"
#	done
#done

# While loop - test a condition and then loop as long as the condition is true.
# The while loop can have multiple conditions but only the final condition that determine the loop termination
#a=unset	#unset is just a string value, not a command it just a choosen word, you change it to anything
#prev=$a
#while 	echo "Previous variable = $prev"
#	echo
#	prev=$a
#	[ "$a" != end ]
#do
#	echo "Input end to exit or anything else to go on!"
#	read a
#	echo "variable = $a"
#done

# Until loop
#until [ "$n" = end ]
#do
#	echo "Input end to exit or something else to move on"
#	read n
#	echo "$n"
#done

# Break and contiune statements
#UPPER_LIMIT=9
#echo "Numbers 1 to 10 (but not 3 and 11)."
#n=0
#while [ "$n" -le "$UPPER_LIMIT" ]
#do
#	n=$(($n+1))
#	if [ "$n" -eq 3 ] || [ "$n" -eq 11 ]
#	then
#		continue
#	fi
#	echo -n "$n "
#done
#echo
#n=0
#while [ "$n" -le "$UPPER_LIMIT" ]
#do
#	n=$(($n+1))
#	if [ "$n" -gt 2 ]
#	then
#		break
#	fi
#	echo -n "$n "
#done
#echo
#
#for i in 1 2
#do
#	echo "Loop 1: iteration $i"
#	for j in 1 2 3
#	do
#		echo -e "\tLoop 2: iteration $j"	# -e tells echo command to print a tab donated by \t
#		for k in 1 2 3 4
#		do
#			echo -e "\t\tLoop 3: iteration $k"
#			if [ "$k" -eq 2 ]
#			then
#				break 2
#			fi
#		done
#	done
#done
#for i in 1 2 3 4 5
#do
#	echo "Loop 1: iteration $i"
#	for j in 1 2 3
#	do
#		echo -e "\tLoop2: interation $j"
#		for k in 1 2 3 4
#		do
#			echo -e "\t\tLoop 3: iteration $k"
#			if [ "$k" -eq 2 ]
#			then
#				continue 3
#			fi
#		done
#	done
#done
#exit 0

# Case and Select construct
#echo -n "Enter a letter or a digit: "
#read a
#case "$a" in
#	[[:lower:]] ) echo "$a is a lowercase letter";;
#	[[:upper:]] ) echo "$a is an uppercase letter";;
#	[0-9]	    ) echo "$a is a digit";;
#	*           ) echo "$a is a special character";;
#esac
#exit 0

# Select construct
# PS3 Prompt String 3 one of the shell prompt avaialble in BASH for linux.  You alos have PS1, PS2 and PS4.  PS commands are written in script as it works hand in hand with the SELECT command in order to privide custom prompt for the user to selece a particular value.  In this case it will privie 1) - 5) index for each of the following colors from the color variable in the SELECT command.  So when the user select the will select a number that representing a particular color.

#PS3='Pick a color: '
#echo
#select color in "brown" "grey" "black" "orange" "red"
#do
#	echo "You selected this color: $color"
#	break
#done

# Internal Commands Section
# =========================
# printf or print format or format printing
#declare -r PI=3.1415926
#printf "First decimal of PI is %1.1f\n" $PI
#printf "First and second decimals of PI is %1.2f\n" $PI
#printf "First, second and third decimals of PI is %1.3f\n" $PI
#printf "First, second, third and fourth decimals of PI is %1.4f\n" $PI
#printf "First, second, third, fourth and fifth decimals of PI is %1.5f\n" $PI
#printf "First, second, third, fourth, fifth and sixth decimals of PI is %1.6f\n" $PI
#printf "First, second, third, fourth, fifth, sixth, and seventh decimals of PI is %1.7f\n" $PI

# Read command
up=$'\x1b[A'	# '\x1b[A' is the key code that recognizing the UP arrow key 
down=$'\x1b[B'	# '\x1b[B' is the key code that recognizing the DOWN arrow key
left=$'\x1b[D'  # '\x1b[D' is the key code that recognizing the LEFT arrow key
right=$'\x1b[C' # '\x1b[C' is the key code that recognizing the RIGHT arrow key

#read -s -n3 -p "Press an arrow key: " arrow
# -s option does not echo input, -n3 meand accept only 3 characters, -p echos the prompt before giving the input.
#case "$arrow" in
#	$up) echo "You pressed up";;
#	$down) echo "You pressed down";;
#	$left) echo "You pressed left";;
#	$right) echo "You pressed right";;
#esac

#echo "Enter a string"
#read -r var		# -r means to read the user argument entered as it is
#echo "$var"

# Read from a file
#echo "Read"
#while read var
#do
#	echo "$var"
#done <opendaylight-inventory.yang

# Eval and Set commands
# The eval commmand combine all the arguments in expression as a string, evaluate them and execute them
#if [ ! -z $1 ]	# This condition checks if the parameters that are passed to the script is not empty
#then
#	process="ps -e | grep $1"
#fi
#eval $process

# The set command is used to change the values to a value of shell options and variables.  Options can be process by minus (-) or a plus (+) signs.  - enable that options and + disable that option
#set +o history   # Unset the history command
#set -o history	 # Set the history command
#echo "Before setting the parameters"
#echo "\$1 = $1"
#echo "\$2 = $2"
#set `echo "thee four"`
#echo "\$1 = $1"
#echo "\$2 = $2"

#Set
#var="1 2 3"
#echo $var
#set -- $var
#i=1
#while [ "$i" -le $# ]
#do
#	echo -n "\$$i = "
#	eval echo \$$i
#	(( i++ ))
#done
#set --
#echo "\$1 = $1"
#echo "\$2 = $2"
#echo "\$3 = $3"

#Unset
#var='1 2 3'
#echo "$var"
#unset var
#echo "$var"

#getopts command is use to phrase argument that are passed through the command line
#while getopts :dm option
#do
#	case $option in
#		d) d_option=1
#		;;
#		m) m_option=1
#		;;
#		*) echo "Usage: -dm"
#	esac
#done
#day=`date | awk '{print $1 " " $3}'`
#if [[ ! -z $m_option ]]
#then
#	echo "Date:     $day"
#fi
#month=`date | awk '{print $2}'`
#if [[ ! -z $m_option ]]
#then
#	echo "Month:    $month"
#fi
#time=`date | awk '{print $4}'`
#if [[ ! -z $m_option ]]
#then
#	echo "Time:     $time"
#fi
#timeZone=`date | awk '{print $5}'`
#if [[ ! -z $m_option ]]
#then
#	echo "TimeZone: $timeZone"
#fi
#year=`date | awk '{print $6}'`
#if [[ ! -z $m_option ]]
#then
#	echo "Year:     $year"
#fi

#shopt, types, jobs, and disown commands
#shopt: is a command for correcting MISSPELLING error in additio to a lot of option 
#You can enable shopt with the -s (set) option and -u (unset) to disable it.
#write shopt and press enter to see other usefull options

#type command is used to tell you what the command does or if it is a shell command or not
#jobs command control jobs running in the background
#disown command ends jobs that are running in the background show by jobs command.

#fg and kill command (forground)
#fg command is used to bring background precesses fo forground

#echo "Waiting for 5 seconds"
#sleep 5 &
#wait
#times
#echo "done"

#ls ()
#{
#	echo "I do not know whatever"
#}
#ls
#echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
#command ls

# Regular Expression
# ==================
# Building find words puzzle
#E_NOPATTERN=71
#DICT=/usr/share/dict/linux.words
#if [ -z "$1" ]
#then
#	echo
#	echo "Read the Usage that follows:"
#	echo "`basename $0` \"pattern,\""
#	echo "Where \"pattern\" is in the form"
#	echo "ooo..oo.o..."
#	echo
#	echo "The o's are letters you already know"
#	echo "and the periods are missing letters."
#	echo "Letters and periods can be in any position"
#	echo "For example: w...i....n"
#	echo
#	exit $E_NOPATTERN
#fi
#grep ^"$1"$ "$DICT"

# Extended Regular Expressions
#E_BADARGS=65
#if [ $# -eq 0 ]
#then
##	echo "Usage: `basename $0` file"
#	exit $E_BADARGS
#else
#	for i
#	do
#		sed -e '1,/^$/d' -e '/^$/d' $i
#	done
#fi

# Globing
#ls -l woo?.txt	# ? replaces only one letter and list the file or folder
#ls - [fw]*	# [fw]* will list any file or folder starts with "f" or "w" 
#ls -l [e-q]*	# [e-q]* will list any file or folder that start with either "e" or "q" and any other letter in between and it also will be Recursive which means list folder and files inside a directory
#ls -l {w*,*oo*} # {w*,*oo*} will list any file or folder that start with "w" and can end with whatever and "*oo*" lists any folder or files that can start and end with any letter but they must have "oo" letters in the middle.  This is also a RECURSIVE command.
#echo *    # will list all the folders but not in a listed way like ls -rck
#echo w*   # will list all folder and file starts with "w"
#echo woo?.txt	# will list wood.txt for example

# Using "nullglob" argument to show hidden files
#for file in *
#do
#	ls -la "$file"
#	shopt -s nullglob  # To list the dot (.) or (..) folder you have to specify "nullglob" argument, because this will negate their hidden settings
#done

# STDIN, STDOUT, STDERR Redirection
# =================================
#file=wood.txt
#echo "The words in this line is going to be redirect to file $file" 1> $file
#echo "The words in this line is going to be redirect and appended to file $file" 1>> $file
#something 2>> wood.txt  # Redirects STDERR error to wood.txt file
#cat wood.txt
#somthing >> wood1.txt 2>&1  # Creates and Redirects STDRR errors to wood1.txt file
#cat wood1.txt 

# Assigning a file descriptor
#echo 12345 > fd.txt   # Create fd.txt file is not exist and echoes 12345 text to it
#exec 3<>fd.txt	      # Opens fd.txt and assign file descriptor of 3 to it
#read -n 2 <&3	      # Reads only 3 characters from a particular file in this case fd.txt      
#echo -n . >&3	      # Write a decimal point there
#exec 3>&-	      # Close file descriptor 3
#cat fd.txt	     

# Redirection using the exec command
#E_FILE_ACCESS=70	# This is the exist status if the file cannot be read
#E_BADARGS=71		# This is the open file is not specified
#if [ ! -r "$1" ]	# If the file is not readable then do the following:
#then
#	echo "Can't read from input file!"
#	echo "Usage: $0 input-file output-file"	# echoes proper usage of the script
#	exit $E_FILE_ACCESS			# Give the exist status 70
#fi
#if [ -z "$2" ]		# Check if the second "$2" parameter is not passed to the script
#then			# if not echos the proper usage of the script and give the exist status of 71
#	echo "Specify output file!"
#	echo "Usage: $0 input-file output-file"
#	exit $E_BADARGS
#fi
#exec 4<&0	# link file descriptor "4" with standard INPUT "4<"
#exec < $1	# Read from input file
#exec 7>&1	# Link file descriptor "7" wit standard OUTPUT "7>"
#exec > $2	# Write to the second file that we will pass to the script
#tr a-z A-Z	# Transform "tr" all LOWERCASE characters "a-z" to UPPERCASE characters "A-Z" 
#exec 1>&7 7>&-	# Restore standard OUTPUT and close the file descriptor 7
#exec 0<&4 4<&-	# Restore standard INPUT and close the file descriptor 4
# The following will execute the script and chain cat the file
#./first.sh file.txt wood.txt && cat file.txt && cat wood.tx

# Subshell
#count=0
#exec 3<> wood.txt	# Open wood.txt file and assign file descriptor of 3 to it
#while read line <&3	# Start reading from the file and begin the loop
#do
#{
#	echo "$line"	# Echo one line at a time
#	(( count++ ))   # Increment the numbers of line 
#}
#done			# End the loop
#exec 3>&-		# Close the file descripto and output the correct numbers of line that was read
#echo "Numbeer of read lines is $count"

# Function Section
# ================
#function1 ()
#{
#	echo "Call \"function2\" from \"function1\"."
#	function2
#}
#function2 ()
#{
#	echo "This is \"function2\""
#}
#function1

#function1 ()
#{
#	echo "This is function 1"
#	function2 ()
#	{
#		echo "This is function 2"
#	}
#	function3 ()
#	{	
#		echo "This is function 3"
#	}
#}
#function1
#function2
#function3

#_()	# Function name can be anything. Here its name is _
#{
#	echo "First arg is $1, Second arg is $2"
#}
#var1=10
#var2=20
#_ var1 var2

# Exit and Return in functions
#E_PARAM_ERR=250
#EQUAL=251
#function_max()
#{
#	if [ -z "$2" ]	# Checks if there is a second parameter
#	then
#		return $E_PARAM_ERR  # If it does not have a second parameter returns error code 250 and the rest of the code will be terminated.
#	fi
#	if [ "$1" -eq "$2" ]	# if the tow parameters are EQUAL returns EQUAL 250.  if they are equal the function will stop here and the rest of the code will be terminated
#	then
#		return $EQUAL
#	else
#		if [ "$1" -gt "$2" ] # Check if Param 1 is greater that the Param 2
#		then
#			return $1    # if it is greater return arg 1
#		else
#			return $2    # if it is not greater return arg 2
#		fi
#	fi
#}
#function_max 12 13	# The function is called with three values
#return_value=$?		# Exit status defined above is assigned to $?
#if [ "$return_value" -eq $E_PARAM_ERR ]
#then
#	echo "Function needs two parameters"
#elif [ "$return_value" -eq $EQUAL ]
#then
#	echo "Numbers are equal"
#else
#	echo "Max number is $return_value"
#fi

#ARGS=1
#E_BADARGS=85
#FILE=/etc/passwd
#pattern=$1
#if [ $# -ne "$ARGS" ] # Check if the argument is less that 1, if it is not exit with BADARGS
#then
#	echo "Usage: `basename $0` USERNAME"
#	exit $E_BADARGS
#fi
#get_real_name()
#{
#	while read line
#	do
#		echo "$line" | grep $1 | awk -F":" '{ print $5 }'
		# while reading /etc/passwd file, grep the pattern, pipe it to -F (Field delimeter) witl a then a ":" instead of a space and print out field number 5
#	done
#} <$FILE
#get_real_name $pattern


# Arrays section
# ==============
#arr[0]=20
#arr[1]=30
#echo -e "${arr[0]} \n${arr[1]}"

#declare -a arr
#arr=( 10 20 30 40 50 50 60 60 70 70 80 80 90 10 20 30 40 ) # initiate the array
#echo -e "${arr[0]} \n${arr[1]}"
#echo -e "${arr[2]} \n${arr[3]}"
#echo -e "${arr[4]} \n${arr[5]}"
#echo -e "${arr[6]} \n${arr[7]}"
#echo -e "${arr[8]} \n${arr[9]}"
#echo -e "${arr[10]} \n${arr[11]}"
#echo -e "${arr[12]} \n${arr[13]}"
#echo -e "${arr[14]} \n${arr[15]}"

#arr=([0]=first [1]="Second" [7]=45)
#echo -e "${arr[0]} \n${arr[1]} \n${arr[7]}"

# Bash allows arrays operations on variables, however this array is going to be seen as one single element
#a=The*PrintsAllArrayAnd0PrintsTheFirstArrayAndLastEchoPrintsTheTotalElementInTheArray=1
#echo ${a[*]}
#echo ${a[0]}
#echo ${#a[@]}

#arr=( Nasser Mustafa Abdelghani Mohammad Fath Elbab Elanani )
#echo "${arr[0]}" "${#arr[0]} Letters" 
#echo "${arr[1]}" "${#arr[1]} Letters"
#echo "${arr[2]}" "${#arr[2]} Letters"
#echo "${arr[3]}" "${#arr[3]} Letters"
#echo "${arr[4]}" "${#arr[4]} Letters"
#echo "${arr[5]}" "${#arr[5]} Letters"
#echo "${arr[6]}" "${#arr[6]} Letters"

#declare -a colors
#echo "Your favourate colors separated by space: "
#read -a colors	# -a allows assignment of elements in array read command you can pass multiple elements
#count=${#colors[@]} # represents the count of an array
#i=0
#while [ "$i" -lt "$count" ]
#do
#	echo ${colors[$i]}
#	(( i++ ))
#done
#echo ${colors[*]} # echoes out all the element [*] of array into a single line
#unset colors[2]  # delets the third elements of an array
#echo ${colors[*]} # echoes out all the element in a single line again in order to see the effect of this
#unset colors
#echo "No colors. Colors gone"
#echo ${colors[*]}
 
# There cannot be an empty array. it can be array with empty element
#arr1=()
#echo ${arr1[*]}
#echo ${#arr1[*]}

# It is possible to load a content of a file into an array
#FILE=wood.txt
#declare -a arr_file
#arr_file=( `cat "$FILE"` )  # This will load the content of wood.txt into arr_file array 
#echo ${arr_file[*]}
#size=${#arr_file[*]}
#echo "Array size is $size"

# Sort an array using buble sort algorithm
#swap()	
#{
#	local tmp=${colors[$1]}  # Assing the value of the first element to the temporary tmp holding location
#	colors[$1]=${colors[$2]}  # First element takes the value of the second element
#	colors[$2]=$tmp		  # Second element takes the value of the tmp location that originally have the value of the first element which means SWAP() the elements
#return
#}
#declare -a colors	# declare Colors array
#colors=(Red Black Blue Grey White Yellow Green Brown) # initialize the arra
#size=${#colors[@]}      # create a size variable and assign the size of our array to it
# in the following for loop, we assign the value of the variable size to last and we are decressing it by 1 (last = $size -1), then a condition called (last > 0) and as long as last is greater that 0 keep decreasing the valuee of last by 1 (last--)
#for (( last = $size -1 ; last > 0 ; last-- ))  
#do
#	for (( i= 0 ; i < last ; i++ ))
#	do
#		[[ "${colors[$i]}" > "${colors[$((i+1))]}" ]] && swap $i $((i+1))#
		# if the value of colors 1 is greater than the following then swap it.
#	done
#done
#echo ${colors[@]}
 
# Lists Section
# ==============
# OR and AND lists
# Donote the AND list with && and the OR list with ||
#E_BADARGS=65
#if [ ! -z "$1" ] && echo "Firs parameter is $1" && [ ! -z "$2" ] && echo "Second parameter is $2"
#then
#	echo "Two parameters are passed to the script"
#else
#	echo "Usage: `basename $0` arg1 arg2" && exit $E_BADARGS
#fi

# Debugging Section
# =================
# I need to read about Debugging in more details in my own through google or somewhere.

--------------- end of bash section ---------------

SDN and Intent-Based-Networking
Software Defined Networking (SDN) moves away from a decentralized approach to management and to a centralized controller. It also moves away from configuring exact settings on each device and centrally configures the needs of the application. Those needs or intents are pushed from a central controller to the devices where the application resides. SDN seeks to program network devices either through a controller or some other external mechanism. It allows the network to be managed as a whole and increase the ability to configure the network in a more deterministic and predictable manner. Several common themes in this trend are disaggregation of a network device’s control and data planes, virtualization of network functionality, policy-centric networking, and movement toward open protocols. Examples of SDN are Cisco ACI in the Data center, Cisco DNA Center in the Campus, and Cisco NSO for Enterprise and Service Provider level administration. SDN is a foundational building block of intent-based networking (IBN).

A trend in the networking industry is to focus on business goals and applications. Intent Based Networking transforms a hardware-centric, manual network into a controller-led network that captures business intent and translates it into policies that can be automated and applied consistently across the network. The goal is for the network to continuously monitor and adjust network performance to help assure desired business outcomes. IBN builds on SDN principles, transforming from a hardware-centric and manual approach to designing and operating networks that are software-centric and fully automated and that add context, learning, and assurance capabilities. IBN captures business intent and uses analytics, machine learning, and automation to align the network continuously and dynamically to changing business needs. That means continuously applying and assuring application performance requirements and automating user, security, compliance, and IT operations policies across the whole network.

An example where SDN and intent-based networking can be used is a modern-day data center. A modern data center is at a large scale and hardware-dense, comprising multiple different technologies from multiple different vendors. Such a large scale, multivendor and dense environment will not support a common command line but instead need to be configured through multiple different clients and GUIs.

Properties of a modern-day data center:
	- Large scale
	- Densely populated
	- Multiple technologies
	- Multivendor

SDN—Characteristics and Components
SDN addresses the needs for:
	- Centralized configuration, management, control, and monitoring of network devices (physical or virtual)
	- The ability to override traditional forwarding algorithms to suite unique business or technical needs
	- Allowing external applications or systems to influence network provisioning and operation
	- Rapid and scalable deployment of network services with life-cycle management

SDN characteristics and components are as follows:
	-Network Devices: 
		Physical and virtual devices that participate in the network, responsible only for forwarding packets based on instructions from SDN Controller. They communicate to an SDN Controller using a southbound interface.
	- SDN Controller: 
		The brains of SDN, responsible for communicating with Network Devices and Applications and Services using APIs. SDN Controller has full awareness of the network state by keeping information in a local database.
	- Southbound Interface: 
		This interface is a layer of device drivers that SDN Controller uses for interacting with physical and virtual devices in the network.
	- Northbound Interface:
		 REST APIs facing outside the network so applications and services can interact with SDN Controller and use resources in the network.
	- Network Management Applications and Services: 
		Clients accessing resources in the network using REST APIs. Clients can be automation user applications, automation servers, or software libraries for many programming languages like Python, Java, Ruby, and others.
		
Northbound APIs or northbound interfaces are responsible for the communication between the SDN controller and the services that run over the network. Northbound APIs enable your applications to manage and control the network. So, rather than adjusting and tweaking your network repeatedly to get a service or application running correctly, you can set up a framework that allows the application to demand the network setup that it needs. These applications range from network virtualization and dynamic virtual network provisioning to more granular firewall monitoring, user identity management, and access policy control. Currently REST API is predominately being used as a single northbound interface that you can use for communication between the controller and all applications.

SDN controller 
architectures have evolved to include a southbound abstraction layer. This abstraction layer abstracts the network away to have one single place where we start writing the applications to and allows application policies to be translated from an application through the APIs, using whichever southbound protocol is supported and available on the controller and infrastructure device. This new approach allows for the inclusion of both new and Southbound Controller Protocols/APIs including (but not limited to):

OpenFlow: 
An industry-standard API, which the Open Networking Foundation (ONF) defines. OpenFlow allows direct access to and manipulation of the forwarding plane of network devices such as switches and routers, both physical and virtual (hypervisor-based). The actual configuration of the devices is by the use of Network Configuration Protocol (NETCONF).

NETCONF: 
An IETF standardized network management protocol. It provides mechanisms to install, manipulate, and delete the configuration of network devices via Remote Procedure Call (RPC) mechanisms. The messages are encoded by using XML. Not all devices support NETCONF; the ones that do support it advertise their capabilities via the API.

RESTCONF: In simplest terms RESTCONF adds a REST API to NETCONF.

OpFlex: 
An open-standard protocol that provides a distributed control system that is based on a declarative policy information model. The big difference between OpFlex and OpenFlow lies with their respective SDN models. OpenFlow uses an imperative SDN model, where a centralized controller sends detailed and complex instructions to the control plane of the network elements to implement a new application policy. In contrast, OpFlex uses a declarative SDN model. The controller, which, in this case, is called by its marketing name Cisco Application Policy Infrastructure Controller (APIC), sends a more abstract policy to the network elements. The controller trusts the network elements to implement the required changes using their own control planes.

REST: 
The software architectural style of the world wide web. REST APIs allow controllers to monitor and manage infrastructure through the HTTP/HTTPS protocols, with the same HTTP verbs (GET, POST, PUT, DELETE, and so on) that web browsers use to retrieve web pages.

SNMP: 
SNMP is used to communicate management information between the network management stations and the agents in the network elements.
Vendor-specific protocols: Lots of vendors use their own proprietary solutions which provide REST API to a device, for example Cisco uses NX-API for Cisco Nexus family of data center switches.

SDN—Benefits
Software-defined networking offers the following benefits:
	- Centralized provisioning: Every networking device is provisioned from an SDN Controller. There is no need for a network administrator to connect to any device and configure it using CLI.
	- Network security: The SDN Controller knows everything about the network and enables easy collection and analysis of network traffic. This information can be used to automatically respond to suspicious activity. Also by using centralized provisioning, policies can be applied consistently through the whole network and applied to all devices.
	- Faster deployments: Applications and services can be deployed faster by using open APIs. Opening a ticket and waiting for the network team to configure devices and security policies to enable your new application becomes a thing of the past.
	- Programmable: Network infrastructures do not have to be rebuilt to be used for a new purpose. They can be programmed to change on-demand without the need to manually configure a single device. Applications can consume REST APIs on the northbound interface to request a network change. The SDN Controller translates that request and uses southbound interface APIs to configure network devices for a new purpose.

Three foundational elements of intent-based networking are:
	- The translation element enables the operator to focus on "what" they want to accomplish, and not "how" they want to accomplish it. The translation element takes the desired intent and translates it to associated network policies and security policies. Before applying these new policies the system checks if these policies are consistent with the already deployed policies or if they will cause any inconsistencies.
	- Once approved, the new policies are then activated (automatically deployed across the network).
	- With assurance, an intent-based network performs continuous verification that the network is operating as intended. Any discrepancies are identified; root-cause analysis can recommend fixes to the network operator. The operator can then "accept" the recommended fixes to be automatically applied, before another cycle of verification. Assurance does not occur at discrete times in an intent-based network. Continuous verification is essential since the state of the network is constantly changing. Continuous verification assures network performance and reliability.

Intent-Based Networking is a form of network administration for automating administrative tasks across the network by allowing only the goal of a request to be specified. For example: "Make sure that this application can access that server."
It is up to the IBN system to determine the following:
	- Determine intent
	- Find network devices between application and server
	- Find optimal route between application and server
	- Configure network devices required for optimal route

In some ways, IBN is similar to SDN:
	- They both rely on a centralized controller to manage devices
	- They are both aware of the current network state and state of every device
Where they differ is how the network is administered. Software-defined networks focus on how a specific set of network device should operate where intent-based networks are focused on what must be done to get to the final goal. This level of abstraction is a key component in IBN. You can think of IBN as a next step in evolution of SDN.

Intent-Based Networking—Characteristics
Every intent-based network, together with characteristics of an SDN, incorporates the following features:
	- Translation and validation: Every intent is verified that it can be properly executed. Only after successful validation can the translation of the intent to valid actions begin.
	- Automation: Resource allocation and policy enforcement are done automatically after the desired state of an intent is known.
	- State awareness: Current state of the network is always known by gathering and monitoring data from all network devices.
	- Assurance and optimization: By learning from gathered data, IBN can assure that the desired state of a network always maintained. This feature is where Artificial Intelligence and Machine Learning become a key part of the network.

IBN—Benefits
In addition to benefits of having a software-defined network, the following are benefits of having an intent-based networking:
	- Reduces complexity: Management and maintenance of an IBN network becomes much simpler.
	- Simplifies deployment: Additional network services deploy faster by abstracting network actions. You do not need to specify what, where, or how to configure specific devices or services, you only request the final state.
	- Strengthens security: Machine-learning algorithms and artificial intelligence can learn and respond to new threats before they become an issue.
	- Improves agility: Having network data in one place helps the network to quickly adapt to changes and have all services available despite failures.
	- Eliminates repetition: Any repetition is prone to errors. Even by using APIs, programmers can make errors. Automation is the key to eliminating repetitive tasks and keeping the manual labor to a minimum.

IBN—Architecture
An enterprise’s network infrastructure may be managed in different domains, separating the operational duties into campus and branch sites, WAN, data center, and the cloud. Applications hosted in the data center or cloud, as well as clients, may also have their own operational procedures, and thus be considered domains. In an IBN, one or more domains are expected to be governed by a controller, which provides a holistic view of the infrastructure and maintains a consistent state (configurations, software images, etc.).

The intent-based system accommodates this arrangement of network infrastructure into domains. Translation and orchestration capabilities are applied across domains, allowing for the characterization of networkwide intent-based policies across the campus and branch sites, WAN, data center and cloud. An orchestration function disseminates the captured policies to the relevant domains, which also enables restriction of some policies’ scope by design. Automating the translation of the model-based policies into device-specific configurations, and instantiating these into the network infrastructure, is covered by the domain-specific controllers. IBN Assurance functions may apply to a particular domain to ensure adherence to the expressed intent-based policy. Additionally, Assurance functions operate across domains to check for compliance with the expressed intent networkwide and end-to-end (from application to application, regardless of where the apps are hosted).

Modern day examples of SDN and IBN technologies:
	- Cisco Application Centric Infrastructure ACI
	- Cisco Digital Network Architecture DMA
	- Cisco Software-Defined WAN SD-WAN
	- Cisco Software-Defined Access SD-Access
	- Cisco Network Services Orchestrator MSO
	
Infrastructure as Code 
Traditional infrastructure management, often looks as follows:
	- A problem is realized and needs to be resolved.
	- Often, a user contacts the helpdesk and the user or an administrator creates a ticket.
	- An engineer opens the ticket and investigates it via the CLI or a GUI on a device-by-device and system-by-system basis.
	- Maybe the ticket is escalated to the next level engineer.
	- This process is repeated until the original problem can be resolved.
	
This process is too slow, too decentralized, and too manual to scale to the level of management needed for a modern-day data center. With IaC, you follow a simple mantra: "If you do it more than once, automate it."

Infrastructure as Code (IaC) allows you to identify state or outcome, produce instructions to accomplish desired state, then reuse, repeat, and evolve to meet new needs as your environment grows. Here you no longer focus on infrastructure but instead on the 'articulation" of the business outcome.

Infrastructure as Code is a way of defining, managing, and interacting with your physical and virtual resources by using machine-readable configuration files and scripts instead of interactive GUI or CLI. These files are often part of the application itself and contain instructions on how to configure, create, and destroy resources in the infrastructure on demand or automatically.

Tools that are involved in provisioning such an infrastructure are text editors, version control systems, and scripts.

Editing a configuration file or a script, applying configuration to infrastructure, and committing changes to a remote code repository is a new process that replaces the legacy workflow. The legacy workflow was to copy CLI commands, edit them to reflect the wanted changes, and paste them to select devices. This practice and the tools it uses are key elements of what is known as DevOps (development and operations merged).

The tools used in Infrastructure as Code are always evolving but their areas of focus are consistent:
	- Centralized storage
	- Collaboration
	- Lifecycle management
	- Automation

With Infrastructure as Code, you start by identifying the steps and tasks you repeat regularly. For example, an administrator who checks the status of their perimeter devices and critical servers every morning before the users start to file into the office. Those GUI or CLI steps will then be coded to produce the desired outcome. The code or script can be used to re-create the desired outcome quickly and identically.
Network as Code is the application of IaC, more specifically to the network infrastructure or what is referred to as the Networking Domain.
NetDevOps is the practice of ongoing development of your network infrastructure using DevOps tools and processes to automate and orchestrate your network operations.

Infrastructure as Code—Benefits
When defining your infrastructure as code, you may observe the following benefits:
	- Capturing your actions "as code" allows you to capture the result or desired state configuration.
	- Declarative model allows you to focus on the state instead of what actions are needed to get there.
	- Lifecycle management is there to help you with evolution of your code from concept to creation to collaboration and use.

When that desired, final state of your infrastructure is defined within code, you can:
	- Store your code in a repository for safe keeping, as a backup of your network configuration
	- Evolve, as your needs change or your network grows, you're able to update and add to your code to meet these changes
	- Collaborate with others in your team and around the world as you work to that final desired state
	- Versioning allows the developers within your team to create a copy of the existing repository or an entirely new "fork"
	- Repeatable, once perfected this code is your product that can be repeated wherever and whenever needed

nfrastructure as Code—Tools
It is important to understand that Infrastructure as Code is not just about the code you create but also managing, storing, collaborating on, and controlling the version of your code. Here are a few tools to help you manage the lifecycle of your living and breathing Infrastructure as Code.

Here is a list of a few tools that are key to the Infrastructure as code lifecycle:
	- GitHub/GitLab
	- Chef/Puppet
	- Ansible
	- Cisco NSO
	- Terraform-Compiler
	- IDEs
	
Cisco DevNet and Google Postman are also invaluable resources.
Github is definitely the largest online service when offering version control and collaboration services.
	- Version control
	- Collaboration
	- Bug tracking
	- Wiki
	- Free public repositories
	- Founded in 2008
	- Acquired by Microsoft in 2018

Github is a web-based service helping developers centrally manage, store, collaborate on and control versions of code. You have the options of performing all management functions via the web interface or to download and install a client on your computer.
Github is available at https://www.github.com.
The Cisco Data Center specific repository on Github is available at https://www.github.com/Datacenter.
The Cisco DevNet repository can be found at https://www.github.com/ciscodevnet.
Chef and Puppet are configuration management systems that require an agent present on a managed host.
	- Open Source
	- Configuration management
	- Agent
	- Pull operations

Chef takes a "recipe" (a single operation) and "cookbooks" (a collection of operations) approach to specifying the steps needed to get to the desired configuration. This process is a procedure-based approach to configuration management. Recipes and cookbooks are used to describe the procedure necessary to get your desired state.
Chef is available at https://www.chef.io/.

Chef and Puppet have many characteristics in common. One key difference is in their approach. Chef focuses more on the control of your nodes while Puppet more on writing the actual configuration files. In other words: Chef is procedural while Puppet is declarative. Chef and Puppet both require agents to be installed on a managed device, which is instructed that a change as been made. They then pull new instructions from your management station and execute them to get to desired state.
Puppet is available at https://puppet.com/.

Ansible is much younger than other configuration management systems but the most widely adopted.
	- RedHat
	- Open Source
	- Configuration
	- Agentless
	- Push operations	

Instead of managing each individual node or system, Ansible uses a model-based approach to your infrastructure by describing how the components and systems are related to one another. Ansible is agentless and uses playbooks to define the declared changes and final state. Ansible's playbooks are written using YAML (Ain't Markup Language) files that are human and machine readable. You issue commands from your management station with Ansible installed and they get executed on the remote system—this approach is known as push.
Ansible is available at https://www.ansible.com/.

Network Services Orchestrator is software for automating services across physical and virtual networks:
	- Cisco
	- Enterprise
	- Service provider
	- Vendor agnostic
	- APIs

Cisco NSO (Network Services Orchestrator) is an Enterprise, a Service Provider level software automation platform that operates across physical and virtual devices. Operations can be accomplished through automation, a self-service portal, and manual provisioning. Latest innovations in Cisco NSO allow the administrator to create their own 'element drivers," or non-Cisco vendor definitions. You can find out more about NSO at http://www.cisco.com/go/nso.

Terraform is an open-source infrastructure-as-code software tool:
	- Hashicorp
	- Execution plans
	- Cloud agnostic
	- APIs

Terraform is a tool that is written to aide the provisioning of your infrastructure. It uses "execution plans" written in code. This execution plans outline exactly what will happen when you run your code. Terraform builds a graph of your resources and can be used to automate changes. Terraform is available at https://www.terraform.io.

System Management with Ansible 
The challenge with modern-data Data Centers lies in their complexity, density, and multivendor solutions spanning across multiple different technologies. The solution is to programmatically manage a Data Center full of devices using a scripting language like PowerShell or full programming language such as Python. These options bring their own challenges of complex features, syntactical instructions, and the learning curve that comes with both. For that reason, another piece in the DevOps puzzle was developed, Ansible. Ansible is an open-source provisioning software that allows for centralized configuration management.

Ansible can be used for almost any automation task. It has these characteristics:
Free, Open-Source
Used for
	- Provisioning
	- Configuration management
	- Deployment tool
Uses its own declarative language
Agentless
Serverless

Unlike other management platforms and services, Ansible does not require an agent to be installed on the system it manages, nor does Ansible need or use a centralized controller for configuration management. Automation can be performed from any management system referencing inventories, modules, or playbooks.

A declarative approach means that you only tell Ansible what you want to achieve as a final goal instead of encoding all instructions to reach it. For example, you do not have to tell Ansible where a specific service resides, how to start it, and what to do after it starts. You simply say: “I want this service to be started, and then I want another service to be restarted.”
More information on Ansible is available at https://www.ansible.com.

System Management with Ansible—Components
The components of Ansible come together to make a very powerful management system.
Understanding each component and their relationships is key to using their power:

Ansible Control 
Station: Is your management station and launching point for all Ansible activities. Unlike many other management platforms, Ansible does not require a dedicated server or elaborate hardware to manage an environment. You could literally manage an enterprise from your personal laptop.

Ansible modules: 
Are a standalone collection of commands that are written in a standard scripting language (ex. Python) and used to execute the desired state change. An administrator can write their own Ansible module using any language so long as that language support JSON as a data format.

Playbooks: 
Are files, also used to definite the desired or final state but are used to orchestrate operation across multiple nodes.

Inventory files (INI): 
Contain systems managed by Ansible. Within this INI file an administrator groups managed systems. Inventory files can be dynamically built by contacting a remote API that responds with valid json response.

YAML (Ain't Markup Language): 
Is commonly referred to as a configuration file and a destination for data being stored. Ultimately YAML is a data format.

Transported
 over SSH by default and with PowerShell support for Windows nodes over the WS-Management protocol.

Ansible Tower: 
Is a web service console (GUI), following the REST standard for programmability. It's a licensed product from Red Hat, based on open source project AWX.

System Management with Ansible—Tools
Managing Ansible is a simple process, simple enough that you're largely able to select your terminal program of choice to connect to your management server running Ansible. You can select your preferred text editor for creating and editing inventory files, playbooks and modules, and use your preferred version control service to manage access to code and control collaboration and editing.
Ansible allows you to pick the tools that you already know:
	- Linux Operating System
	- Terminal program
	- Text editor
	- Version control system
	
How Ansible Works
Working with Ansible requires only a few, quick installation and update steps. Once the installation and update is complete, you're able to configure Ansible operations and defaults. An inventory file and modules will work together to help you execute your changes to the specified target systems.
Getting Ansible to run is simple and straightforward. Follow a few simple steps and complete your Ansible installation in a few moments:
	-Installing
	-Ansible Configuration files
	-INI files
	-Modules
		-Built-in
		-Custom
	-Execution
Installing Ansible can be done in multiple ways but the easiest one is to use python pip. With your python virtual environment active, it is enough to execute pip install ansible and you are done. You can then execute ansible -version to verify that installation was successful.

Ansible configuration files are used to configure Ansible operations. The base configuration file is located at /etc/ansible/ansible.cfg

Inventory files enable an administrator to define an inventory of systems against which Ansible modules will be executed. The basic contents of an INI file are hosts and groups. Host entries point to the DNS name or IP address of a managed end system, while groups are a collection of hosts under a collective label.

How Ansible Works—a "Push Model"
With Ansible installed and upgraded and a list of devices within your network defined in an inventory file, it's time to manage configuration modules and execute them to make changes to your managed devices.

Ansible modules can be thought of as a small program pushed to and run on the managed device to achieve the desired configuration state of that device. Most modules are standalone. Ansible also gives an administrator the ability to write their own module using standard scripting languages such as python. With over 750 built-in modules that are organized by vendor and technology, Ansible enables administrators to ramp up and manage their environment quickly and easily.

There are many Cisco-built Ansible modules, for various data center technologies, in the Cisco Data Center GitHub repository at https://www.github.com/datacenter. More specifically; Cisco ACI modules for Ansible at https://github.com/datacenter/aci-ansible.

You can use ansible-doc modulename to quickly view the information and examples on how to use installed module. More information can be found online at https://docs.ansible.com.

Once your modules are installed, they can be executed from the Ansible host, against the systems defined with your inventory file using the ansible-playbook example-playbook.yml command. Ansible will connect to all systems defined in the inventory simultaneously, make the prescribed changes, and display a status or 'PLAY RECAP" on the terminal screen.

Ansible also supports ad-hoc execution of modules using ansible host1,group2 -m modulename -a moduleargs where host1/group2 are entries that are defined in your inventory file, modulename is the name of the ansible module you want to execute, and moduleargs are the required arguments for that specific module.

Some modules don't require arguments and can be called only with modulename, for example ping module: ansible host1,host2,host3 -m ping would return success if all hosts are reachable.

Ansible Playbooks—Terms
As with any technology, a set of terms and their definitions is key to better understanding that technology. Ansible uses the following terms for orchestration through Playbooks:

Tool			Description
----			-----------
Module			Code, which is written using many scripting languages (for example python) that performs an action on a managed device.
Task			An action referencing a module to run together with input argument and actions.
Play			A set of Tasks to a host or group of hosts.
Playbook		A file, which is written in YAML, that includes one or more plays.
Role			A set of Playbooks, often prebuilt, used to execute a standard configuration in a repeatable manner.
You can assign multiple roles to a single host.

A playbook consists of a (optional) name, hosts, and tasks that should be performed. There are a lot of optional keywords that change the way how Ansible interacts with hosts that are defined in your inventory file. gather_facts is an instruction to Ansible that enables/disables runtime variables available to your tasks. Setting it to no will make execution faster, but your tasks lose access to variables that Ansible collects before execution (ansible_distribution would be empty instead of value Ubuntu or RedHat).

A pound sign (#) indicates a comment everything from here to the end of the line is ignored by ansible.

The vars section contains user-defined variables that are later referenced in tasks section. Variable expansion is done by enclosing the variable name inside double-curly braces, '{{ variable }}."

The tasks section contains tasks that are executed in the order they appear, and they are executed in linear fashion on each host (when one task finishes on all hosts, then execution of the next task starts). This can be changed by setting strategy to free under top-level section of the playbook.

Ansible will run in parallel on maximum 5 hosts, meaning you must wait longer for each task to complete if you add more hosts. This behavior can be changed with forks set to 20 under top-level section of the playbook.

Each task section starts with an (optional) name, followed by a module name that you want to use (apt and service in this example). Module parameters belong under module name and must be indented to indicate that they belong to module and not task. Some module parameters are required, others are optional. You can find out what parameters are required by an equal (=) sign in the output of ansible-doc apt.

CI/CD Pipelines for Infrastructure Automation 
Once you see the power of automation, it’s easy to imagine the other side. The side of automation where an incorrect configuration value is entered and deployed across your entire enterprise, breaking an application or severing connectivity. For that reason, you need to strike a balance between introducing changes to the automation system and make sure that said changes do not harm the enterprise network. That is where CI/CD comes into play.

CI/CD, Continuous Integration/Continuous Delivery, is the process of testing to make sure that changes do not break an application or network (CI) and making sure the change to be deployed is ready for the production environment (CD). CI/CD is simply a Quality Assurance and Testing process for the scripted changes that will ultimately be applied to your production environment.

You can image that testing your existing infrastructure is not an easy task. On one hand you need to test how a new feature, an application, or service behaves in your existing environment. On the other hand, testing in production is out of the question. For these reasons,, you need to have a separate test environment that resembles your production environment as close as it can.

CD/CI server can create test/staging environment automatically as a part of your workflow when you commit changes to your source code repository, or you can have a CI/CD server monitor for changes in your repository and execute automated builds for every new commit or periodically.

Code review also very important in projects where many team members are collaborating. Everyone has their own style of coding and everyone uses a different code editor or IDE to get the work done. You can image that in the end, code would look very inconsistent. This is where code review becomes a necessity. Your IDE can be configured to follow specific rules of code (variable names, function names, indentation) and automatically corrects or warns you if those rules are broken.

Because IDE configuration is not a part of the code repository, code review rules must be implemented on the code repository. This way code that doesn't follow rules gets very early because code review runs before any other tests.

CI/CD Pipelines for Infrastructure Automation—Example
Instead of making changes to code and executing everything from your workstation, a CI/CD pipeline should be implemented to push that change to production in a consistent and predictable manner. Everyone collaborating on the project should see what other team members changed and when that change was made.

The tests can only be run in a staging environment when the source code repository accepts a change, and the changes can only be propagated into the production environment when the staging environment results in a stable state.

Pipelines really depend on the complexity of your infrastructure and tools used. You will focus on the Infrastructure as code with Ansible playbooks as examples.

The following pipeline is very common:
	- You commit a change for example.yml to a code repository
	- Repository executes syntax and sanity checks, code review rules and:
		- Accepts your commit and notifies CI/CD server to run tests
		- Rejects your commit based on failed checks
	- CI/CD prepares an environment and runs predefined tests for any Ansible playbook:
		- pip install ansible==2.8.1
		- ansible-version
		- ansible-playbook example.yml --syntax-check
		- ansible-playbook -i staging_inventory.cfg -check
		- ansible-playbook -i staging_inventory.cfg -vvvv
	- You are notified that you can propagate your changes to production environment

A pipeline like this ensures the following:
	- No syntax errors in your playbook
	- No missing or misspelled modules in your playbook
	- Code is compliant with rules of coding for this project (code review)
	- Ansible version 2.8.1 is used for tests
	- Your playbook executed successfully in staging environment
	- Everyone can see the change and why it failed/succeeded

CI/CD Pipeline—Components
More than one tool is needed for the CI/CD pipeline to be created and integrated into the workflow:
	- A code repository or "repo" to centrally store scripts for safe keeping, availability, version control, forking, and collaboration by others within your development team.
	- A build system to build the automation script. The key here is the tools that you use for creating and testing your script.
	- An integration server/orchestrator to build and run test scripts. On one end of the spectrum this could be as easy as your personal laptop, or it could be as formalized as a dedicated enterprise server.
	- Tools for automatic configuration and deployment would include application for delivering the final scripts for testing in your lab environment.

CI/CD Pipeline—Tools
Here you see how each component from just a moment ago is filled in with an actual tool and tool name that performs the operations of that component. For example, a component would be code repository and Github would perform the operations that are outlined for that component.
CI/CD tools:
	- Github: A code repository
	- Visual Studio Code: Build system (IDE)
	- Standard Linux: build and run scripts
	- Ansible, Teraform: Configuration and deployment
	- Jenkins: Build server
Another helpful tool in the CI/CD/CD pipeline process is GitLab. GitLab helps you visualize the stages and components that are involved and the current position as the changes are progressing through the pipeline.

Software Test Types
Larger projects have many components, and it gets hard to manage what needs to be tested and why. An effective strategy to integrate tests into your workflow is to divide testing into layers. One of the ways to do it is described by the concept of the testing pyramid. It was first mentioned by Mike Cohn (http://www.mountaingoatsoftware.com/blog/the-forgotten-layer-of-the-test-automation-pyramid) in 2009. This concept proposes two things:
	- 1. Tests from upper layer are started when tests from lower layers are finished
	- 2. There should be more tests that are executed in lower layer than in the upper layers

There are many variations of testing pyramid concept and perhaps the most common one is separating layers as following (from lower layer to highest):
	- 1. Unit testing: Testing smallest units of software. Unit can be module, function, class, procedure, and so on.
	- 2, Integration testing: Testing how software components work together
	- 3. System testing: Testing functionality of the entire system
	- 4. Acceptance testing: Testing if system is ready for delivery

Unit Testing
Unit testing should be conducted from the beginning of the development process. Unit under test (UUT) is part of software that can be tested in isolation from the rest of the system, which means that unit tests should not test interaction between multiple system components. UUT can be something like a method, function, procedure, or even an entire class. Good unit tests should have the following characteristics

Reliable: 	Unit test should only fail if there is a bug in the UUT. Sometimes this condition cannot be fulfilled. For example, when you try to write unit test for a method of a class, and you need to call constructor, there's a chance that there's a bug in constructor, which will make the test for the method fail.

Isolated: 	UUT might interact with other components, which might have bugs of their own. Tests should be designed to isolate UUT from other components to avoid test failure in the case when one of those components would cause test failure. This test is usually done with using test doubles. You will learn more about test doubles later on.

Fast: 		Unit tests should cover great number of test cases for each system unit. This condition results in great number of tests. Unit tests are mostly run by developer who need quick feedback if the feature they are working on is working properly. Running huge number of tests can be time costly so it is necessary that one-unit test executes as fast as possible to reduce the time developer has to wait for feedback.

Readable: 	Most of the systems are not finished work and are constantly evolving. Features change and so do the needed tests. Maintaining tests becomes exponentially more difficult if great care is not given to their readability. One of the most recommended practices for writing unit tests is to follow "Arrange, Act, Assert" principle:
	- Arrange: Initialize variables and objects that are required for test
	- Act: Run UUT
	- Assert: Check if result matches the expected

Tested units usually have dependencies to other parts of the system. To isolate UUT from those dependencies, you can use test doubles. A test double is an object that mimics some aspect or functionality of some other object, usually the object on which UUT depends on. There are several types of test doubles:
	- Fake: Object that implements required interface methods but in lightweight manner, which cannot be used in production.
	- Stub: Implements interface methods but responses for given input are known.
	- Mock: Used to verify interaction with mocked component. After the execution of UUT, you can inspect if certain methods of mocked object have been called and how many times they have been called.
	- Dummy: Object that is passed but never actually used or called.
	- Spy: Act like stubs but can provide more information on how methods were called

Integration Testing
Testing each component individually can give you great confidence that for a given input, you will get an output that matches the specification. This fact does not necessarily mean that a component can flawlessly work with other system components. The consequence of isolating a UUT from its dependencies is that you must replace those dependencies with test doubles, which requires abstracting some aspects of the required dependencies. After the unit testing is finished, you need to validate that the component can interact properly with actual system components. Combining and testing multiple components and their interaction is called integration testing. The component that is tested is called Component Under Testing (CUT) or, in some documents, Module Under Testing (MUT).

When conducting integration testing, you can use one of the two approaches:
	- 1. Big bang approach: It requires all components to be finished and all tests use only actual components.
	- 2. Incremental approach: The component can be tested when it is developed. This approach requires test doubles to replace some dependencies.
With the big bang approach, you get more reliable testing results. A disadvantage to this approach is that you need to wait for all components to be finished before you can conduct integration testing. Incremental approach allows testing a component against other components when it is developed.

A common system structure would have core components that have no dependencies on other parts of the system and are considered as lower-level components. Some other system components would depend on these core components, other components would depend on them, etc. Components further down this dependency chain would be considered as higher-level components. When you are testing with incremental approach, you can choose to do go with one of three directions:
	- Top-down approach: From higher-level components to lower-level components
	- Bottom-up approach: From lower-level components to higher-level components
	- Sandwich approach: Combination of top-down and bottom-up approaches

In case where you need to test your component against another component that is not yet developed, you use test doubles to replace them. There are two types of test doubles used with integration testing:
	- 1.Driver: When you are using incremental bottom-up approach, it is expected that lower-level components are already developed and tested but higher-level components are still not developed. Driver replaces missing higher-level components and provides the functionality of initiating and maintaining required interaction with CUT.
	- 2. Stub: It is used in top-down approach. Stub replaces unfinished lower level components and provides basic functionality to enable required interaction for CUT.
	
System Testing
The next step after integration testing is system testing. This layer of testing is probably the hardest part to understand as there are many different expert views on what should be covered by this layer. The purpose of this testing layer is validating that the system or product works as a whole. A lot of aspects of a system could not be tested at lower testing layers and need to be tested before shipping the product to the client. Inputs that you'll get for system testing include available documentation and system requirements. Tests are executed in a testing environment, which differs from production environment only in that it is a closed environment to the developers and testers, and access is not available to end users.

The list of types of system tests that can be conducted is long and it includes:
Functionality testing: 		Testing of system functions. Focus is on defining system functions and validating that for given input to the system you get desired output.

Installation testing: 		Testing installation procedures and showing if there may be some problems with setting up system or product on certain platform, operating system, or device.

Usability testing: 			Tests if system satisfies requirements for targeted userbase. For example, if system can be used by deaf or blind people (mobile apps with haptic feedback).

Security testing: 			Testing system for possible security breaches.

Performance testing: 		Testing system performance such as response times, average time to handle request, number of request sent in certain amount of time.

Load testing: 				Subtype of performance testing. The goal is to validate if system can handle required load such as number of users who are connected at the same time or minimum number of requests in queue.

Stress testing: 			Subtype of performance testing. The goal is to validate stability of a system in extreme conditions such as increased load above maximum load the system can handle.

Regression testing: 		Checks if new version of a system is compatible with older versions.

Storage testing: 			Tests that cover usage of Solid State Drive (SSD) or hard disc such as disc storage capacity.

Configuration testing: 		A given system can have different running configurations. An example would be a cloud-based app that uses configuration file to manage endpoints for used services, which depend on the region and location of the server where app is deployed. Testing systems against different configurations can help expose fallacies in configuration handling

Compatibility testing: 		Checks if the system is compatible with different environments, operating system, mobile devices, etc.

Reliability testing: 		Tests if for same given input system gives same output. Example of a case where you might get different result with same input is when you have system, which delegates execution of some task dynamically to multiple service instances (microservices) to decrease load on single service. When task is run multiple times, it might be delegated to different services, which would each return different result, which would show that this part of the system is not reliable and needs to be fixed.

Recovery testing: 			Tests systems capability of recovery if there is system failure.

Procedure testing: 			Tests defined system procedures (for example, procedure for migrating a database).

The types of test you will use can depend on the product. For example, if you are developing a product, which does not store any data and does not interact with file system, there is no need to do storage testing since no storage capability is required by your product.

Acceptance Testing
Once developers are finished with developing and testing the product, it is ready for delivery. Customers can push delivered product directly to production environment or they can do some tests of their own. This phase of testing belongs to the acceptance testing and it serves the purpose of validating that delivered product matches the requirements of the client. Acceptance testing can also be executed by developers or the end user to validate that product matches users needs. If the product does not satisfy some requirements, it can be returned to developers to fix or improve the product.

There are few types of acceptance testing:
	- Alpha testing: 	Done by developers in a development environment. Developers assume user the role and test the usability of product.
	- Beta testing: 	The selected group of users gains access to the product before its release. They use the product and provide feedback, which helps to find missed bugs and increase product quality.
	- Contract testing: Validates that product satisfies requirements that are requested by a given contract.
	- Regulation testing: Validates that product satisfies legal and government regulations.
	- Operational testing: Validates that product is ready for operational environment. This testing includes validating workflows for client or business operations.

Verifying Code Behavior with Unit Tests 
The goal of software development is to bring a requirement or an idea to life by using various technologies and programming languages. The goal of software testing is to make sure that the implementation of that idea really produces the real intention. Software testing is an integral part of any modern software solution. There are many different types of testing strategies that can be used on a system under test (SUT). The smallest piece of software that you can write to determine whether an application behaves as expected is called a unit test.

Unit tests are automated tests that individually focus on a small portion of the application code that you want to test. You typically define a bunch of unit tests to cover higher percentage of functionality of the SUT, and they can be executed whenever you need to check the validity of the application code. Because a single unit test covers a small portion (unit) of the code in isolation, you can quickly see which parts of the system behave unexpectedly. The size of a unit under test is not strictly defined, and neither is the definition of what a unit is. The team should decide what makes sense to be a unit for the purpose of understanding the system and testing it. Usually, a unit is a single function, method, class, or any statement inside these constructs, that needs verification, but a unit can also span a tightly coupled collection of any of these constructs

Unit Testing Frameworks
As seen on a previous example of implementing your own unit test framework, you can quickly spend more time writing the code that tests your application than on the actual application code. Usually you will be using an existing framework for unit testing, which does not require any code to enable testing features. You will still need to learn how to use it, but once you do that, the only thing you will have to do is to write tests that are tailored for your specific use case using the framework syntax.

The original paper on testing frameworks Simple Smalltalk Testing: With Patterns, which is written by Kent Beck, is the basis for most of the practices used in modern unit testing frameworks. Some of the important concepts that were introduced are support for test automation, shared setup and teardown code for tests, aggregation of tests into collections, and independence of the tests from the reporting framework.
These concepts can be defined with the following terminology:
	- Test fixture: Create a common state of the environment that is needed by the tests and return to the original state (cleanup) afterwards. For example, starting and stopping a server process, defining a temporary database, etc.
	- Test case: The elemental component of a test, an individual unit of testing. Used for checking for a specific response on a set of inputs.
	- Test suite: An aggregation of test cases and other test suites that should be executed together using the same fixture. The order of tests inside a suite does not matter.
	- Test runner: A component for orchestrating the execution of tests and providing a graphical or textual output of the testing results.

Deckerfile Composition
The directory where the Dockerfile is located is called build's context and is used by the docker build command when building the Docker image. All of the files and folders in the context can be used during this process for copying or executing by the instructions in the Dockerfile. The context can be specified also with the PATH or URL instructions in the Dockerfile.

The instructions in the Dockerfile are not case-sensitive, but the conventions dictates they need to be uppercase for clearer readability. A Dockerfile must begin with the FROM instruction which specifies the parent image, one exception is the ARG instruction that can be placed before in case arguments are used in the FROM instruction.
Lines that start with the "#" sign are treated as comments.
Process of creating a container:
	- 1. Write the Dockerfile.
	- 2. Add files to the build's context.
	- 3. Build the image using the docker build command.
	- 4. Start the container with the new image.

Dockerfile properties are:
	- Convention dictates the instructions to be uppercase.
	- Starts with the instruction FROM.
	- Lines starting with "#" are comments.
The basic instructions used in Docker files are:
	- FROM: Specifies the parent (base) image to be used for the following instructions in the Dockerfile. The Dockerfile might have multiple FROM instructions to build multiple images. Usage:
		- The image build with the FROM instruction below will have the latest Ubuntu release for the base image:
			- FROM ubuntu:latest
	- COPY: Used to copy files or directories from the build context into the container. The destination can be an absolute or a relative path in the container's filesystem. Relative paths are relative to the working directory. Usage:
		- The instruction below copies the users.txt file from the build's context to /home/cisco/user_abs.txt. The destination is set with an absolute path:
			- COPY users.txt /home/cisco/users_abs.txt
		- The instruction below copies the users.txt file from the build's context to WORKDIR/user_rel.txt. The destination is set with a relative path:
			- COPY users.txt users_rel.txt
		- ENV: Creates a new environment variable or sets a value of an existing one inside the container. There are two possible ways of defining the environment variables, with or without the "=" sign. By using the "=" multiple variables can be set in the same instruction. Usage:
			- Without the "=" sign:
				- ENV APP_VERSION 6.3
			- With the "=" sign:
				- ENV app_name="Device Manager", app_maintainer=John\ Smith, app_directory=/opt/app
				The double quotes are used when the value of the variable contains spaces, another option is to use the "\" as the escape character.
	- RUN: Used to run a single or multiple commands in a shell in the container. There are two forms of writing and running the commands:
		- The shell form runs the command in the shell with /bin/sh -c:
			- RUN apt update
		- Running the commands in the exec form requires the definition of the shell to be used and the following commands as separate elements:
			- RUN ["/bin/bash", "-c", "echo $HOME"]
	- VOLUME: Creates a mounting point for persisting data that is consumed by the Docker containers. Volumes are managed by Docker and do not get deleted when the container stops running. Usage:
		- The instruction below creates a mounting point with for the directory /opt/app:
			= VOLUME /opt/app
	- EXPOSE: Exposes a TCP or UDP port on which the application running in the container is accessible on. The instruction servers more as a documentation for the one running the container to correctly publish the ports to the outside network when running the container. Usage:
		- The instruction below specifies the port on which the application in the container listens on:
			- EXPOSE 8080
			
Dockerfile Example
The following Dockerfile defines an image that is based on the latest release of Ubuntu. The install.sh file is copied into the /app directory and then mounting point for the directory is created. The image is updated and software is installed. The install is run with the /bin/sh shell and the port 8080 gets exposed.

# Ubuntu based image
FROM ubuntu:latest
COPY install.sh /app
VOLUME /app
RUN apt-get update && \
     apt-get install -qy \
     python3.7
RUN ["/bin/sh", "-c", "/app/install.sh"]
EXPOSE 8080

Using Docker in a Local Developer Environment 
Containerization technology made possible for the developers to replicate the production environment on the local computer. As a developer, you can manage the containers via the terminal. Some basic operations consist of pulling an image from a repository, starting, stopping, and entering a container. To access the container via the network, you can configure the Docker network so it fits the requirements of the application, which is being developed.

Similarly to Git repositories, where the code is stored in a repository, Docker images are stored in Docker repositories. An image might have multiple versions, each with its own unique tag. An image and its versions are stored in a repository and multiple repositories make a registry. Docker Hub is a registry with publicly available and private repositories of Docker images. To transfer an image from a repository to your local Docker installation, use the docker pull image_name:tag command. For example, to pull the latest Ubuntu image, use the following command:

cisco@workstation:~$ docker pull ubuntu:latest

Images that are created locally with Dockerfile or pulled from a repository are templates for running instances called containers. To list the images on the host, use the docker images command. The command displays the properties of the images available on the host, the repository, tag, image ID, when it was created, and its size.

cisco@workstation:~$ docker images
REPOSITORY  TAG    IMAGE ID     CREATED     SIZE
ubuntu      latest 775349758637 3 weeks ago 64.2MB

To run a container from an image, Docker uses the repository and the tag or just the image ID to uniquely identify an image. To run a container from an image, the docker run image command is used. To run an Ubuntu container from the image using the repository name and the tag, use the following command:

cisco@workstation:~$ docker run ubuntu:latest
To run the container using the image ID, use the following command:

cisco@workstation:~$ docker run 775349758637

Additional options can be specified with the command. For example, the previous commands to run the Ubuntu container would exit immediately since no process inside the container is running—Docker containers are in running state only if a process inside them is running. To start a container in interactive mode with the shell displayed in the terminal, options -i and -t should be added to the docker run command.

cisco@workstation:~$ docker run -i -t ubuntu:latest
root@60530976efe7:/# 
To manage the docker containers you first need to get their name or ID. The docker ps command shows the currently running containers, their ID, image from which were created, the command executed, time since creation, the status, which is published ports (if any) and their names.

cisco@workstation:~$ docker ps
CONTAINER ID IMAGE         COMMAND     CREATED        STATUS        PORTS NAMES
7059eef1a2ae ubuntu:latest "/bin/bash" 12 seconds ago Up 11 seconds       compassionate_satoshi

The -a option, appended to the previous docker ps command, displays all existing containers on the host, including the non-running ones.

cisco@workstation:~$ docker ps -a
CONTAINER ID IMAGE         COMMAND     CREATED        STATUS                    PORTS NAMES
7059eef1a2ae ubuntu:latest "/bin/bash" 13 minutes ago Up 13 minutes                   compassionate_satoshi
c396aa7e5cfb ubuntu:latest "/bin/bash" 15 minutes ago Exited (0) 15 minutes ago       clever_hugle

To stop a running container, use the docker stop command. For specifying the container to stop use either the container's name or its ID.

cisco@workstation:~$ docker stop 7059eef1a2ae
7059eef1a2ae
cisco@workstation:~$ docker ps -a
CONTAINER ID IMAGE         COMMAND     CREATED        STATUS                    PORTS NAMES
7059eef1a2ae ubuntu:latest "/bin/bash" 18 minutes ago Exited (0) 13 seconds ago       compassionate_satoshi
c396aa7e5cfb ubuntu:latest "/bin/bash" 15 minutes ago Exited (0) 15 minutes ago       clever_hugle      

To start a container, use the docker start command and specify the container with its name or ID.

cisco@workstation:~$ docker start 7059eef1a2ae
7059eef1a2ae
cisco@workstation:~$ docker ps -a
CONTAINER ID IMAGE         COMMAND     CREATED        STATUS                    PORTS NAMES
7059eef1a2ae ubuntu:latest "/bin/bash" 20 minutes ago Up 4 seconds                    compassionate_satoshi
c396aa7e5cfb ubuntu:latest "/bin/bash" 22 minutes ago Exited (0) 22 minutes ago       clever_hugle                
When you need to remove a container, use the docker rm command and specify the container with its name or ID. Only non-running containers can be deleted.

cisco@workstation:~$ docker rm 7059eef1a2ae
Error response from daemon: You cannot remove a running container 7059eef1a2ae00910a64682cf278ce9cd8aa356683d9936679ac2bbba147bf7a. Stop the container before attempting removal or force remove
cisco@workstation:~$ docker rm c396aa7e5cfb
c396aa7e5cfb
cisco@workstation:~$ docker ps -a
CONTAINER ID IMAGE         COMMAND     CREATED        STATUS       PORTS NAMES
7059eef1a2ae ubuntu:latest "/bin/bash" 20 minutes ago Up 4 seconds       compassionate_satoshi

Docker Network
For connecting a container to another container or machine on the host's network or outside it, Docker uses network drivers. Different types of drivers are available, the default being the bridge driver, which is installed automatically during Docker installment. It connects the containers in the same bridge network and isolates the containers in different bridge networks. When starting a container if no network driver is specified, the bridge network is used.

For example, an application in a container runs on port 8080 and you would like to reach it via the port 80 from the host, you need to publish the port when starting the container using the -p or --publish option. The following example runs a container and publishes a port:

cisco@workstation:~$docker run -p 80:8080 ubuntu:latest
cisco@workstation:~$docker ps -a
CONTAINER ID IMAGE         COMMAND     CREATED       STATUS       PORTS                NAMES
19c353471bea ubuntu:latest "/bin/bash" 7 seconds ago Up 5 seconds 0.0.0.0:80->8080/tcp romantic_joliot

From the host, the application is accessible on the port 80 and the default bridge network driver is used. The PORTS column specifies the port bindings and in this case all traffic coming from any IP on the port 80 is directed to port 8080 of the container.

Docker keeps track of the ports being used by the containers and it in case of publishing a port in use, an error shows saying the port is already allocated to a running container.

cisco@workstation:~$docker run -it -p 8080:81 ubuntu:latest
docker: Error response from daemon: driver failed programming external connectivity on endpoint hopeful_chandrasekhar (45251c13f6c722e711421172790f22128850d9273849712f81974e9d9548d89b): Bind for 0.0.0.0:8080 failed: port is already allocated

Application Security
The security measures can be done on various (TCP/IP) layers:
	- The Internet layer:
		- Using a router to prevent an attacker from viewing a computer's IP address.
	- The application layer:
		- Using a web application firewall (WAF) to prevent malicious unsecure request being accepted.

Web application security relies on a couple of components. Same-origin policy, for example, is one of the basic approaches of identification assurance. Once a website's (such as, https://example.com) content gets permission to access resources on a web-client then the content from any URL will share these permissions. This can happen if the following components of the URL are the same:
	- URI scheme
	- Hostname
	- Port number
If any of these three components are different from the trusted URL then the permission is not granted anymore.

Data Handling
Application data comes in various forms—either as data in motion or as data at rest. Data in motion is data that transits from a device to device or location A (web server or application) to a different location B via a private network or the internet. Data at rest is data that does not move from one place to another and is stored on a computer, hard drive, flash drive, server, and so on. While data in motion can be assumed as less secure, because of the transition, data at rest is usually more valuable and targeted since it consists more sensitive information. Protection of both types (of data) is important and needs to be addressed aggressively since both data types can be exposed to risks.

There are many ways to secure the data in both states and one of the most used tools is encryption. Sensitive data in transit is encrypted and/or send via encrypted connections like HTTPS, Transport Layer Security (TLS), FPTS etc., to preserve its content. Data at rest can also be encrypted before it is stored, or the whole storage drive itself can be encrypted.

It is also important that the HTTPS (with a certificate) connection is established throughout every step of the (data) route:
	- User
	- Browser
	- Router/Firewall/Load Balancer
	- End server

Data Handling with Top OWASP Threats—XSS
The Open Web Application Security Project (OWASP) Foundation is a none-profit charitable organization that came online in late 2001. It is an open community that enables organizations to develop, invent, handle and maintain applications which are trustworthy. Everything that OWASP provides, from tools to documentation and forums, is free and open to anyone. Because the organization has no commercial interest it can provide fair and free information about the application security and its issues.

Cross-site scripting (XSS), occurs when an attacker executes malicious scripts in a victim's web browser. It exploits known vulnerabilities in web applications, web applications servers and its plug-in systems. The attacker injects malicious code, mostly JavaScript, into a legitimate web page or application. When the victim visits the compromised web page, the script is executed. Since it comes from a trusted source, the web browser does not check the content for malicious scripts. This way the attacker gains access-privileges to cookies, delicate content, and other session information operated by the web browser from the user. The most common XSS attacks occur on web pages that grant user comments and web forums.

There are many types of cross-site scripting attacks, but the most common are:
	- Stored XSS attacks occur when the injected malicious code is stored on the targeted web application's server (such as, database). Once the request for the stored information is made, the victim unknowingly executes the malicious script.
	- Reflected XSS attacks occur when the injected malicious code is reflected with any response from the web server that consists of the input sent to the web server. The attack is distributed to the victim in a different way, usually in an email massage. Once the victim clicks on the link, the malicious code navigates to the web page. This action reflects the attack back to the victim's web browser, which then executes the malicious script since the script came from an already trusted web server.

Other XSS attacks (such as, Document Object Model [DOM] Based XSS).

The consequence of the XSS attack is the same no matter the type. The severity of the attack can range from a modified content presentation to hijacking the user's bank or other accounts.

To prevent such attacks it is important that the HTTP TRACE is turned off on a web application server, to deny all untrusted data into the web page HTML document, escape (HTML, JavaScript, cascading style sheet [CSS]) tags and sanitize any of the user's input.

Data Handling with Top OWASP Threats—SQL Injection
Another common web application threat is the Structured Query Language (SQL) injection attack (SQLI). A SQL injection is made of insertion of a SQL query trough the input data from an attacker to the web application. With a SQL injection the attacker can read (sensitive) or modify (such as, Insert or Delete) data from the database. The attacker can execute (sometimes) operating system commands, recover file content or issue administration operations on the Database Management System (DBMS). A successful SQLI attack inserts a metacharacter to data input, which then puts SQL commands into the control plane. Note that SQL treats the data and control plane almost the same.

Any platform that has an interaction with a SQL database is at risk, but websites are among the most common targets.
SQL injection attacks can be separated into three different types:
	-In-band SQL injection is the most common and the easiest to exploit. It happens when the attack can use the identical communication channel to initiate the attack and collect the desired data. Error-based and Union-based are the two most common subtypes of the In-band SQLI.
	
	-Inferential SQL injection or BLIND SQLI takes longer to exploit compared to in-band but is just as malicious as the former. No actual data is sent to the attacker and the attacker can't see the result right away. It can however change the structure of the database. Boolean-based and Time-based are among the most common subtypes of the Inferential SQL injections.
	
	-Out-of-band SQL injection happens when the attacker cannot use the identical communication channel to initiate the attack and collect the results. Since it relies on enabled features on the database server applied by the attacked web application, it is not commonly used. This technique depends on the database server's capacity to issue Domain Name System (DNS) or HTTP request to get a hold of the desired data.

To prevent SQLI attacks you will need to:
	- Sanitize any input data that you get from the user.
	- Use prepared statements and not dynamic SQL
	- Specify the output data so that you do not leak any sensitive data that is not supposed to be seen.

The following is an example of a simple SQL injection.
 	SELECT * FROM Users WHERE UserID = 20 OR 1=1;
Because OR 1=1 is TRUE, SQL query will return all rows from the "Users" table. If the "Users" table contains names and passwords, then this simple attack can be very harmful.

Data Handling with Top OWASP Threats—CSRF/SSRF
Cross-site request forgery (CSRF) attack occurs when an attacker forces a victim to issue undesirable actions on the victim-authenticated web application. The attack is not executed to collect any data but to target state-changing requests (such as, money transfer), because the attacker cannot see the return of the forged request. CSRF is done trough social engineering where a victim clicks on a link (such as, inside an email) and unknowingly submits a forged request. The CSRF attack can also be stored inside an image tag, hidden form or JavaScript XMLHttpRequests, which makes it harder for the victim to detect the attack, because it is done on a legitimate web page and not on some random page. The attack exploits the web applications trust in a user's browser as oppose to the XSS where the user's trust of a web application is exploited.

Prevention of a CSRF attack can be done with anti-forgery tokens. You need to introduce a unique and secret token with every HTTP response. The anti-forgery tokens are usually random numbers that are stored inside a cookie and stored on a web server. With every HTTP request the tokens are validated from the server and if the tokens match, on both the cookie and the server, the request is accepted.

Server-side request forgery vulnerabilities allow the attacker to send forged request from a web server on the behalf of the attacker. In this type of attack the targets are usually internal system behind some firewalls. The web application request sometimes retrieves external information from a third-party resource (such as, updates) and the attacker can modify or control such requests. The attacker can also:
	- Read internal resources and file from the vulnerable server.
	- Scan local or external networks.
To prevent this kind of attacks, you need to use a whitelist of allowed domains and protocols from which the server can retrieve external resources.

Securing and Scaling Application Ingress Traffic 
Applications need to be available 24 hours every day. A successful web app should be able to handle ingress traffic even when number of users drastically rises and be able to support any amount of traffic. For example, if your web page loads in a matter of a couple seconds with 100k users a month, it should be able to load within the same time even with double or triple the amount of user. But just scaling the traffic is not enough, the ingress traffic needs to be secure too. While a lot of business deal with traffic security and scaling when it is usually too late, you need to be prepared and use the right prevention tools and methods.
One of those methods is Secure Sockets Layer (SSL) Offloading, for example, with a load balancer on the edge of your network.
SSL offloading has two different approaches:
	- SSL Termination
	- SSL Bridging
	
SSL termination happens when a load balancer (or proxy server), used for SSL offloading, decrypts the HTTPS connection from the user to the web application. The connection later, from the load balancer to the application server, goes through HTTP. When a user connects to the web app, the connection used by the user's browser is still through HTTPS (and encrypted) and just the communication after the load balancer is changed.

SSL bridging, on the other hand, just changes the data encryption. Instead of sending the traffic requests forward via HTTP, SSL bridging usually re-encrypts the traffic with less-bit Rivest, Shamir, and Adleman (RSA) keys.

SSL termination and SSL bridging both execute traffic analysis and help excessively with handling immense amount of traffic on your network. Since data encryption is a high demanding CPU task, SSL offloading can help you with scaling the web application traffic. However, data encryption is still a desired and best practice procedure.
It is also possible to change the public SSL/TLS certificates once they reach your local network to private (certificates).
A SSL/TLS certificate joins together:
	- Hostname, server name, or domain name
	- Identity and the location of an organization
	- A public key of the web application/page

Public-facing web pages use public SSL/TLS certificates. A public certificate is used to secure user-to-server or server-to-server requests and communication. Because of the regular public policy updates, the public certificate needs to be changed more frequent, whereas a private certificate does not need to.

A private SSL/TLS certificate is a certificate, like the public one, that needs to be approved by a certification authority (CA). The main difference is that it can be used only for server-to-server communication and for non-registered private network domains.

Web Application Firewall
A software or a device that can block, filter, and monitor HTTP requests to and from a web application is called a WAF. The difference between a regular network firewall and WAF is that the network firewall operates on layer 3 (and 4) whereas the WAF operates on layers 3-7.

Reverse Proxy Scrubbing
A reverse proxy server, sometimes called a surrogate, is a server, which resembles a traditional server. It forwards requests to a traditional web server (or multiple servers) that later deals with the requests. The server then responds back to the reverse proxy server, which handles and returns the request to the user. The user however does not know that the request came from the proxy server. A reverse proxy server handles the application ingress traffic in many ways.

SSL encryption is usually done on the reverse proxy with the right SSL acceleration hardware and not by the web application server itself. A (reverse) proxy server can remove the need for multiple separate SSL certificates for each web (application) server behind the reverse proxy and can implement SSL encryption for multiple number of servers. A reverse proxy can also operate as a load balancer by scattering the load to different web application servers. It can remove the burden of the web application servers by caching (static) content, which optimizes the speed of the web page loading time. A security layer is added with the reverse proxy server, protecting against the attack on the operating system and the web server. Nonetheless, the proxy does not implement protection against the web application itself.

Load Balancing
A load balancer distributes the workload and ingress traffic over multiple web servers. It optimizes use of resources, minimizes the response time and helps avoid overload.
A load-balanced system can consist of:
	- Databases
	- Web pages
	- File Transfer Protocol (FTP) sites
	- DNS servers

If you have a simple local network without a designated load balancer provisioned before your web application server, all user's request will go straight to the web server. As the number of users grows, so does the number of requests that need to be handled by the web server. This can slow down your web page or application. In a worst-case scenario the web server can go down and users will lose access to the web page itself.

The most straightforward way to balance the traffic to a server farm (multiple servers) is to implement load balancing on the 4th layer. Load balancer scatters user traffic depending on the IP scope and ports. When a user request includes https://example.com, something the load balancer redirects the request to the back-end servers (or database) of the web application on port 80.

A different and more convoluted approach is to implement load balancing on the 7th layer. Layer 7 load balancing allows to differentiate the request placed on the content of the requests. This allows you to operate several web servers with the same domain name and port number. When a user request includes, for example, https://example.com/shop, the load balancer redirects the request to the series of back-end servers that operate the shop back end. For example, requests for videos on the web server are redirected to a different backend, which can run another application even when both backends use the same database.

A load balancer can use different approaches or algorithms:
	- Round Robin is the default algorithm, which selects servers in turns.
	- Least Conn selects a server with the least number of connections.
	- Source algorithm is established of a hash of the users IP address. This makes sure that the user connects to the same server.
	- Sticky Sessions obtains that the same user connects to the same web server when web applications demand so.

The advantage of sticky session is that the user session does not move from one server to the other. This can lead to more efficient performance because only one server creates the session object, and there is no need for a common data layer for syncing sessions with multiple servers if the application requires so. However, non-sticky load balancing might be favorable for most cases because it is harder to overload one machine. If a node is lost, the sticky session is lost too, which can result in inconsistent behavior for a user.

Health check can be also done by the load balancer, which prevents a single point of failure. If a server does not respond (such as, becomes unhealthy), it becomes disabled and the traffic will not be redirected to the unhealthy server before it becomes responsive again. This is also known as a high availability infrastructure.

Securing and Scaling DNS
The DNS is the second most attacked protocol after the HTTP, which is why securing your DNS servers is very important.

The DNS is an essential component on the networks you are connecting to with your devices. It is storage that holds mapping between names and numbers, similar as a phone book. In a typical user network, the domain names are resolved to IP addresses using a public DNS. You may also use a private DNS to resolve names that you do not intend to expose to the internet. DNS queries and responses can be a rich source of security-related information regarding activity on your network. DNS records provide a wealth of information about an organization's infrastructure, to legitimate users and potential attackers. Making sure that the DNS infrastructure is resilient is critical for the security and operation of internal and external network applications.

Publicly available DNS servers need to be trustworthy and are not supposed to operate recursively. Attackers use DNS recursion to gain knowledge about your internal network. If your domain names have to be resolved by the public, then only those DNS servers should perform those actions. Other DNS servers need to be secured only for your internal network.

All DNS servers need to be an element of a High Availability cluster. If one DNS server goes down, then other will accept the load. This outcume can also be done in High Availability pairs (of two).

Primary name servers only serve the information to secondary DNS servers inside the organization that is why they need to be hidden to end users and not used for queries. They should be accessible to organization IT maintenance employers only to protect the honesty of the DNS information. When public DNS servers are available, the DNS servers in internal network must be behind a network firewall.

A web application can hold tens or hundreds of external (linked) resources where all, resources, may need a DNS lookup for a web application to function properly. If the DNS serves are not provisioned properly, it can slow down the application. That is why it is important for organizations to have on-site DNS servers on every branch instead of just at headquarters or remote sites.

When DNS servers are producing authoritative information, they cannot be used as recursive servers. Zone transfers between DNS servers need to be secured by access control lists (ACLs), this action prevents DDoS attacks. Internal secondary DNS servers must refuse all zone transfer requests.

To provide secure trustworthy DNS queries Domain Name System Security (DNSSEC) extensions need to be executed. DNS information is digitally signed by DNSSEC and makes sure that end users connect to legit web pages or services depending on a domain name. This is done via public key infrastructure (PKI). A link of assurance, between the head of the DNS tree and the bottom end nodes, is created from the root server's digital certificate to the name server.

Network Simulation and Test Tools 
The challenge with modern-data Data Center lies in their complexity, density, and multivendor solutions spanning across multiple different technologies. The solution is to programmatically manage a Data Center full of devices using a scripting language like PowerShell or full programming language such as Python. These options bring their own challenges of complex features, syntactical instructions, and the learning curve that comes with both. For that reason, another piece in the DevOps puzzle was developed, Ansible. Ansible is an open-source provisioning software that allows for centralized configuration management.

When creating a test DevOps environment, it is best to re-create your production environment, or at least include as many of your real-world devices as possible in a test network. Using all physical devices will be prohibitive for many reasons; cost, space, convenience, power, noise being just a few of them. For that reason, instantiating as many of your production devices as virtual machines is an ideal way to re-create your production environment in a test lab. Today that process is easy as so many of your physical devices can also run as a virtual machine.

Here is a short list of only a few emulator and vDevices, network devices in a virtual machine form factor.
	- NX-Osv: The NS-OS Operating System found on your Cisco Nexus switches, running as a virtual machine.
	- UCS-PE: Cisco UCS Manager Platform Emulator. A virtual machine emulating Cisco UCS Manager running in Fabric Interconnects. A download of the UCS-PE is available from the Cisco Community pages at https://community.cisco.com.
	- ASAv: Cisco ASA firewall as a virtual machine. Supporting programmability feature natively, needing only to enable the API services.
	- CSR1Kv: The Cloud Service Router, also known as the CSR 1000v, running as a virtual machine, extends an enterprise into the Public cloud.
	- ACIS: Application Centric Infrastructure Simulator running is a real, fully featured APIC software, along with simulated running as a virtual machine.

Network simulation tools are as follows:
	- VIRL: Cisco VIRL offers fast and easy to deploy network modeling and environment simulations, to include a simulated physical environment connection. VIRL enables administrators to build highly accurate models of existing networks using authentic versions of Cisco network operation systems for Layer 2 and Layer 3 devices including NX-OSv, CSR1000v, ASAv, and IOS-XRv. Cisco VIRL is available at http://virl.cisco.com/.

	- pyATS: Is a Python framework for creating automated tests and validations. Everything from device, to network or even web GUI features can be tested. It enables developers to construct small test cases that can later scale with infrastructure. Cisco pyATS is available at https://developer.cisco.com/pyats/

	- GNS3: Is one of the oldest network simulators around. It can run Cisco IOS images, KVM and VirtualBox machines, Docker containers and more. GNS3 is available at https://www.gns3.com/




